[
["index.html", "Prácticas de Simulación Prólogo", " Prácticas de Simulación Rubén F. Casal (ruben.fcasal@udc.es) 2017-10-05 Prólogo Este libro contiene algunas de las prácticas de la asignatura de Simulación Estadística del Máster interuniversitario en Técnicas Estadísticas). Este libro ha sido escrito en R-Markdown empleando el paquete bookdown y está disponible en el repositorio Github: rubenfcasal/simbook. Para generar el libro (compilar) puede ser recomendable instalar la última versión de RStudio y la versión de desarrollo de bookdown disponible en Github: devtools::install_github(&quot;rstudio/bookdown&quot;) Este obra está bajo una licencia de Creative Commons Reconocimiento-NoComercial-SinObraDerivada 4.0 Internacional (espero poder liberarlo bajo una licencia menos restrictiva más adelante…). "],
["introduccion-a-la-simulacion.html", "Capítulo 1 Introducción a la simulación 1.1 Introducción 1.2 Generación de números (pseudo)aleatorios 1.3 Números aleatorios puros 1.4 Números pseudoaleatorios", " Capítulo 1 Introducción a la simulación 1.1 Introducción Problemas de la experimentación directa sobre la realidad: Coste elevado. En ocasiones las pruebas son destructivas. Lentitud. Puede no ser ética. experimentación sobre seres humanos. Puede resultar imposible. Acontecimientos futuros, … … Puede ser preferible trabajar con un modelo del sistema real. La realidad puede ser muy compleja por lo que es habitual emplear un modelo para tratar de explicarla: Modelos deterministas. Modelos estocásticos (con componente aleatoria). Cuando no se dispone de la suficiente información sobre las variables que influyen en el fenómeno en estudio. Se tienen en cuenta esta incertidumbre. La inferencia estadística proporciona herramientas para estimar los parámetros y contrastar la validez de un modelo estocástico a partir de los datos observados. La idea es emplear el modelo para resolver el problema de interés. Cuando la solución no se puede obtener de modo analítico (teórico) se puede recurrir a la simulación. Simulación: realizar experimentos con un modelo con el objetivo de recopilar información bajo determinadas condiciones. Nos centraremos en el caso de la simulación estocástica: Las conclusiones se obtienen habitualmente generando repetidamente simulaciones del modelo aleatorio. 1.1.1 Ventajas de la simulación (Shannon, 1975): Cuando la resolución analítica no puede llevarse a cabo. Cuando existen medios de resolver analíticamente el problema pero dicha resolución es complicada y costosa (o solo proporciona una solución aproximada). Si se desea experimentar antes de que exista el sistema (pruebas para la construcción de un sistema). Cuando es imposible experimentar sobre el sistema real por ser dicha experimentación destructiva. En ocasiones en las que la experimentación sobre el sistema es posible pero no ética. En sistemas que evolucionan muy lentamente en el tiempo. 1.1.2 Inconvenientes de la simulación: La construcción de un buen modelo puede ser una tarea muy costosa (compleja, laboriosa y requerir mucho tiempo; e.g. modelos climáticos). Frecuentemente el modelo omite variables o relaciones importantes entre ellas (los resultados pueden no ser válidos para el sistema real). Resulta difícil conocer la precisión de la simulación (especialmente en lo relativo a la precisión del modelo formulado). Problemas de extrapolación de las conclusiones. Tiempo de computación. 1.2 Generación de números (pseudo)aleatorios Aplicaciones: Estadística: Muestreo, remuestreo, … Distribución de estadísticos Realización de contrastes, intervalos de confianza, … Comparación de estimadores, contrastes, … Validación teoría (distribución asintótica,…) Inferencia Bayesiana Optimización: Algoritmos genéticos, … Computación: Diseño, verificación y validación de algoritmos,… Criptografía: Protocolos de comunicación segura, … Física: Simulación de fenómenos naturales, … Análisis numérico: Evaluación de expresiones, … … En el Capítulo XX nos centraremos en algunas de las aplicaciones de utilidad en Estadística. 1.3 Números aleatorios puros Una sucesión de números aleatorios puros (true random), se caracteriza por que no existe ninguna regla o plan que nos permita conocer sus valores. Se almacena(ban) en tablas de dígitos aleatorios (true random) y normalmente son obtenidos por procesos físicos (loterías, ruletas, ruidos,…) Se emplean para seleccionar números aleatorios en un rango de 1 a m: Se selecciona al azar un pto de inicio en la tabla y una dirección. Se agrupan los dígitos de forma que “cubran” el valor de m. Se seleccionan los valores menores o iguales que m (se descarta el resto). Algunos enlaces: A Million Random Digits with 100,000 Normal Deviates . RAND Corporation. 1955. Generadores de números aleatorios “online”: http://www.random.org/integers (ver paquete random en R). http://www.fourmilab.ch/hotbits Generadores mediante hardware: http://software.intel.com. http://spectrum.ieee.org Sus principales aplicaciones hoy en día son en criptografía (impredictibilidad). 1.3.1 Inconvenientes: Es necesario/recomendable conocer su distribución. Los datos generados deberían ser i.i.d. Reproductivilidad. Pueden requerir del almacenamiento en tablas. 1.3.2 Alternativas: números pseudo-aleatorios: simulan realizaciones de una variable aleatoria (uniforme). números cuasi-aleatorios: secuencias determinísticas con una distribución más uniforme en el rango considerado (se podría pensar que son una única generación de una variable aleatoria). 1.4 Números pseudoaleatorios 1.4.1 Generación de números pseudoaleatorios mediante software La mayoría de los métodos de simulación se basan en la posibilidad de generar números pseudoaleatorios con distribución \\(\\mathcal{U}(0,1)\\). Se obtienen mediante un algoritmo recursivo denominado generador: \\[x_{i}=f\\left( x_{i-1},x_{i-2},\\cdots,x_{i-k}\\right)\\] \\(k\\) orden del generador. \\(\\left( x_{0},x_{1},\\cdots,x_{k-1}\\right)\\) semilla (estado inicial). El periodo o longitud del ciclo es la longitud de la secuencia antes de que vuelva a repetirse. Lo denotaremos por \\(p\\). Los números de la sucesión serán predecibles, conociendo el algoritmo y la semilla. Sin embargo, si no se conociesen, no se debería poder distinguir una serie de números pseudoaleatorios de una sucesión de números verdaderamente aleatoria (utilizando recursos computacionales razonables). En caso contrario esta predecibilidad puede dar lugar a serios problemas (e.g. http://eprint.iacr.org/2007/419). Como regla general, por lo menos mientras se está desarrollando un programa, interesa fijar la semilla de aleatorización. Permite la reproductibilidad de los resultados. Facilita la depuración del código. Todo generador de números pseudoaleatorios mínimamente aceptable debe comportarse como si se tratase de una muestra genuina de datos independientes de una \\(\\mathcal{U}(0,1)\\). Otras propiedades de interés serían: Reproducibilidad a partir de la semilla. Periodo suficientemente largo. Eficiencia (rapidez y requerimientos de memoria). Portabilidad. Generación de sub-secuencias (computación en paralelo). Parsimonia, … Gran cantidad de algoritmos: Cuadrados medios, Lehmer,… Congruenciales Registros desfasados Combinaciones … Código fuente disponible en múltiples librerias: GNU Scientific Library (GSL): http://www.gnu.org/software/gsl/manual StatLib: http://lib.stat.cmu.edu Numerical recipes: http://www.nrbook.com/nr3 http://random.mat.sbg.ac.at/software KISS (Keep It Simple Stupid / Small and Simple): http://www.fortran.com/kiss.f90 UNU.RAN (paquete Runuran): http://statmath.wu.ac.at/unuran … Nos centraremos en los generadores congruenciales, descritos en la Sección 3.1. "],
["numeros-aleatorios-en-r.html", "Capítulo 2 Números aleatorios en R 2.1 Opciones 2.2 Paquetes de R 2.3 Ejercicios 2.4 Tiempo de CPU", " Capítulo 2 Números aleatorios en R La generación de números pseudoaleatorios en R es una de las mejores disponibles en paquetes estadísticos. Entre las herramientas en el paquete base de R estarían: set.seed(entero): permite establecer la semilla (y el generador). RNGkind: selecciona el generador. rdistribución(n,...): genera valores aleatorios de la correspondiente distribución. Por ejemplo: runif(n, min = 0, max = 1), generaría n valores de una uniforme. sample: genera muestras aleatorias (v.a. discretas) y permutaciones. La semilla se almacena (en globalenv) en .Random.seed; es un vector de enteros cuya dimensión depende del tipo de generador: No debe ser modificado manualmente; se guarda con el entorno de trabajo. Si no se especifica con set.seed (o no existe) se genera a partir del reloj del sistema. Nota: Puede ser recomendable (para depurar) almacenarla antes de generar simulaciones, e.g. semilla &lt;- .Random.seed. 2.1 Opciones La función RNGkind(kind = NULL, normal.kind = NULL) permite seleccionar el tipo de generador (en negrita los valores por defecto): kindespecifica el generador aleatorio: “Wichmann-Hill”: Ciclo \\(6.9536\\times10^{12}\\) “Marsaglia-Multicarry”: Ciclo mayor de \\(2^{60}\\) “Super-Duper”: Ciclo aprox. \\(4.6\\times10^{18}\\) (S-PLUS) “Mersenne-Twister”: Ciclo \\(2^{19937}-1\\) y equidistribution en 623 dim. “Knuth-TAOCP-2002”: Ciclo aprox. \\(2^{129}\\). “Knuth-TAOCP” “user-supplied” normal.kindselecciona el método de generación de normales (se tratará más adelante). “Kinderman-Ramage”, “Buggy Kinderman-Ramage”, “Ahrens-Dieter”, “Box-Muller”, “Inversion” , o “user-supplied” 2.2 Paquetes de R Otros paquetes de R que pueden ser de interés: setRNG contiene herramientas que facilitan operar con la semilla (dentro de funciones,…). random generación de numeros “true random”. randtoolbox implementa generadores más recientes (rngWELL) y generación de secuencias cuasi-aleatorias. RDieHarder implementa diversos contrastes para el análisis de la calidad de un generador y varios generadores. Runuran interfaz para la librería UNU.RAN para la generación (automática) de variables aleatorias no uniformes. rsprng y rstream implementan la generación de múltiples secuencias (e.g. para programación paralela). gls, rngwell19937, randaes, SuppDists, lhs, mc2d, fOptions, … 2.3 Ejercicios Ejercicio 2.1 Sea \\((X,Y)\\) es un vector aleatorio con distribución uniforme en el cuadrado \\([-1,1]\\times\\lbrack-1,1]\\) de área 4. Aproximar mediante simulación \\(P\\left(X + Y \\leq 0 \\right)\\) y compararla con la probabilidad teórica (obtenida aplicando la regla de Laplace \\(\\frac{\\text{área favorable}}{\\text{área posible}}\\)). Probabilidad teórica 1/2 (area favorable / área total) set.seed(1) n &lt;- 10000 x &lt;- runif(n, -1, 1) y &lt;- runif(n, -1, 1) indice &lt;- (x+y &lt; 0) # Aproximación por simulación sum(indice)/n ## [1] 0.4996 mean(indice) # Alternativa ## [1] 0.4996 Nota: R maneja internamente los valores lógicos como 1 (TRUE) y 0 (FALSE). Aproximar el valor de \\(\\pi\\) mediante simulación a partir de \\(P\\left( X^2 +Y^2 \\leq 1 \\right)\\). set.seed(1) n &lt;- 10000 x &lt;- runif(n, -1, 1) y &lt;- runif(n, -1, 1) indice &lt;- (x^2+y^2 &lt; 1) mean(indice) ## [1] 0.7806 pi/4 ## [1] 0.7853982 pi_aprox &lt;- 4*mean(indice) pi_aprox ## [1] 3.1224 Gráfico # Colores y símbolos depediendo de si el índice correspondiente es verdadero: color &lt;- ifelse(indice, &quot;black&quot;, &quot;red&quot;) simbolo &lt;- ifelse(indice, 1, 4) plot(x, y, pch = simbolo, col = color, xlim = c(-1, 1), ylim = c(-1, 1), xlab=&quot;X&quot;, ylab=&quot;Y&quot;, asp = 1) # asp = 1 para dibujar circulo symbols(0, 0, circles = 1, inches = FALSE, add = TRUE) symbols(0, 0, squares = 2, inches = FALSE, add = TRUE) Ejercicio 2.2 Consideramos el experimento de Bernoulli consistente en el lanzamiento de una moneda. Empleando la función sample, obtener 1000 simulaciones del lanzamiento de una moneda (0 = cruz, 1 = cara), suponiendo que no está trucada. Aproximar la probabilidad de cara a partir de las simulaciones. set.seed(1) nsim &lt;- 10000 x &lt;- sample(c(cara = 1, cruz = 0), nsim, replace = TRUE, prob = c(0.5,0.5)) mean(x) ## [1] 0.4953 barplot(100*table(x)/nsim) # Representar porcentajes En R pueden generarse valores de la distribución de Bernoulli mediante la función rbinom(nsim, size=1, prob). Generar un gráfico de lineas considerando en el eje \\(X\\) el número de lanzamientos (de 1 a 10000) y en el eje \\(Y\\) la frecuencia relativa del suceso cara (puede ser recomendable emplear la función cumsum). set.seed(1) nsim &lt;- 1000 p &lt;- 0.4 x &lt;- rbinom(nsim, size = 1, prob = p) # Simulamos una Bernouilli n &lt;- 1:nsim # Alternativa programación: x &lt;- runif(nsim) &lt; p mean(x) ## [1] 0.394 plot(n, cumsum(x)/n, type=&quot;l&quot;, ylab=&quot;Proporción de caras&quot;, xlab=&quot;Número de lanzamientos&quot;, ylim=c(0,1)) abline(h=p, lty=2, col=&quot;red&quot;) Ejercicio 2.3 Simular el paso de corriente a través del siguiente circuito, donde figuran las probabilidades de que pase corriente por cada uno de los interruptores: Considerar que cada interruptor es una v.a. de Bernoulli independiente para simular 1000 valores de cada una de ellas. Nota: R maneja internamente los valores lógicos como 1 (TRUE) y 0 (FALSE). Recíprocamente, cualquier nº puede ser tratado como lógico (al estilo de C). El entero 0 es equivalente a FALSE y cualquier entero distinto de 0 a TRUE. set.seed(1) nsim &lt;- 10000 x1 &lt;- rbinom(nsim, size=1, prob=0.9) x2 &lt;- rbinom(nsim, size=1, prob=0.8) z1 &lt;- x1 | x2 # Operador lógico &quot;O&quot; x3 &lt;- rbinom(nsim, size=1, prob=0.6) x4 &lt;- rbinom(nsim, size=1, prob=0.5) z2 &lt;- x3 | x4 z3 &lt;- z1 | z2 x5 &lt;- rbinom(nsim, size=1, prob=0.7) fin &lt;- z3 &amp; x5 # Operador lógico &quot;Y&quot; mean(fin) ## [1] 0.6918 Ejercicio 2.4 En 1651, el Caballero de Méré le planteó a Pascal una pregunta relacionada con las apuestas y los juegos de azar: ¿es ventajoso apostar a que en cuatro lanzamientos de un dado se obtiene al menos un seis? Este problema generó una fructífera correspondencia entre Pascal y Fermat que se considera, simbólicamente, como el nacimiento del Cálculo de Probabilidades. Escribir una función que simule el lanzamiento de \\(n\\) dados. El parámetro de entrada es el número de lanzamientos \\(n\\), que toma el valor 4 por defecto, y la salida debe ser TRUE si se obtiene al menos un 6 y FALSE en caso contrario. deMere &lt;- function(n = 4){ lanz &lt;- sample(1:6, replace=TRUE, size=n) return(6 %in% lanz) } n &lt;- 4 lanz &lt;- sample(1:6, replace=TRUE, size=n) lanz ## [1] 4 5 3 1 6 %in% lanz ## [1] FALSE Utilizar la función anterior para simular \\(nsim=10000\\) jugadas de este juego y calcular la proporción de veces que se gana la apuesta (obtener al menos un 6 en \\(n\\) lanzamientos), usando \\(n=4\\). Comparar el resultado con la probabilidad teórica \\(1-(5/6)^{n}\\). set.seed(1) n &lt;- 4 nsim &lt;- 10000 mean(replicate(nsim, deMere(n))) ## [1] 0.5195 1-(5/6)^n ## [1] 0.5177469 2.4 Tiempo de CPU La velocidad del generador suele ser una característica importante. Para evaluar el rendimiento están disponibles en R distintas herramientas: proc.time(): permite obtener tiempo de computación real y de CPU. tini &lt;- proc.time() # Código a evaluar tiempo &lt;- proc.time() - tini system.time(expresión): muestra el tiempo de computación (real y de CPU) de expresión. Rprof(fichero): permite evaluar el rendimiento muestreando la pila en intervalos para determinar en que funciones se emplea el tiempo de computación (de utilidad para optimizar la velocidad).Rprof(NULL): desactiva el muestreo.summaryRprof(fichero): muestra los resultados. 2.4.1 Utilidades tiempo de CPU Por ejemplo, podríamos emplear las siguientes funciones para ir midiendo los tiempos de CPU durante una simulación: CPUtimeini &lt;- function() { .tiempo.ini &lt;&lt;- proc.time() .tiempo.last &lt;&lt;- .tiempo.ini } CPUtimeprint &lt;- function() { tmp &lt;- proc.time() cat(&quot;\\nTiempo última operación:\\n&quot;) print(tmp-.tiempo.last) cat(&quot;Tiempo total operación:\\n&quot;) print(tmp-.tiempo.ini) .tiempo.last &lt;&lt;- tmp } Llamando a CPUtimeini() donde se quiere empezar a contar, y a CPUtimeprint() para imprimir el tiempo total y el tiempo desde la última llamada a una de estas funciones. Ejemplo: funtest &lt;- function(n) median(runif(n)) # Función de prueba... CPUtimeini() funtest(1000000) ## [1] 0.5004536 CPUtimeprint() ## ## Tiempo última operación: ## user system elapsed ## 0.11 0.01 0.13 ## Tiempo total operación: ## user system elapsed ## 0.11 0.01 0.13 funtest(1000) ## [1] 0.4954613 CPUtimeprint() ## ## Tiempo última operación: ## user system elapsed ## 0 0 0 ## Tiempo total operación: ## user system elapsed ## 0.11 0.01 0.13 2.4.2 Paquetes de R Por ejemplo, se puede emplear el paquete npsp (fichero cpu.time.R): Call cpu.time(restart = TRUE) where you want to start counting. Call cpu.time() to print/get total and/or partial (since the last call to this function) real and CPU times. # CPU time utilities # ------------------ #&#39; @rdname npsp-internals #&#39; @keywords internal #&#39; @export .cpu.time.ini &lt;- function() { time.ini &lt;- structure(rep(0, 5), .Names = c(&quot;user.self&quot;, &quot;sys.self&quot;, &quot;elapsed&quot;, &quot;user.child&quot;, &quot;sys.child&quot;), class = &quot;proc_time&quot;)# proc.time() time.last &lt;- time.ini function(..., reset = FALSE, total = TRUE, last = TRUE, flush = FALSE) { res &lt;- list(time = proc.time()) if (reset) { time.ini &lt;&lt;- res$time time.last &lt;&lt;- time.ini res$last &lt;- res$total &lt;- 0 if (total | last) cat(&quot;CPU time has been initialized.\\n&quot;) } else { res$last &lt;- res$time - time.last res$total &lt;- res$time - time.ini if (last) { cat(&quot;Time of last operation:&quot;, ..., &quot;\\n&quot;) print(res$last) } if (total) { cat(&quot;Total time:\\n&quot;) print(res$total) } if (flush) flush.console() time.last &lt;&lt;- res$time } return(invisible(res)) } } #&#39; Total and partial CPU time used #&#39; #&#39; Returns and (optionally) prints the total and/or partial #&#39; (since the last call to this function) #&#39; real and CPU times. #&#39; @param ... objects (describing the last operation) to be printed #&#39; (using \\code{\\link{cat}}), #&#39; if \\code{last == TRUE}. #&#39; @param reset logical; if \\code{TRUE}, time counters are initialized. #&#39; @param total logical; if \\code{TRUE}, the total time used is printed. #&#39; @param last logical; if \\code{TRUE}, the partial time used is printed. #&#39; @param flush logical; if \\code{TRUE}, \\code{\\link{flush.console}} is called. #&#39; @return Invisibly returns a list with the following 3 components #&#39; (objects of class \\code{&quot;proc_time&quot;}): #&#39; \\item{time}{user, system, and total elapsed times for the currently running R process #&#39; (result of a call to \\code{\\link{proc.time}}). } #&#39; \\item{last, total}{differences between the corresponding \\code{\\link{proc.time}} calls.} #&#39; @seealso #&#39; \\code{\\link{proc.time}}, \\code{\\link{system.time}}, \\code{\\link{flush.console}}. #&#39; @export cpu.time &lt;- .cpu.time.ini() Ejemplo: cpu.time(reset = TRUE) ## CPU time has been initialized. res &lt;- funtest(1000000) cpu.time(&#39;\\nSample median of&#39;, 1000000, &#39;values =&#39;, res, total = FALSE) ## Time of last operation: ## Sample median of 1e+06 values = 0.4993793 ## user system elapsed ## 0.06 0.02 0.08 res &lt;- funtest(1000) cpu.time(&#39;\\nSample median of&#39;, 1000, &#39;values =&#39;, res) ## Time of last operation: ## Sample median of 1000 values = 0.4936829 ## user system elapsed ## 0.02 0.00 0.02 ## Total time: ## user system elapsed ## 0.08 0.02 0.10 Otro paquete que puede ser de utilidad es microbenchmark (si se quieren estudiar con más detalle los tiempos de computación; aunque en este libro no será el caso…). Hay que tener en cuenta que, por construcción, aunque se realicen en la mismas condiciones (en el mismo equipo), los tiempos de CPU en R pueden variar “ligeramente” entre ejecuciones. "],
["generacion-de-numeros-pseudoaleatorios-1.html", "Capítulo 3 Generación de números pseudoaleatorios 3.1 Generadores congruenciales 3.2 Análisis de la calidad de un generador 3.3 Ejercicios de fin de práctica", " Capítulo 3 Generación de números pseudoaleatorios 3.1 Generadores congruenciales 3.1.1 Generador congruencial lineal (simple) Partiendo de una semilla inicial \\(x_{0}\\): \\[\\begin{aligned} x_{i} &amp; =(ax_{i-1}+c)\\operatorname{mod}m\\\\ u_{i} &amp; =\\dfrac{x_{i}}{m}\\\\ i &amp; =1,2,\\ldots \\end{aligned}\\] donde \\(a\\) es el multiplicador, \\(c\\) el incremento y \\(m\\) el módulo del generador (parámetros enteros fijados de antemano). Si \\(c=0\\) se denomina congruencial multiplicativo Si \\(c\\neq0\\) se denomina congruencial mixto Implementado en el fichero RANDC.R (tratando de imitar el funcionamiento en R, aunque de forma no muy eficiente…): # Generador congruencial de números pseudoaleatorios # ================================================== # -------------------------------------------------- # initRANDC(semilla,a,c,m) # Selecciona el generador congruencial # Por defecto RANDU de IBM con semilla del reloj # OJO: No se hace ninguna verificación de los parámetros initRANDC &lt;- function(semilla=as.numeric(Sys.time()), a=2^16+3, c=0, m=2^31) { .semilla &lt;&lt;- as.double(semilla) %% m #Cálculos en doble precisión .a &lt;&lt;- a .c &lt;&lt;- c .m &lt;&lt;- m return(invisible(list(semilla=.semilla,a=.a,c=.c,m=.m))) #print(initRANDC()) } # -------------------------------------------------- # RANDC() # Genera un valor pseudoaleatorio con el generador congruencial # Actualiza la semilla (si no existe llama a initRANDC) RANDC &lt;- function() { if (!exists(&quot;.semilla&quot;, envir=globalenv())) initRANDC() .semilla &lt;&lt;- (.a * .semilla + .c) %% .m return(.semilla/.m) } # -------------------------------------------------- # RANDCN(n) # Genera un vector de valores pseudoaleatorios con el generador congruencial # (por defecto de dimensión 1000) # Actualiza la semilla (si no existe llama a initRANDC) RANDCN &lt;- function(n=1000) { x &lt;- numeric(n) for(i in 1:n) x[i]&lt;-RANDC() return(x) # return(replicate(n,RANDC())) # Alternativa más rápida } initRANDC(543210) # Fijar semilla 543210 para reproductibilidad Nota: Para evitar problemas computacionales, se recomienda emplear un algoritmo como el descrito en L’Ecuyer (1988). Ejemplos: \\(c=0\\), \\(a=2^{16}+3=65539\\) y \\(m=2^{31}\\), generador RANDU de IBM (no recomendable). \\(c=0\\), \\(a=7^{5}=16807\\) y \\(m=2^{31}-1,\\) Park y Miller (1988) “minimal standar”,empleado por las librerías IMSL y NAG. Los parámetros y la semilla determinan los valores generados: \\[x_{i}=\\left( a^{i}x_{0}+c\\frac{a^{i}-1}{a-1}\\right) \\operatorname{mod}m\\] A pesar de su simplicidad, una adecuada elección de los parámetros permite obtener de manera eficiente secuencias de números “aparentemente” i.i.d. \\(\\mathcal{U}(0,1).\\) 3.1.2 Periodo Teorema 3.1 (Hull y Dobell, 1962) Un generador congruencial tiene período máximo (\\(p=m\\)) si y solo si: \\(c\\) y \\(m\\) son primos relativos (i.e. \\(m.c.d.\\left( c,m\\right) =1\\)). \\(a-1\\) es múltiplo de todos los factores primos de \\(m\\) (i.e. \\(a\\equiv1\\operatorname{mod}q\\), para todo \\(q\\) factor primo de \\(m\\)). Si \\(m\\) es múltiplo de \\(4\\), entonces \\(a-1\\) también lo ha de ser (i.e. \\(m\\equiv0\\operatorname{mod}4\\Rightarrow a\\equiv 1\\operatorname{mod}4\\)). . Algunas consecuencias: Si \\(m\\) primo, \\(p=m\\Leftrightarrow a=1\\) Un generador multiplicativo no cumple la condición 1. Teorema 3.2 Un generador multiplicativo tiene período máximo (\\(p=m-1\\)) si: \\(m\\) es primo. \\(a\\) es una raiz primitiva de \\(m\\)(i.e. el menor entero \\(q\\) tal que \\(a^{q}=1\\operatorname{mod}m\\) es \\(q=m-1\\)). . Además de preocuparse de la longitud del ciclo, las secuencias generadas deben aparentar muestras i.i.d. \\(\\mathcal{U}(0,1)\\). Por ejemplo, los valores generados pueden mostrar una estructura reticular. Marsaglia (1968): \\(k\\)-uplas de generadores multiplicativos contenidas en a lo sumo \\(\\left(k!m\\right)^{1/k}\\) hiperplanos paralelos. Generador RANDU de IBM (70’s): library(rgl) system.time(u &lt;- RANDCN(9999)) # Generar xyz &lt;- matrix(u, ncol = 3, byrow = TRUE) plot3d(xyz) # rglwidget() Se han propuesto diversas pruebas (ver sección siguiente) para determinar si un generador tiene problemas de este tipo y se han realizado numerosos estudios para determinadas familias (e.g. Park y Miller, 1988, \\(m=2^{31}-1\\)). En cualquier caso, se recomienda considerar un “periodo de seguridad” \\(\\approx \\sqrt{p}\\) para evitar este tipo de problemas. Aunque estos generadores tiene limitaciones en su capacidad para producir secuencias muy largas de números i.i.d. \\(\\mathcal{U}(0,1)\\), es un elemento básico en generadores más avanzados. 3.1.3 Otros generadores Generalizaciones del generador congruencial lineal simple: \\[x_{i}=f\\left( x_{i-1},x_{i-2},\\cdots,x_{i-k}\\right) \\operatorname{mod}m\\] no lineal: e.g. \\(\\ x_{i}=a_{1}x_{i-1}+a_{2}x_{i-1}^{2}\\operatorname{mod}m\\) lineal múltiple: \\(x_{i}=a_{1}x_{i-1}+a_{2}x_{i-2}+\\cdots+a_{k}x_{i-k}\\operatorname{mod}m\\), con \\(p\\leq m^{k}-1\\)) matricial: \\(\\boldsymbol{x}_{i} = A_{1}\\boldsymbol{x}_{i-1} + A_{2}\\boldsymbol{x}_{i-2} + \\cdots + A_{k}\\boldsymbol{x}_{i-k} \\operatorname{mod}m\\) (por ejemplo el generador por defecto de R). Generadores de registros desfasados: generadores de bits basados en el lineal múltiple \\(m=2\\); \\(a_{i},x_{i}\\in \\left \\{ 0,1\\right \\}\\) (cálculos mediante operaciones lógicas). Combinación de generadores: Combinación de salidas: \\(u_{i}=\\sum u_{i}^{(l)}\\operatorname{mod}1\\) Barajar salidas… Ejercicio 2.2 Considera el generador congruencial definido por: \\[\\begin{aligned} x_{n+1} &amp; =(5x_{n}+1)\\ \\operatorname{mod}\\ 512,\\nonumber\\\\ u_{n+1} &amp; =\\frac{x_{n+1}}{512},\\ n=0,1,\\dots\\nonumber\\end{aligned}\\] (de ciclo máximo). Algoritmo implementado en el fichero RANDC.R: Generar 500 valores de este generador, obtener el tiempo de CPU, representar su distribución mediante un histograma (en escala de densidades) y compararla con la densidad teórica. initRANDC(321, 5, 1, 512) # Fijar semilla para reproductibilidad nsim &lt;- 500 system.time(u &lt;- RANDCN(nsim)) # Generar ## user system elapsed ## 0.01 0.00 0.02 hist(u, freq = FALSE) abline(h = 1) # curve(dunif(x, 0, 1), add = TRUE) Calcular la media de las simulaciones (mean) y compararla con la teórica. La aproximación por simulación de la media teórica es: mean(u) ## [1] 0.4999609 La media teórica es 0.5. Error absoluto \\(3.90625\\times 10^{-5}\\). Aproximar (mediante simulación) la probabilidad del intervalo \\((0.4;0.8)\\) y compararla con la teórica. La probabilidad teórica es 0.8 - 0.4 = 0.4 La aproximación mediante simulación: sum((0.4 &lt; u) &amp; (u &lt; 0.8))/nsim ## [1] 0.402 mean((0.4 &lt; u) &amp; (u &lt; 0.8)) # Alternativa ## [1] 0.402 3.2 Análisis de la calidad de un generador Para verificar si un generador tiene las propiedades estadísticas deseadas hay disponibles una gran cantidad de test de hipótesis (baterías de contrastes) y métodos gráficos: Contrastes genéricos de bondad de ajuste y aleatoriedad. Contrastes específicos para generadores aleatorios. Se trata principalmente de contrastar si las muestras generadas son i.i.d. \\(\\mathcal{U}\\left(0,1\\right)\\) (análisis univariante). Aunque los métodos más avanzados tratan normalmente de contrastar si las \\(k\\)-uplas: \\[(U_{t+1},U_{t+2},...,U_{t+k-1}); \\ t=(i-1)k, \\ i=1,...,m\\] son i.i.d. \\(\\mathcal{U}\\left(0,1\\right)^{k}\\) (uniformes independientes en el hipercubo; análisis multivariante). Nos centraremos en los métodos genéricos. Pueden usarse en: Evaluación de generadores aleatorios Evaluación de generadores de variables aleatorias Modelado de entradas de modelos de simulación Uno de los contrastes más conocidos es el test ji-cuadrado de bondad de ajuste (chisq.test para el caso discreto). Aunque si la variable de interés es continua, habría que discretizarla (con la correspondiente perdida de información). Por ejemplo, se podría emplear la siguiente función (que imita a las incluídas en R): #------------------------------------------------------------------------------- # chisq.test.cont(x, distribution, nclasses, output, nestpar,...) #------------------------------------------------------------------------------- # Realiza el test ji-cuadrado de bondad de ajuste para una distribución continua # discretizando en intervalos equiprobables. # Parámetros: # distribution = &quot;norm&quot;,&quot;unif&quot;,etc # nclasses = floor(length(x)/5) # output = TRUE # nestpar = 0= nº de parámetros estimados # ... = parámetros distribución # Ejemplo: # chisq.test.cont(x, distribution=&quot;norm&quot;, nestpar=2, mean=mean(x), sd=sqrt((nx-1)/nx)*sd(x)) #------------------------------------------------------------------------------- chisq.test.cont &lt;- function(x, distribution = &quot;norm&quot;, nclasses = floor(length(x)/5), output = TRUE, nestpar = 0, ...) { # Funciones distribución q.distrib &lt;- eval(parse(text = paste(&quot;q&quot;, distribution, sep = &quot;&quot;))) d.distrib &lt;- eval(parse(text = paste(&quot;d&quot;, distribution, sep = &quot;&quot;))) # Puntos de corte q &lt;- q.distrib((1:(nclasses - 1))/nclasses, ...) tol &lt;- sqrt(.Machine$double.eps) xbreaks &lt;- c(min(x) - tol, q, max(x) + tol) # Gráficos y frecuencias if (output) { xhist &lt;- hist(x, breaks = xbreaks, freq = FALSE, lty = 2, border = &quot;grey50&quot;) curve(d.distrib(x, ...), add = TRUE) } else { xhist &lt;- hist(x, breaks = xbreaks, plot = FALSE) } # Cálculo estadístico y p-valor O &lt;- xhist$counts # Equivalente a table(cut(x, xbreaks)) pero más eficiente E &lt;- length(x)/nclasses DNAME &lt;- deparse(substitute(x)) METHOD &lt;- &quot;Pearson&#39;s Chi-squared test&quot; STATISTIC &lt;- sum((O - E)^2/E) names(STATISTIC) &lt;- &quot;X-squared&quot; PARAMETER &lt;- nclasses - nestpar - 1 names(PARAMETER) &lt;- &quot;df&quot; PVAL &lt;- pchisq(STATISTIC, PARAMETER, lower.tail = FALSE) # Preparar resultados classes &lt;- format(xbreaks) classes &lt;- paste(&quot;(&quot;, classes[-(nclasses + 1)], &quot;,&quot;, classes[-1], &quot;]&quot;, sep = &quot;&quot;) RESULTS &lt;- list(classes = classes, observed = O, expected = E, residuals = (O - E)/sqrt(E)) if (output) { cat(&quot;\\nPearson&#39;s Chi-squared test table\\n&quot;) print(as.data.frame(RESULTS)) } if (any(E &lt; 5)) warning(&quot;Chi-squared approximation may be incorrect&quot;) structure(c(list(statistic = STATISTIC, parameter = PARAMETER, p.value = PVAL, method = METHOD, data.name = DNAME), RESULTS), class = &quot;htest&quot;) } Por ejemplo, continuando con el generador congruencial anterior, obtendríamos: chisq.test.cont(u, distribution = &quot;unif&quot;, nclasses = 10, nestpar = 0, min = 0, max = 1) ## ## Pearson&#39;s Chi-squared test table ## classes observed expected residuals ## 1 (-1.490116e-08, 1.000000e-01] 51 50 0.1414214 ## 2 ( 1.000000e-01, 2.000000e-01] 49 50 -0.1414214 ## 3 ( 2.000000e-01, 3.000000e-01] 49 50 -0.1414214 ## 4 ( 3.000000e-01, 4.000000e-01] 50 50 0.0000000 ## 5 ( 4.000000e-01, 5.000000e-01] 51 50 0.1414214 ## 6 ( 5.000000e-01, 6.000000e-01] 51 50 0.1414214 ## 7 ( 6.000000e-01, 7.000000e-01] 49 50 -0.1414214 ## 8 ( 7.000000e-01, 8.000000e-01] 50 50 0.0000000 ## 9 ( 8.000000e-01, 9.000000e-01] 50 50 0.0000000 ## 10 ( 9.000000e-01, 9.980469e-01] 50 50 0.0000000 ## ## Pearson&#39;s Chi-squared test ## ## data: u ## X-squared = 0.12, df = 9, p-value = 1 Importante: Empleando los métodos genéricos del modo habitual, desconfiamos del generador si la muestra/secuencia no se ajusta a la distribución teórica (p-valor \\(\\leq \\alpha\\)). En este caso además, también se sospecha si se ajusta demasiado bien a la distribución teórica (p-valor \\(\\geq1-\\alpha\\)). Otro contraste de bondad de ajuste muy conocido es el test de Kolmogorov-Smirnov, implementado en ks.test. Ejercicio 2.4 Continuando con el generador congruencial anterior: initRANDC(321, 5, 1, 512) # Fijar semilla para reproductibilidad nsim &lt;- 500 system.time(u &lt;- RANDCN(nsim)) # Generar Realizar el contraste de Kolmogorov-Smirnov para estudiar el ajuste a una \\(\\mathcal{U}(0,1)\\). # Distribución empírica curve(ecdf(u)(x), type = &quot;s&quot;, lwd = 2) curve(punif(x, 0, 1), add = TRUE) # Test de Kolmogorov-Smirnov ks.test(u, &quot;punif&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: u ## D = 0.0033281, p-value = 1 ## alternative hypothesis: two-sided Obtener el gráfico secuencial y el de dispersión retardado, ¿se observa algún problema? Gráfico secuencial: plot(as.ts(u)) Gráfico de dispersión retardado: plot(u[-nsim],u[-1]) Estudiar las correlaciones del vector \\((u_{i},u_{i+k})\\), con \\(k=1,...,10\\). Contrastar si son nulas. Correlaciones: acf(u) Test de Ljung-Box: Box.test(u, lag = 10, type = &quot;Ljung&quot;) ## ## Box-Ljung test ## ## data: u ## X-squared = 22.533, df = 10, p-value = 0.01261 3.2.1 Repetición de contrastes Los contrastes se plantean habitualmente desde el punto de vista de la inferencia estadística en la práctica: se realiza una prueba sobre la única muestra disponible. Si se realiza una única prueba, en las condiciones de \\(H_{0}\\) hay una probabilidad \\(\\alpha\\) de rechazarla. En simulación tiene mucho más sentido realizar un gran número de pruebas: La proporción de rechazos debería aproximarse al valor de \\(\\alpha\\)(se puede comprobar para distintos valores de \\(\\alpha\\)). La distribución del estadístico debería ajustarse a la teórica bajo \\(H_{0}\\)(se podría realizar un nuevo contraste de bondad de ajuste). Los p-valores obtenidos deberían ajustarse a una \\(\\mathcal{U}\\left(0,1\\right)\\) (se podría realizar también un contraste de bondad de ajuste). Este procedimiento es también el habitual para validar un método de contraste de hipótesis por simulación. Ejemplo 3.1 Consideramos el generador congruencial RANDU: # Valores iniciales initRANDC(543210) # Fijar semilla para reproductibilidad # set.seed(543210) n &lt;- 500 nsim &lt;- 1000 estadistico &lt;- numeric(nsim) pvalor &lt;- numeric(nsim) # Realizar contrastes for(isim in 1:nsim) { u &lt;- RANDCN(n) # Generar # u &lt;- runif(n) tmp &lt;- chisq.test.cont(u, distribution=&quot;unif&quot;, nclasses=100, output=FALSE, nestpar=0, min=0, max=1) estadistico[isim] &lt;- tmp$statistic pvalor[isim] &lt;- tmp$p.value } Proporción de rechazos: # cat(&quot;\\nProporción de rechazos al 1% =&quot;, sum(pvalor &lt; 0.01)/nsim, &quot;\\n&quot;) cat(&quot;\\nProporción de rechazos al 1% =&quot;, mean(pvalor &lt; 0.01), &quot;\\n&quot;) ## ## Proporción de rechazos al 1% = 0.014 # cat(&quot;Proporción de rechazos al 5% =&quot;, sum(pvalor &lt; 0.05)/nsim, &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 5% =&quot;, mean(pvalor &lt; 0.05), &quot;\\n&quot;) ## Proporción de rechazos al 5% = 0.051 # cat(&quot;Proporción de rechazos al 10% =&quot;, sum(pvalor &lt; 0.1)/nsim, &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 10% =&quot;, mean(pvalor &lt; 0.1), &quot;\\n&quot;) ## Proporción de rechazos al 10% = 0.112 Análisis del estadístico contraste: # Histograma hist(estadistico, breaks = &quot;FD&quot;, freq=FALSE) curve(dchisq(x,99), add=TRUE) # Test ji-cuadrado # chisq.test.cont(estadistico, distribution=&quot;chisq&quot;, nclasses=20, nestpar=0, df=99) # Test de Kolmogorov-Smirnov ks.test(estadistico, &quot;pchisq&quot;, df=99) ## Warning in ks.test(estadistico, &quot;pchisq&quot;, df = 99): ties should not be ## present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: estadistico ## D = 0.023499, p-value = 0.6388 ## alternative hypothesis: two-sided Análisis de los p-valores: # Histograma hist(pvalor, freq=FALSE) abline(h=1) # curve(dunif(x,0,1), add=TRUE) # Test ji-cuadrado # chisq.test.cont(pvalor, distribution=&quot;unif&quot;, nclasses=20, nestpar=0, min=0, max=1) # Test de Kolmogorov-Smirnov ks.test(pvalor, &quot;punif&quot;, min=0, max=1) ## Warning in ks.test(pvalor, &quot;punif&quot;, min = 0, max = 1): ties should not be ## present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: pvalor ## D = 0.023499, p-value = 0.6388 ## alternative hypothesis: two-sided 3.2.2 Baterías de contrastes Contrastes específicos para generadores aleatorios: Diehard tests (The Marsaglia Random Number CDROM): http://www.stat.fsu.edu/pub/diehard. TestU01: http://www.iro.umontreal.ca/simardr/testu01/tu01.html. NIST test suite: http://csrc.nist.gov/groups/ST/toolkit/rng. Dieharder (paquete RDieHarder): http://www.phy.duke.edu/rgb/General/dieharder.php Entidad Certificadora (gratuita): CAcert. Documentación adicional: Randomness Tests: A Literature Survey http://www.ciphersbyritter.com/RES/RANDTEST.HTM- Marsaglia, Tsang (2002). Some Difficult-to-pass Tests of Randomness: http://www.jstatsoft.org/v07/i03 http://www.csis.hku.hk/cisc/download/idetect 3.3 Ejercicios de fin de práctica Ejercicio 3.1 Uno de los primeros generadores fue el denominado método de los cuadrados medios propuesto por Von Neumann (1946). Con este procedimiento se generan números pseudoaleatorios de 4 dígitos de la siguiente forma: Se escoge un número de cuatro dígitos \\(x_{0}\\) (semilla). Se eleva al cuadrado (\\(x_{0}^{2}\\)) y se toman los cuatro dígitos centrales (\\(x_{1}\\)). Se genera el número pseudo-aleatorio como\\[u_{1}=\\frac{x_{1}}{10^{4}}.\\] Volver al paso ii y repetir el proceso. Para obtener los \\(k\\) (número par) dígitos centrales de \\(x_{i}^{2}\\) se puede utilizar que: \\[x_{i+1}=\\left\\lfloor \\left( x_{i}^{2}-\\left\\lfloor \\dfrac{x_{i}^{2}}{10^{(2k-\\frac{k}{2})}}\\right\\rfloor 10^{(2k-\\frac{k}{2})}\\right) /10^{\\frac{k}{2}}\\right\\rfloor\\] El algoritmo está implementado en el fichero RANDVN.R: # Generador Von Neumann de números pseudoaleatorios # ================================================= # ------------------------------------------------- # initRANDVN(semilla,n) # Inicia el generador # n número de digitos centrales, 4 por defecto (debe ser un nº par) # Por defecto semilla del reloj # OJO: No se hace ninguna verificación de los parámetros initRANDVN &lt;- function(semilla = as.numeric(Sys.time()), n = 4) { .semilla &lt;&lt;- as.double(semilla) %% 10^n # Cálculos en doble precisión .n &lt;&lt;- n .aux &lt;&lt;- 10^(2*n-n/2) .aux2 &lt;&lt;- 10^(n/2) return(invisible(list(semilla=.semilla,n=.n))) } # ------------------------------------------------- # RANDVN() # Genera un valor pseudoaleatorio con el generador de Von Neumann # Actualiza la semilla (si no existe llama a initRANDVN) RANDVN &lt;- function() { if (!exists(&quot;.semilla&quot;, envir=globalenv())) initRANDVN() z &lt;- .semilla^2 .semilla &lt;&lt;- trunc((z-trunc(z/.aux)*.aux)/.aux2) return(.semilla/10^.n) } # ------------------------------------------------- # RANDVNN(n) # Genera un vector de valores pseudoaleatorios con el generador congruencial # (por defecto de dimensión 1000) # Actualiza la semilla (si no existe llama a initRANDVN) RANDVNN &lt;- function(n = 1000) { x &lt;- numeric(n) for(i in 1:n) x[i] &lt;- RANDVN() return(x) # return(replicate(n,RANDVN())) # Alternativa más rápida } Estudiar las características del generador de cuadrados medios a partir de una secuencia de 500 valores. Emplear únicamente métodos gráficos. Ejercicio 3.2 Considerando el generador congruencial multiplicativo de parámetros \\(a=7^{5}=16807\\), \\(c=0\\) y \\(m=2^{31}-1\\). ¿Se observan los mismos problemas que con el algoritmo RANDU al considerar las tripletas \\((x_{k},x_{k+1},x_{k+2})\\)? Ejercicio 3.3 (para entregar) Considera el generador congruencial definido por: \\[\\begin{aligned} x_{n+1} &amp; =(65x_{n}+1)\\ \\operatorname{mod}\\ 2048,\\nonumber\\\\ u_{n+1} &amp; =\\frac{x_{n+1}}{2048},\\ n=0,1,\\dots\\nonumber \\end{aligned}\\] Indicar razonadamente si es de ciclo máximo. Generar 1000 valores tomando como semilla inicial el nº de grupo multiplicado por 100 y obtener el tiempo de CPU. Representar gráficamente el ajuste a la densidad teórica y realizar el correspondiente contraste de Kolmogorov-Smirnov. Representar los pares de datos \\(\\left( u_{i}, u_{i+1} \\right)\\), ¿se observa algún problema?. Estudiar la aleatoriedad de este generador empleando repetidamente el test de Ljung-Box, considerando 500 pruebas con muestras de tamaño 50 y hasta el salto 10 (Box.test(u,lag=10, type=Ljung)). Comparar el ajuste de las distribuciones del estadístico y \\(p\\)-valor a las de referencia. "],
["tecnicas-de-reduccion-de-la-varianza.html", "Capítulo 4 Técnicas de reducción de la varianza 4.1 Reducción de la varianza 4.2 Variables antitéticas 4.3 Estratificación 4.4 Variables de control 4.5 Números aleatorios comunes 4.6 Ejercicios fin de práctica", " Capítulo 4 Técnicas de reducción de la varianza 4.1 Reducción de la varianza Éstas técnicas son aplicadas normalmente cuando se pretende ofrecer respuestas lo más precisas posibles (con menor costo computacional) y principalmente sobre cantidades medias. Supongamos que estamos interesados en aproximar la media de un estadístico mediante simulación y no nos interesa aproximar su varianza. Existe un sinfín de técnicas encaminadas a reducir la varianza en un estudio de simulación (respecto a una aproximación estandar). Algunas de ellas son: Muestreo por importancia. Variables antitéticas. Muestreo estratificado. Variables de control. Números aleatorios comunes. Métodos de remuestreo. Condicionamiento. … 4.2 Variables antitéticas Supongamos que pretendemos aproximar \\[\\theta=E\\left( Z\\right)\\] con \\(Var\\left( Z \\right) = \\sigma^{2}\\). Si generamos \\(n\\) pares \\(\\left( X_{1},Y_{1}\\right), ... ,\\left( X_{n},Y_{n}\\right)\\) de \\(X\\sim Y\\sim Z\\) con \\(Cov\\left( X,Y\\right) &lt; 0\\), el estimador combinado tiene menor varianza: \\[\\begin{aligned} Var\\left( \\frac{\\overline{X}+\\overline{Y}}{2}\\right) &amp; =\\frac{1}{4}\\left( Var\\left( \\overline{X}\\right) +Var\\left( \\overline{Y}\\right) +2Cov\\left( \\overline{X},\\overline{Y}\\right) \\right) \\\\ &amp; =\\frac{\\sigma^{2}}{2n}+\\frac{1}{2n}Cov\\left( X,Y\\right) \\\\ &amp; =\\frac{\\sigma^{2}}{2n}\\left( 1+\\rho \\left( X,Y\\right) \\right), \\end{aligned}\\] que el equivalente a una muestra unidimensional independiente con el mismo número de observaciones \\(2n\\) (con una reducción del \\(-100\\rho \\left( X,Y\\right) \\%\\)). 4.2.1 Ejemplo: Integración Monte Carlo Para aproximar: \\[I=\\int_{0}^{1}h\\left( x\\right) dx,\\] a partir de \\(x_{1},x_{2},\\ldots,x_{n}\\) \\(i.i.d.\\) \\(\\mathcal{U}\\left(0,1\\right)\\). Podemos emplear: \\[\\begin{aligned} I &amp; =E\\left( \\frac{h\\left( U\\right) +h\\left( 1-U\\right) }{2}\\right) \\\\ &amp; \\approx \\frac{1}{2n}\\sum \\limits_{i=1}^{n}\\left( h\\left( x_{i}\\right) +h\\left( 1-x_{i}\\right) \\right). \\end{aligned}\\] 4.2.2 Generación de variables antitéticas Cuando se utiliza el método de inversión resulta sencillo obtener pares de variables con correlación negativa: \\(U\\sim \\mathcal{U}\\left( 0,1\\right)\\) para simular \\(X\\). \\(1-U\\) para simular la variable antitética \\(Y\\). En el caso general, si \\(X=h\\left( U_{1},\\ldots,U_{d}\\right)\\) y \\(h\\) es monótona puede verse (e.g. Ross, 1997) que \\(Y=h\\left( 1-U_{1},\\ldots,1-U_{d}\\right)\\) está negativamente correlada con \\(X\\). Si \\(X\\sim \\mathcal{N}(\\mu,\\sigma)\\) puede tomarse como variable antitética \\[Y=2\\mu-X\\] En general esto es válido para cualquier variable simétrica repecto a un parámetro \\(\\mu\\). (e.g. \\(X\\sim \\mathcal{U}(a,b)\\) e \\(Y=a+b-X\\)). Ejercicio 4.1 Variables antitéticas en integración Monte Carlo Crear una función que implemente la técnica de variables antitéticas para aproximar integrales del tipo:\\[I=\\int_{a}^{b}h\\left( x\\right) dx.\\] Emplearla para aproximar: \\[E\\left( e^{\\mathcal{U}(0,2)}\\right) =\\int_{0}^{2}\\frac{1}{2}e^{x}dx\\approx3.194,\\label{integralmc}\\] y representar gráficamente la aproximación en función de \\(n\\). Función objetivo: a &lt;- 0; b &lt;- 2 ftn &lt;- function(x) return(exp(x)/(b-a)) curve(ftn, a, b, ylim=c(0,4)) abline(h=0,lty=2) abline(v=c(a,b),lty=2) Se trata de calcular la media de \\(e^{\\mathcal{U}(0,2)}\\): teor &lt;- (exp(b)-exp(a))/(b-a) teor ## [1] 3.194528 Para la aproximación por integración Monte Carlo podemos emplear la función del capítulo anterior: mc.integral &lt;- function(ftn, a, b, n, plot=TRUE) { fx &lt;- sapply(runif(n, a, b), ftn)*(b-a) if (plot) { estint &lt;- cumsum(fx)/(1:n) esterr &lt;- sqrt(cumsum((fx-estint)^2))/(1:n) plot(estint, ylab=&quot;Media y rango de error&quot;, type=&quot;l&quot;, lwd= 2, ylim=mean(fx)+2*c(-esterr[1],esterr[1]), xlab=&quot;Iteraciones&quot;) abline(h = estint[n], lty=2) lines(estint+2*esterr, lty = 3) lines(estint-2*esterr, lty = 3) return(list(valor=estint[n], error=2*esterr[n])) } else return(list(valor=mean(fx), error=2*sd(fx)/sqrt(n))) } set.seed(54321) res &lt;- mc.integral(ftn, a, b, 500) abline(h = teor) res ## $valor ## [1] 3.184612 ## ## $error ## [1] 0.1619886 Para la integración Monte Carlo con variables antitéticas podríamos considerar: mc.integrala &lt;- function(ftn, a, b, n, plot=TRUE,...) { # n es el nº de evaluaciones de la función objetivo (para facilitar comparaciones, solo se genera la mitad) x &lt;- runif(n%/%2, a, b) # La siguiente línea solo para representar alternando x &lt;- as.numeric(matrix(c(x,a+b-x),nrow=2,byrow=TRUE)) # bastaría con emplear p.e. c(x,a+b-x) fx &lt;- sapply(x, ftn)*(b-a) if (plot) { estint &lt;- cumsum(fx)/(1:n) esterr &lt;- sqrt(cumsum((fx-estint)^2))/(1:n) plot(estint, ylab=&quot;Media y rango de error&quot;,type=&quot;l&quot;, lwd = 2, ylim=mean(fx)+2*c(-esterr[1],esterr[1]),xlab=&quot;Iteraciones&quot;,...) abline(h = estint[n], lty=2) lines(estint+2*esterr, lty = 3) lines(estint-2*esterr, lty = 3) return(list(valor=estint[n],error=2*esterr[n])) } else return(list(valor=mean(fx),error=2*sd(fx)/sqrt(n))) } set.seed(54321) res &lt;- mc.integrala(ftn, a, b, 500) res ## $valor ## [1] 3.222366 ## ## $error ## [1] 0.1641059 Pero aunque aparentemente converge antes, parece no haber una mejora en la precisión de la aproximación. Si calculamos el porcentaje (estimado) de reducción del error: 100*(0.1619886-0.1641059)/0.1619886 ## [1] -1.307067 El problema es que en este caso se está estimando mal la varianza (asumiendo independencia). Hay que tener cuidado con las técnicas de reducción de la varianza si uno de los objetivos de la simulación es precisamente estimar la variabilidad. En este caso, una versión de la función anterior para integración Monte Carlo con variables antitéticas, con aproximación del error bajo dependencia podría ser: mc.integrala2 &lt;- function(ftn, a, b, n, plot = TRUE,...) { # n es el nº de evaluaciones de la función objetivo (para facilitar comparaciones, solo se genera la mitad) x &lt;- runif(n%/%2, a, b) # La siguiente línea solo para representar alternando x &lt;- matrix(c(x,a+b-x),nrow=2,byrow=TRUE) # bastaría con emplear p.e. c(x,a+b-x) fx &lt;- apply(x, 1, ftn)*(b-a) corr &lt;- cor(fx[,1], fx[,2]) fx &lt;- as.numeric(fx) return(list(valor=mean(fx), error=2*sd(fx)/sqrt(n)*sqrt(1+corr))) } set.seed(54321) res &lt;- mc.integrala2(ftn, a, b, 500) res ## $valor ## [1] 3.222366 ## ## $error ## [1] 0.05700069 Porcentaje estimado de reducción del error: 100*(0.1619886-0.05700069)/0.1619886 ## [1] 64.81191 En este caso puede verse que la reducción teórica de la varianza es del 96.7% 4.3 Estratificación Si se divide la población en estratos y se genera en cada uno un número de observaciones proporcional a su tamaño (a la probabilidad de cada uno) nos aseguramos de que se cubre el dominio de interés y se puede acelerar la convergencia. Por ejemplo, para generar una muestra de tamaño \\(n\\) de una \\(\\mathcal{U}\\left( 0,1\\right)\\), se pueden generar \\(l=\\frac{n}{k}\\) observaciones (\\(1\\leq k\\leq n\\)) de la forma: \\[U_{j_{1}},\\ldots,U_{j_{l}}\\sim \\mathcal{U}\\left( \\frac{(j-1)}{k},\\frac{j}{k}\\right) \\text{ para }j=1,...,k.\\] Si en el número de obsevaciones se tiene en cuenta la variabilidad en el estrato se puede obtener una reducción significativa de la varianza. Ejemplo 4.1 Muestreo estratificado de una exponencial (libro Ricardo) Supóngase el siguiente problema (absolutamente artificial pero ilustrativo para comprender esta técnica). Dada una muestra de tamaño 10 de una población con distribución: \\[X \\sim \\exp\\left( 1 \\right),\\] se desea aproximar la media poblacional (es sobradamente conocido que es 1) a partir de 10 simulaciones. Supongamos que para evitar que, por puro azar, exista alguna zona en la que la exponencial toma valores, no representada en la muestra simulada de 10 datos, se consideran tres estratos. Por ejemplo, el del 40% de valores menores, el siguiente 50% de valores (intermedios) y el 10% de valores más grandes para esta distribución. El algoritmo de inversión (optimizado) para simular una \\(\\exp\\left(1\\right)\\) es: Generar \\(U\\sim U\\left( 0,1\\right)\\). Hacer \\(X=-\\ln U\\). Dado que, en principio, simulando diez valores \\(U_{1},U_{2},\\ldots,U_{10}\\sim U\\left( 0,1\\right)\\), no hay nada que nos garantice que las proporciones de los estratos son las deseadas (aunque sí lo serán en media). Una forma de garantizar el que obtengamos 4, 5 y 1 valores, repectivamente, en cada uno de los tres estratos, consiste en simular: 4 valores de \\(U[0.6,1)\\) para el primer estrato, 5 valores de \\(U[0.1,0.6)\\) para el segundo y uno de \\(U[0,0.1)\\) para el tercero. Otra forma de proceder consistiría en rechazar valores de \\(U\\) que caigan en uno de esos tres intervalos cuando el cupo de ese estrato esté ya lleno (lo cual no sería computacionalmente eficiente). El algoritmo con la estratificación propuesta sería como sigue: Para \\(i=1,2,\\ldots, 10\\): Generar \\(U_{i}\\): 2a. Generar \\(U\\sim U\\left( 0,1\\right)\\). 2b. Si \\(i\\leq4\\) hacer \\(U_{i} = 0.4 \\cdot U + 0.6\\). 2c. Si \\(4&lt;i\\leq9\\) hacer \\(U_{i} = 0.5 \\cdot U + 0.1\\). 2d. Si \\(i=10\\) hacer \\(U_{i} = 0.1 \\cdot U\\). Devolver \\(X_{i}=-\\ln U_{i}\\). No es difícil probar que: \\(Var\\left( X_{i}\\right) = 0.0214644\\) si \\(i=1,2,3,4\\), \\(Var\\left( X_{i}\\right) = 0.229504\\) si \\(i=5,6,7,8,9\\) y \\(Var\\left( X_{10}\\right) = 1\\). Como consecuencia: \\[Var\\left( \\overline{X}\\right) =\\frac{1}{10^{2}}\\sum_{i=1}^{10} Var\\left( X_{i} \\right) = 0.022338\\] que es bastante menor que 1 (la varianza en el caso de muestreo aleatorio simple no estratificado). Ejercicio 2.3 Integración Monte Carlo con estratificación Aproximar la integral anterior empleando la técnica de estratificación, considerando \\(k\\) subintervalos regularmente espaciados en el intervalo \\(\\left[ 0, 2 \\right]\\). ¿Como varía la reducción en la varianza dependiendo del valor de \\(k\\)? mc.integrale &lt;- function(ftn, a, b, n, k) { # Integración Monte Carlo con estratificación l &lt;- n%/%k int &lt;- seq(a, b, len=k+1) x &lt;- runif(l*k, rep(int[-(k+1)], each=l), rep(int[-1], each=l)) # l uniformes en cada uno de los intervalos [(j-1)/k , j/k] fx &lt;- sapply(x, ftn)*(b-a) return(list(valor=mean(fx), error=2*sd(fx)/sqrt(n))) # error mal calculado } set.seed(54321) res &lt;- mc.integral(ftn, a, b, 500) abline(h = teor) res ## $valor ## [1] 3.184612 ## ## $error ## [1] 0.1619886 set.seed(54321) mc.integrale(ftn, a, b, 500, 50) ## $valor ## [1] 3.193338 ## ## $error ## [1] 0.1597952 set.seed(54321) mc.integrale(ftn, a, b, 500, 100) ## $valor ## [1] 3.193927 ## ## $error ## [1] 0.1599089 De esta forma no se tiene en cuenta la variabilidad en el estrato. El tamaño de las submuestras debería incrementarse hacia el extremo superior. Ejercicio 4.2 Repetir el ejemplo anterior considerando intervalos regularmente espaciados en escala exponencial. 4.4 Variables de control En este caso se trata de sacar partido tanto a una covarianza positiva como negativa. La idea básica es emplear una variable \\(Y\\), con media conocida \\(\\mu_{Y}\\), para controlar la variable \\(X\\) (con media desconocida), de forma que ambas variables estén “suficientemente” correlacionadas. La versión “controlada” de \\(X\\) será: \\[X^{\\ast}=X+\\alpha \\left( Y-\\mu_{Y}\\right)\\] con \\(E(X^{\\ast})=E(X)=\\theta\\). Puede verse que \\(Var(X^{\\ast})=Var(X)+\\alpha^{2}Var(Y)+2\\alpha Cov(X,Y)\\) es mínima para: \\[\\alpha^{\\ast}=-\\frac{Cov(X,Y)}{Var(Y)},\\] con \\(Var(X^{\\ast}) = Var(X) \\left( 1-\\rho^{2} \\left( X, Y \\right) \\right)\\) (lo que supone una reducción del \\(100\\rho^{2}\\left( X, Y \\right) \\%\\)). En la práctica normalmente \\(\\alpha^{\\ast}\\) no es conocida. Para estimarlo se puede realizar ajuste lineal de \\(X\\) sobre \\(Y\\) (a partir de los datos simulados \\(X_{i}\\) e \\(Y_{i}\\), \\(1\\leq i\\leq n\\)): Si \\(\\hat{x}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}y\\) es la recta ajustada, con \\(\\hat{\\beta}_{1} = \\dfrac{S_{XY}}{S_{Y}^{2}}\\) y \\(\\hat{\\beta}_{0} = \\overline{X}-\\hat{\\beta}_{1}\\overline{Y}\\), la estimación sería: \\[\\hat{\\alpha}^{\\ast}=-\\hat{\\beta}_{1}\\] Adicionalmente, para aproximar \\(\\theta\\): \\[\\begin{aligned} \\hat{\\theta} &amp; =\\overline{X}^{\\ast}=\\overline{X}-\\hat{\\beta}_{1}\\left( \\overline{Y}-\\mu_{Y}\\right) \\\\ &amp; =\\hat{\\beta}_{0}+\\hat{\\beta}_{1}\\mu_{Y} \\end{aligned}\\] Si \\(\\mu_{Y}=0\\Rightarrow \\hat{\\theta}=\\overline{X}^{\\ast}=\\hat{\\beta}_{0}\\). Ejercicio 4.3 Integración Monte Carlo con variables de control Aproximar la integral anterior empleando la variable \\(U\\sim\\mathcal{U}(0,2)\\) para controlar la variable \\(e^{U}\\). Se trata de calcular la media de \\(exp(\\mathcal{U}(a,b))\\): a &lt;- 0; b &lt;- 2 teor &lt;- (exp(b)-exp(a))/(b-a) teor ## [1] 3.194528 Aproximación clásica por simulación: set.seed(54321) nsim &lt;- 1000 u &lt;- runif(nsim, a, b) expu &lt;- exp(u) mean(expu) ## [1] 3.182118 Con variable control: plot(u, expu) reg &lt;- lm(expu ~ u)$coef abline(reg, col=&#39;blue&#39;) # summary(lm(expu ~ u)) # R-squared: 0.9392 reg[1]+reg[2] # Coincidirá con la solución mean(expuc) ## (Intercept) ## 3.204933 Lo siguiente ya no sería necesario: expuc &lt;- expu - reg[2]*(u-1) mean(expuc) ## [1] 3.204933 Estimación del porcentaje de reducción en la varianza: 100*(var(expu)-var(expuc))/var(expu) ## [1] 93.91555 4.5 Números aleatorios comunes Se trataría de una técnica básica del diseño de experimentos: realizar comparaciones homogéneas (bloquear). Por ejemplo cuando se diseña un experimento para la comparación de la media de dos variables, se pueden emplear las denominadas muestras apareadas, en lugar de muestras independientes. Supóngamos que estamos interesados en las diferencias entre dos estrategias (e.g. dos estimadores): \\[E\\left( X\\right) -E\\left( Y\\right) =E\\left( X-Y\\right).\\] Para ello se generan dos secuencias \\(X_{1}\\), \\(X_{2}\\), \\(\\ldots\\), \\(X_{n}\\), e \\(Y_{1}\\), \\(Y_{2}\\), \\(\\ldots\\), \\(Y_{n}\\) y se calcula: \\[\\overline{X}-\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}\\left( X_{i}-Y_{i}\\right)\\] Si las secuencias se generan de modo independiente: \\[Var\\left( \\overline{X} - \\overline{Y} \\right) = \\frac{1}{n} \\left( Var\\left( X \\right) + Var\\left( Y \\right) \\right)\\] Si se generar las secuencias empleando la misma semilla, los datos son dependientes: \\[Cov\\left( X_{i}, Y_{i} \\right) &gt; 0\\] y tendríamos que: \\[\\begin{aligned} Var\\left( \\overline{X}-\\overline{Y}\\right) &amp; = \\frac{1}{n^{2}}\\sum_{i=1}^{N}Var\\left( X_{i}-Y_{i}\\right) = \\frac{1}{n}Var\\left( X_{i}-Y_{i}\\right) \\\\ &amp; = \\frac{1}{n}\\left( Var\\left( X_{i} \\right) + Var\\left( Y_{i} \\right) - 2Cov\\left( X_{i},Y_{i} \\right) \\right) \\\\ &amp; \\leq \\frac{1}{n}\\left( Var\\left( X_{i} \\right) + Var\\left( Y_{i}\\right) \\right) \\end{aligned}\\] En el capítulo de aplicaciones de la simulación se empleó esta técnica para comparar distribuciones de estimadores… 4.6 Ejercicios fin de práctica Ejercicio 4.4 Aproximar mediante integración Monte Carlo (clásica) la media de una distribución exponencial de parámetro \\(1/2\\):\\[I=\\int_{0}^{\\infty}\\frac{x}{2}e^{-\\frac{x}{2}}dx\\] y representar gráficamente la aproximación en función de \\(n\\). Comparar los resultados con los obtenidos empleando variables antitéticas, ¿se produce una reducción en la varianza? Nota: Puede ser recomendable emplear el método de inversión para generar las muestras (antitéticas) de la exponencial. MC clásico: nsim &lt;- 1000 lambda &lt;- 0.5 set.seed(1) x &lt;- - log(runif(nsim)) / lambda # Aprox por MC da media mean(x) # valor teor 1/lambda = 2 ## [1] 1.97439 # Aprox da precisión var(x) ## [1] 3.669456 MC con variables antitéticas: # xa &lt;- # mean(xa) # Aprox por MC da media (valor teor 1/lambda = 2) # var(xa) # Aprox da precisión supoñendo independencia # corr &lt;- cor(x1,x2) # var(xa)*(1 + corr) # Estimación varianza supoñendo dependencia Estimación del porcentaje de reducción en la varianza # 100*(var(x) - var(xa))/var(x) "],
["referencias.html", "Referencias", " Referencias Azarang, M. R. y García Dunna, E. (1996). Simulación y análisis de modelos estocásticos. McGraw-Hill. Bratley, P., Fox, B. L. y Schrage L. E. (1990). A guide to simulation. Springer-Verlag. Devroye, L. (1986). Non-uniform random variate generation. Springer-Verlag. Gentle, J.E. (1998). Random number generation and Monte Carlo methods. Springer-Verlag. Law, A.M. y Kelton, W.D. (1991). Simulation, modeling and analysis. McGraw-Hill. Moeschlin, O., Grycko, E., Pohl, C. y Steinert, F. (1998). Experimental stochastics. Springer-Verlag. Nelson, R. (1995). Probability, stochastic processes, and queueing theory: the mathematics of computer performance modelling. Springer-Verlag. Pardo, L. y Valdés, T. (1987). Simulación. Aplicaciones prácticas a la empresa. Díaz de Santos. Ross, S. M. (1999). Simulación. Prentice Hall "]
]
