[
["index.html", "Prácticas de Simulación Prólogo", " Prácticas de Simulación Rubén F. Casal (ruben.fcasal@udc.es) 2017-10-09 Prólogo Este libro contiene algunas de las prácticas de la asignatura de Simulación Estadística del Máster interuniversitario en Técnicas Estadísticas). Este libro ha sido escrito en R-Markdown empleando el paquete bookdown y está disponible en el repositorio Github: rubenfcasal/simbook. Para generar el libro (compilar) puede ser recomendable instalar la última versión de RStudio y la versión de desarrollo de bookdown disponible en Github: devtools::install_github(&quot;rstudio/bookdown&quot;) Este obra está bajo una licencia de Creative Commons Reconocimiento-NoComercial-SinObraDerivada 4.0 Internacional (espero poder liberarlo bajo una licencia menos restrictiva más adelante…). "],
["introduccion-a-la-simulacion.html", "Capítulo 1 Introducción a la simulación 1.1 Introducción 1.2 Generación de números (pseudo)aleatorios 1.3 Números aleatorios puros 1.4 Números pseudoaleatorios", " Capítulo 1 Introducción a la simulación 1.1 Introducción Problemas de la experimentación directa sobre la realidad: Coste elevado. En ocasiones las pruebas son destructivas. Lentitud. Puede no ser ética. experimentación sobre seres humanos. Puede resultar imposible. Acontecimientos futuros, … … Puede ser preferible trabajar con un modelo del sistema real. La realidad puede ser muy compleja por lo que es habitual emplear un modelo para tratar de explicarla: Modelos deterministas. Modelos estocásticos (con componente aleatoria). Cuando no se dispone de la suficiente información sobre las variables que influyen en el fenómeno en estudio. Se tienen en cuenta esta incertidumbre. La inferencia estadística proporciona herramientas para estimar los parámetros y contrastar la validez de un modelo estocástico a partir de los datos observados. La idea es emplear el modelo para resolver el problema de interés. Cuando la solución no se puede obtener de modo analítico (teórico) se puede recurrir a la simulación. Simulación: realizar experimentos con un modelo con el objetivo de recopilar información bajo determinadas condiciones. Nos centraremos en el caso de la simulación estocástica: Las conclusiones se obtienen habitualmente generando repetidamente simulaciones del modelo aleatorio. 1.1.1 Ventajas de la simulación (Shannon, 1975): Cuando la resolución analítica no puede llevarse a cabo. Cuando existen medios de resolver analíticamente el problema pero dicha resolución es complicada y costosa (o solo proporciona una solución aproximada). Si se desea experimentar antes de que exista el sistema (pruebas para la construcción de un sistema). Cuando es imposible experimentar sobre el sistema real por ser dicha experimentación destructiva. En ocasiones en las que la experimentación sobre el sistema es posible pero no ética. En sistemas que evolucionan muy lentamente en el tiempo. 1.1.2 Inconvenientes de la simulación: La construcción de un buen modelo puede ser una tarea muy costosa (compleja, laboriosa y requerir mucho tiempo; e.g. modelos climáticos). Frecuentemente el modelo omite variables o relaciones importantes entre ellas (los resultados pueden no ser válidos para el sistema real). Resulta difícil conocer la precisión de la simulación (especialmente en lo relativo a la precisión del modelo formulado). Problemas de extrapolación de las conclusiones. Tiempo de computación. 1.2 Generación de números (pseudo)aleatorios Aplicaciones: Estadística: Muestreo, remuestreo, … Aproximación de distribuciones (de estadísticos, estimadores, …) Realización de contrastes, intervalos de confianza, … Comparación de estimadores, contrastes, … Validación teoría (distribución asintótica,…) Inferencia Bayesiana Optimización: Algoritmos genéticos, … Computación: Diseño, verificación y validación de algoritmos,… Criptografía: Protocolos de comunicación segura, … Física: Simulación de fenómenos naturales, … Análisis numérico: Evaluación de expresiones, … … En el Capítulo XX nos centraremos en algunas de las aplicaciones de utilidad en Estadística. 1.3 Números aleatorios puros Una sucesión de números aleatorios puros (true random), se caracteriza por que no existe ninguna regla o plan que nos permita conocer sus valores. Se almacena(ban) en tablas de dígitos aleatorios (true random) y normalmente son obtenidos por procesos físicos (loterías, ruletas, ruidos,…) Se emplean para seleccionar números aleatorios en un rango de 1 a m: Se selecciona al azar un pto de inicio en la tabla y una dirección. Se agrupan los dígitos de forma que “cubran” el valor de m. Se seleccionan los valores menores o iguales que m (se descarta el resto). Algunos enlaces: A Million Random Digits with 100,000 Normal Deviates . RAND Corporation. 1955. Generadores de números aleatorios “online”: http://www.random.org/integers (ver paquete random en R). http://www.fourmilab.ch/hotbits Generadores mediante hardware: http://software.intel.com. http://spectrum.ieee.org Sus principales aplicaciones hoy en día son en criptografía (impredictibilidad). 1.3.1 Inconvenientes: Es necesario/recomendable conocer su distribución. Los datos generados deberían ser i.i.d. Reproductivilidad. Pueden requerir del almacenamiento en tablas. 1.3.2 Alternativas: números pseudo-aleatorios: simulan realizaciones de una variable aleatoria (uniforme). números cuasi-aleatorios: secuencias determinísticas con una distribución más uniforme en el rango considerado (se podría pensar que son una única generación de una variable aleatoria). 1.4 Números pseudoaleatorios 1.4.1 Generación de números pseudoaleatorios mediante software La mayoría de los métodos de simulación se basan en la posibilidad de generar números pseudoaleatorios con distribución \\(\\mathcal{U}(0,1)\\). Se obtienen mediante un algoritmo recursivo denominado generador: \\[x_{i}=f\\left( x_{i-1},x_{i-2},\\cdots,x_{i-k}\\right)\\] \\(k\\) orden del generador. \\(\\left( x_{0},x_{1},\\cdots,x_{k-1}\\right)\\) semilla (estado inicial). El periodo o longitud del ciclo es la longitud de la secuencia antes de que vuelva a repetirse. Lo denotaremos por \\(p\\). Los números de la sucesión serán predecibles, conociendo el algoritmo y la semilla. Sin embargo, si no se conociesen, no se debería poder distinguir una serie de números pseudoaleatorios de una sucesión de números verdaderamente aleatoria (utilizando recursos computacionales razonables). En caso contrario esta predecibilidad puede dar lugar a serios problemas (e.g. http://eprint.iacr.org/2007/419). Como regla general, por lo menos mientras se está desarrollando un programa, interesa fijar la semilla de aleatorización. Permite la reproductibilidad de los resultados. Facilita la depuración del código. Todo generador de números pseudoaleatorios mínimamente aceptable debe comportarse como si se tratase de una muestra genuina de datos independientes de una \\(\\mathcal{U}(0,1)\\). Otras propiedades de interés serían: Reproducibilidad a partir de la semilla. Periodo suficientemente largo. Eficiencia (rapidez y requerimientos de memoria). Portabilidad. Generación de sub-secuencias (computación en paralelo). Parsimonia, … Gran cantidad de algoritmos: Cuadrados medios, Lehmer,… Congruenciales Registros desfasados Combinaciones … Código fuente disponible en múltiples librerias: GNU Scientific Library (GSL): http://www.gnu.org/software/gsl/manual StatLib: http://lib.stat.cmu.edu Numerical recipes: http://www.nrbook.com/nr3 http://random.mat.sbg.ac.at/software KISS (Keep It Simple Stupid / Small and Simple): http://www.fortran.com/kiss.f90 UNU.RAN (paquete Runuran): http://statmath.wu.ac.at/unuran … Nos centraremos en los generadores congruenciales, descritos en la Sección 3.1. "],
["numeros-aleatorios-en-r.html", "Capítulo 2 Números aleatorios en R 2.1 Opciones 2.2 Paquetes de R 2.3 Ejercicios 2.4 Tiempo de CPU", " Capítulo 2 Números aleatorios en R La generación de números pseudoaleatorios en R es una de las mejores disponibles en paquetes estadísticos. Entre las herramientas en el paquete base de R estarían: set.seed(entero): permite establecer la semilla (y el generador). RNGkind: selecciona el generador. rdistribución(n,...): genera valores aleatorios de la correspondiente distribución. Por ejemplo: runif(n, min = 0, max = 1), generaría n valores de una uniforme. sample: genera muestras aleatorias (v.a. discretas) y permutaciones. La semilla se almacena (en globalenv) en .Random.seed; es un vector de enteros cuya dimensión depende del tipo de generador: No debe ser modificado manualmente; se guarda con el entorno de trabajo. Si no se especifica con set.seed (o no existe) se genera a partir del reloj del sistema. Nota: Puede ser recomendable (para depurar) almacenarla antes de generar simulaciones, e.g. semilla &lt;- .Random.seed. 2.1 Opciones La función RNGkind(kind = NULL, normal.kind = NULL) permite seleccionar el tipo de generador (en negrita los valores por defecto): kindespecifica el generador aleatorio: “Wichmann-Hill”: Ciclo \\(6.9536\\times10^{12}\\) “Marsaglia-Multicarry”: Ciclo mayor de \\(2^{60}\\) “Super-Duper”: Ciclo aprox. \\(4.6\\times10^{18}\\) (S-PLUS) “Mersenne-Twister”: Ciclo \\(2^{19937}-1\\) y equidistribution en 623 dim. “Knuth-TAOCP-2002”: Ciclo aprox. \\(2^{129}\\). “Knuth-TAOCP” “user-supplied” normal.kindselecciona el método de generación de normales (se tratará más adelante). “Kinderman-Ramage”, “Buggy Kinderman-Ramage”, “Ahrens-Dieter”, “Box-Muller”, “Inversion” , o “user-supplied” 2.2 Paquetes de R Otros paquetes de R que pueden ser de interés: setRNG contiene herramientas que facilitan operar con la semilla (dentro de funciones,…). random generación de numeros “true random”. randtoolbox implementa generadores más recientes (rngWELL) y generación de secuencias cuasi-aleatorias. RDieHarder implementa diversos contrastes para el análisis de la calidad de un generador y varios generadores. Runuran interfaz para la librería UNU.RAN para la generación (automática) de variables aleatorias no uniformes. rsprng y rstream implementan la generación de múltiples secuencias (e.g. para programación paralela). gls, rngwell19937, randaes, SuppDists, lhs, mc2d, fOptions, … 2.3 Ejercicios Ejercicio 2.1 Sea \\((X,Y)\\) es un vector aleatorio con distribución uniforme en el cuadrado \\([-1,1]\\times\\lbrack-1,1]\\) de área 4. Aproximar mediante simulación \\(P\\left(X + Y \\leq 0 \\right)\\) y compararla con la probabilidad teórica (obtenida aplicando la regla de Laplace \\(\\frac{\\text{área favorable}}{\\text{área posible}}\\)). Probabilidad teórica 1/2 (area favorable / área total) set.seed(1) n &lt;- 10000 x &lt;- runif(n, -1, 1) y &lt;- runif(n, -1, 1) indice &lt;- (x+y &lt; 0) # Aproximación por simulación sum(indice)/n ## [1] 0.4996 mean(indice) # Alternativa ## [1] 0.4996 Nota: R maneja internamente los valores lógicos como 1 (TRUE) y 0 (FALSE). Aproximar el valor de \\(\\pi\\) mediante simulación a partir de \\(P\\left( X^2 +Y^2 \\leq 1 \\right)\\). set.seed(1) n &lt;- 10000 x &lt;- runif(n, -1, 1) y &lt;- runif(n, -1, 1) indice &lt;- (x^2+y^2 &lt; 1) mean(indice) ## [1] 0.7806 pi/4 ## [1] 0.7853982 pi_aprox &lt;- 4*mean(indice) pi_aprox ## [1] 3.1224 Gráfico # Colores y símbolos depediendo de si el índice correspondiente es verdadero: color &lt;- ifelse(indice, &quot;black&quot;, &quot;red&quot;) simbolo &lt;- ifelse(indice, 1, 4) plot(x, y, pch = simbolo, col = color, xlim = c(-1, 1), ylim = c(-1, 1), xlab=&quot;X&quot;, ylab=&quot;Y&quot;, asp = 1) # asp = 1 para dibujar circulo symbols(0, 0, circles = 1, inches = FALSE, add = TRUE) symbols(0, 0, squares = 2, inches = FALSE, add = TRUE) Ejercicio 2.2 Consideramos el experimento de Bernoulli consistente en el lanzamiento de una moneda. Empleando la función sample, obtener 1000 simulaciones del lanzamiento de una moneda (0 = cruz, 1 = cara), suponiendo que no está trucada. Aproximar la probabilidad de cara a partir de las simulaciones. set.seed(1) nsim &lt;- 10000 x &lt;- sample(c(cara = 1, cruz = 0), nsim, replace = TRUE, prob = c(0.5,0.5)) mean(x) ## [1] 0.4953 barplot(100*table(x)/nsim) # Representar porcentajes En R pueden generarse valores de la distribución de Bernoulli mediante la función rbinom(nsim, size=1, prob). Generar un gráfico de lineas considerando en el eje \\(X\\) el número de lanzamientos (de 1 a 10000) y en el eje \\(Y\\) la frecuencia relativa del suceso cara (puede ser recomendable emplear la función cumsum). set.seed(1) nsim &lt;- 1000 p &lt;- 0.4 x &lt;- rbinom(nsim, size = 1, prob = p) # Simulamos una Bernouilli n &lt;- 1:nsim # Alternativa programación: x &lt;- runif(nsim) &lt; p mean(x) ## [1] 0.394 plot(n, cumsum(x)/n, type=&quot;l&quot;, ylab=&quot;Proporción de caras&quot;, xlab=&quot;Número de lanzamientos&quot;, ylim=c(0,1)) abline(h=p, lty=2, col=&quot;red&quot;) Ejercicio 2.3 Simular el paso de corriente a través del siguiente circuito, donde figuran las probabilidades de que pase corriente por cada uno de los interruptores: Considerar que cada interruptor es una v.a. de Bernoulli independiente para simular 1000 valores de cada una de ellas. Nota: R maneja internamente los valores lógicos como 1 (TRUE) y 0 (FALSE). Recíprocamente, cualquier nº puede ser tratado como lógico (al estilo de C). El entero 0 es equivalente a FALSE y cualquier entero distinto de 0 a TRUE. set.seed(1) nsim &lt;- 10000 x1 &lt;- rbinom(nsim, size=1, prob=0.9) x2 &lt;- rbinom(nsim, size=1, prob=0.8) z1 &lt;- x1 | x2 # Operador lógico &quot;O&quot; x3 &lt;- rbinom(nsim, size=1, prob=0.6) x4 &lt;- rbinom(nsim, size=1, prob=0.5) z2 &lt;- x3 | x4 z3 &lt;- z1 | z2 x5 &lt;- rbinom(nsim, size=1, prob=0.7) fin &lt;- z3 &amp; x5 # Operador lógico &quot;Y&quot; mean(fin) ## [1] 0.6918 Ejercicio 2.4 En 1651, el Caballero de Méré le planteó a Pascal una pregunta relacionada con las apuestas y los juegos de azar: ¿es ventajoso apostar a que en cuatro lanzamientos de un dado se obtiene al menos un seis? Este problema generó una fructífera correspondencia entre Pascal y Fermat que se considera, simbólicamente, como el nacimiento del Cálculo de Probabilidades. Escribir una función que simule el lanzamiento de \\(n\\) dados. El parámetro de entrada es el número de lanzamientos \\(n\\), que toma el valor 4 por defecto, y la salida debe ser TRUE si se obtiene al menos un 6 y FALSE en caso contrario. deMere &lt;- function(n = 4){ lanz &lt;- sample(1:6, replace=TRUE, size=n) return(6 %in% lanz) } n &lt;- 4 lanz &lt;- sample(1:6, replace=TRUE, size=n) lanz ## [1] 4 5 3 1 6 %in% lanz ## [1] FALSE Utilizar la función anterior para simular \\(nsim=10000\\) jugadas de este juego y calcular la proporción de veces que se gana la apuesta (obtener al menos un 6 en \\(n\\) lanzamientos), usando \\(n=4\\). Comparar el resultado con la probabilidad teórica \\(1-(5/6)^{n}\\). set.seed(1) n &lt;- 4 nsim &lt;- 10000 mean(replicate(nsim, deMere(n))) ## [1] 0.5195 1-(5/6)^n ## [1] 0.5177469 2.4 Tiempo de CPU La velocidad del generador suele ser una característica importante. Para evaluar el rendimiento están disponibles en R distintas herramientas: proc.time(): permite obtener tiempo de computación real y de CPU. tini &lt;- proc.time() # Código a evaluar tiempo &lt;- proc.time() - tini system.time(expresión): muestra el tiempo de computación (real y de CPU) de expresión. Rprof(fichero): permite evaluar el rendimiento muestreando la pila en intervalos para determinar en que funciones se emplea el tiempo de computación (de utilidad para optimizar la velocidad).Rprof(NULL): desactiva el muestreo.summaryRprof(fichero): muestra los resultados. 2.4.1 Utilidades tiempo de CPU Por ejemplo, podríamos emplear las siguientes funciones para ir midiendo los tiempos de CPU durante una simulación: CPUtimeini &lt;- function() { .tiempo.ini &lt;&lt;- proc.time() .tiempo.last &lt;&lt;- .tiempo.ini } CPUtimeprint &lt;- function() { tmp &lt;- proc.time() cat(&quot;\\nTiempo última operación:\\n&quot;) print(tmp-.tiempo.last) cat(&quot;Tiempo total operación:\\n&quot;) print(tmp-.tiempo.ini) .tiempo.last &lt;&lt;- tmp } Llamando a CPUtimeini() donde se quiere empezar a contar, y a CPUtimeprint() para imprimir el tiempo total y el tiempo desde la última llamada a una de estas funciones. Ejemplo: funtest &lt;- function(n) median(runif(n)) # Función de prueba... CPUtimeini() funtest(1000000) ## [1] 0.5004536 CPUtimeprint() ## ## Tiempo última operación: ## user system elapsed ## 0.20 0.09 0.21 ## Tiempo total operación: ## user system elapsed ## 0.20 0.09 0.21 funtest(1000) ## [1] 0.4954613 CPUtimeprint() ## ## Tiempo última operación: ## user system elapsed ## 0 0 0 ## Tiempo total operación: ## user system elapsed ## 0.20 0.09 0.21 2.4.2 Paquetes de R Por ejemplo, se puede emplear el paquete npsp (fichero cpu.time.R): Call cpu.time(restart = TRUE) where you want to start counting. Call cpu.time() to print/get total and/or partial (since the last call to this function) real and CPU times. # CPU time utilities # ------------------ #&#39; @rdname npsp-internals #&#39; @keywords internal #&#39; @export .cpu.time.ini &lt;- function() { time.ini &lt;- structure(rep(0, 5), .Names = c(&quot;user.self&quot;, &quot;sys.self&quot;, &quot;elapsed&quot;, &quot;user.child&quot;, &quot;sys.child&quot;), class = &quot;proc_time&quot;)# proc.time() time.last &lt;- time.ini function(..., reset = FALSE, total = TRUE, last = TRUE, flush = FALSE) { res &lt;- list(time = proc.time()) if (reset) { time.ini &lt;&lt;- res$time time.last &lt;&lt;- time.ini res$last &lt;- res$total &lt;- 0 if (total | last) cat(&quot;CPU time has been initialized.\\n&quot;) } else { res$last &lt;- res$time - time.last res$total &lt;- res$time - time.ini if (last) { cat(&quot;Time of last operation:&quot;, ..., &quot;\\n&quot;) print(res$last) } if (total) { cat(&quot;Total time:\\n&quot;) print(res$total) } if (flush) flush.console() time.last &lt;&lt;- res$time } return(invisible(res)) } } #&#39; Total and partial CPU time used #&#39; #&#39; Returns and (optionally) prints the total and/or partial #&#39; (since the last call to this function) #&#39; real and CPU times. #&#39; @param ... objects (describing the last operation) to be printed #&#39; (using \\code{\\link{cat}}), #&#39; if \\code{last == TRUE}. #&#39; @param reset logical; if \\code{TRUE}, time counters are initialized. #&#39; @param total logical; if \\code{TRUE}, the total time used is printed. #&#39; @param last logical; if \\code{TRUE}, the partial time used is printed. #&#39; @param flush logical; if \\code{TRUE}, \\code{\\link{flush.console}} is called. #&#39; @return Invisibly returns a list with the following 3 components #&#39; (objects of class \\code{&quot;proc_time&quot;}): #&#39; \\item{time}{user, system, and total elapsed times for the currently running R process #&#39; (result of a call to \\code{\\link{proc.time}}). } #&#39; \\item{last, total}{differences between the corresponding \\code{\\link{proc.time}} calls.} #&#39; @seealso #&#39; \\code{\\link{proc.time}}, \\code{\\link{system.time}}, \\code{\\link{flush.console}}. #&#39; @export cpu.time &lt;- .cpu.time.ini() Ejemplo: cpu.time(reset = TRUE) ## CPU time has been initialized. res &lt;- funtest(1000000) cpu.time(&#39;\\nSample median of&#39;, 1000000, &#39;values =&#39;, res, total = FALSE) ## Time of last operation: ## Sample median of 1e+06 values = 0.4993793 ## user system elapsed ## 0.18 0.06 0.21 res &lt;- funtest(1000) cpu.time(&#39;\\nSample median of&#39;, 1000, &#39;values =&#39;, res) ## Time of last operation: ## Sample median of 1000 values = 0.4936829 ## user system elapsed ## 0 0 0 ## Total time: ## user system elapsed ## 0.18 0.06 0.21 Otro paquete que puede ser de utilidad es microbenchmark (si se quieren estudiar con más detalle los tiempos de computación; aunque en este libro no será el caso…). Hay que tener en cuenta que, por construcción, aunque se realicen en la mismas condiciones (en el mismo equipo), los tiempos de CPU en R pueden variar “ligeramente” entre ejecuciones. "],
["generacion-de-numeros-pseudoaleatorios-1.html", "Capítulo 3 Generación de números pseudoaleatorios 3.1 Generadores congruenciales 3.2 Análisis de la calidad de un generador 3.3 Ejercicios de fin de práctica", " Capítulo 3 Generación de números pseudoaleatorios 3.1 Generadores congruenciales 3.1.1 Generador congruencial lineal (simple) Partiendo de una semilla inicial \\(x_{0}\\): \\[\\begin{aligned} x_{i} &amp; =(ax_{i-1}+c)\\operatorname{mod}m\\\\ u_{i} &amp; =\\dfrac{x_{i}}{m}\\\\ i &amp; =1,2,\\ldots \\end{aligned}\\] donde \\(a\\) es el multiplicador, \\(c\\) el incremento y \\(m\\) el módulo del generador (parámetros enteros fijados de antemano). Si \\(c=0\\) se denomina congruencial multiplicativo Si \\(c\\neq0\\) se denomina congruencial mixto Implementado en el fichero RANDC.R (tratando de imitar el funcionamiento en R, aunque de forma no muy eficiente…): # Generador congruencial de números pseudoaleatorios # ================================================== # -------------------------------------------------- # initRANDC(semilla,a,c,m) # Selecciona el generador congruencial # Por defecto RANDU de IBM con semilla del reloj # OJO: No se hace ninguna verificación de los parámetros initRANDC &lt;- function(semilla=as.numeric(Sys.time()), a=2^16+3, c=0, m=2^31) { .semilla &lt;&lt;- as.double(semilla) %% m #Cálculos en doble precisión .a &lt;&lt;- a .c &lt;&lt;- c .m &lt;&lt;- m return(invisible(list(semilla=.semilla,a=.a,c=.c,m=.m))) #print(initRANDC()) } # -------------------------------------------------- # RANDC() # Genera un valor pseudoaleatorio con el generador congruencial # Actualiza la semilla (si no existe llama a initRANDC) RANDC &lt;- function() { if (!exists(&quot;.semilla&quot;, envir=globalenv())) initRANDC() .semilla &lt;&lt;- (.a * .semilla + .c) %% .m return(.semilla/.m) } # -------------------------------------------------- # RANDCN(n) # Genera un vector de valores pseudoaleatorios con el generador congruencial # (por defecto de dimensión 1000) # Actualiza la semilla (si no existe llama a initRANDC) RANDCN &lt;- function(n=1000) { x &lt;- numeric(n) for(i in 1:n) x[i]&lt;-RANDC() return(x) # return(replicate(n,RANDC())) # Alternativa más rápida } initRANDC(543210) # Fijar semilla 543210 para reproductibilidad Nota: Para evitar problemas computacionales, se recomienda emplear un algoritmo como el descrito en L’Ecuyer (1988). Ejemplos: \\(c=0\\), \\(a=2^{16}+3=65539\\) y \\(m=2^{31}\\), generador RANDU de IBM (no recomendable). \\(c=0\\), \\(a=7^{5}=16807\\) y \\(m=2^{31}-1,\\) Park y Miller (1988) “minimal standar”,empleado por las librerías IMSL y NAG. Los parámetros y la semilla determinan los valores generados: \\[x_{i}=\\left( a^{i}x_{0}+c\\frac{a^{i}-1}{a-1}\\right) \\operatorname{mod}m\\] A pesar de su simplicidad, una adecuada elección de los parámetros permite obtener de manera eficiente secuencias de números “aparentemente” i.i.d. \\(\\mathcal{U}(0,1).\\) 3.1.2 Periodo Teorema 3.1 (Hull y Dobell, 1962) Un generador congruencial tiene período máximo (\\(p=m\\)) si y solo si: \\(c\\) y \\(m\\) son primos relativos (i.e. \\(m.c.d.\\left( c,m\\right) =1\\)). \\(a-1\\) es múltiplo de todos los factores primos de \\(m\\) (i.e. \\(a\\equiv1\\operatorname{mod}q\\), para todo \\(q\\) factor primo de \\(m\\)). Si \\(m\\) es múltiplo de \\(4\\), entonces \\(a-1\\) también lo ha de ser (i.e. \\(m\\equiv0\\operatorname{mod}4\\Rightarrow a\\equiv 1\\operatorname{mod}4\\)). . Algunas consecuencias: Si \\(m\\) primo, \\(p=m\\Leftrightarrow a=1\\) Un generador multiplicativo no cumple la condición 1. Teorema 3.2 Un generador multiplicativo tiene período máximo (\\(p=m-1\\)) si: \\(m\\) es primo. \\(a\\) es una raiz primitiva de \\(m\\)(i.e. el menor entero \\(q\\) tal que \\(a^{q}=1\\operatorname{mod}m\\) es \\(q=m-1\\)). . Además de preocuparse de la longitud del ciclo, las secuencias generadas deben aparentar muestras i.i.d. \\(\\mathcal{U}(0,1)\\). Por ejemplo, los valores generados pueden mostrar una estructura reticular. Marsaglia (1968): \\(k\\)-uplas de generadores multiplicativos contenidas en a lo sumo \\(\\left(k!m\\right)^{1/k}\\) hiperplanos paralelos. Generador RANDU de IBM (70’s): library(rgl) system.time(u &lt;- RANDCN(9999)) # Generar xyz &lt;- matrix(u, ncol = 3, byrow = TRUE) plot3d(xyz) # rglwidget() Se han propuesto diversas pruebas (ver sección siguiente) para determinar si un generador tiene problemas de este tipo y se han realizado numerosos estudios para determinadas familias (e.g. Park y Miller, 1988, \\(m=2^{31}-1\\)). En cualquier caso, se recomienda considerar un “periodo de seguridad” \\(\\approx \\sqrt{p}\\) para evitar este tipo de problemas. Aunque estos generadores tiene limitaciones en su capacidad para producir secuencias muy largas de números i.i.d. \\(\\mathcal{U}(0,1)\\), es un elemento básico en generadores más avanzados. 3.1.3 Otros generadores Generalizaciones del generador congruencial lineal simple: \\[x_{i}=f\\left( x_{i-1},x_{i-2},\\cdots,x_{i-k}\\right) \\operatorname{mod}m\\] no lineal: e.g. \\(\\ x_{i}=a_{1}x_{i-1}+a_{2}x_{i-1}^{2}\\operatorname{mod}m\\) lineal múltiple: \\(x_{i}=a_{1}x_{i-1}+a_{2}x_{i-2}+\\cdots+a_{k}x_{i-k}\\operatorname{mod}m\\), con \\(p\\leq m^{k}-1\\)) matricial: \\(\\boldsymbol{x}_{i} = A_{1}\\boldsymbol{x}_{i-1} + A_{2}\\boldsymbol{x}_{i-2} + \\cdots + A_{k}\\boldsymbol{x}_{i-k} \\operatorname{mod}m\\) (por ejemplo el generador por defecto de R). Generadores de registros desfasados: generadores de bits basados en el lineal múltiple \\(m=2\\); \\(a_{i},x_{i}\\in \\left \\{ 0,1\\right \\}\\) (cálculos mediante operaciones lógicas). Combinación de generadores: Combinación de salidas: \\(u_{i}=\\sum u_{i}^{(l)}\\operatorname{mod}1\\) Barajar salidas… Ejercicio 2.2 Considera el generador congruencial definido por: \\[\\begin{aligned} x_{n+1} &amp; =(5x_{n}+1)\\ \\operatorname{mod}\\ 512,\\nonumber\\\\ u_{n+1} &amp; =\\frac{x_{n+1}}{512},\\ n=0,1,\\dots\\nonumber\\end{aligned}\\] (de ciclo máximo). Algoritmo implementado en el fichero RANDC.R: Generar 500 valores de este generador, obtener el tiempo de CPU, representar su distribución mediante un histograma (en escala de densidades) y compararla con la densidad teórica. initRANDC(321, 5, 1, 512) # Fijar semilla para reproductibilidad nsim &lt;- 500 system.time(u &lt;- RANDCN(nsim)) # Generar ## user system elapsed ## 0 0 0 hist(u, freq = FALSE) abline(h = 1) # curve(dunif(x, 0, 1), add = TRUE) Calcular la media de las simulaciones (mean) y compararla con la teórica. La aproximación por simulación de la media teórica es: mean(u) ## [1] 0.4999609 La media teórica es 0.5. Error absoluto \\(3.90625\\times 10^{-5}\\). Aproximar (mediante simulación) la probabilidad del intervalo \\((0.4;0.8)\\) y compararla con la teórica. La probabilidad teórica es 0.8 - 0.4 = 0.4 La aproximación mediante simulación: sum((0.4 &lt; u) &amp; (u &lt; 0.8))/nsim ## [1] 0.402 mean((0.4 &lt; u) &amp; (u &lt; 0.8)) # Alternativa ## [1] 0.402 3.2 Análisis de la calidad de un generador Para verificar si un generador tiene las propiedades estadísticas deseadas hay disponibles una gran cantidad de test de hipótesis (baterías de contrastes) y métodos gráficos: Contrastes genéricos de bondad de ajuste y aleatoriedad. Contrastes específicos para generadores aleatorios. Se trata principalmente de contrastar si las muestras generadas son i.i.d. \\(\\mathcal{U}\\left(0,1\\right)\\) (análisis univariante). Aunque los métodos más avanzados tratan normalmente de contrastar si las \\(k\\)-uplas: \\[(U_{t+1},U_{t+2},...,U_{t+k-1}); \\ t=(i-1)k, \\ i=1,...,m\\] son i.i.d. \\(\\mathcal{U}\\left(0,1\\right)^{k}\\) (uniformes independientes en el hipercubo; análisis multivariante). Nos centraremos en los métodos genéricos. Pueden usarse en: Evaluación de generadores aleatorios Evaluación de generadores de variables aleatorias Modelado de entradas de modelos de simulación Uno de los contrastes más conocidos es el test ji-cuadrado de bondad de ajuste (chisq.test para el caso discreto). Aunque si la variable de interés es continua, habría que discretizarla (con la correspondiente perdida de información). Por ejemplo, se podría emplear la siguiente función (que imita a las incluídas en R): #------------------------------------------------------------------------------- # chisq.test.cont(x, distribution, nclasses, output, nestpar,...) #------------------------------------------------------------------------------- # Realiza el test ji-cuadrado de bondad de ajuste para una distribución continua # discretizando en intervalos equiprobables. # Parámetros: # distribution = &quot;norm&quot;,&quot;unif&quot;,etc # nclasses = floor(length(x)/5) # output = TRUE # nestpar = 0= nº de parámetros estimados # ... = parámetros distribución # Ejemplo: # chisq.test.cont(x, distribution=&quot;norm&quot;, nestpar=2, mean=mean(x), sd=sqrt((nx-1)/nx)*sd(x)) #------------------------------------------------------------------------------- chisq.test.cont &lt;- function(x, distribution = &quot;norm&quot;, nclasses = floor(length(x)/5), output = TRUE, nestpar = 0, ...) { # Funciones distribución q.distrib &lt;- eval(parse(text = paste(&quot;q&quot;, distribution, sep = &quot;&quot;))) d.distrib &lt;- eval(parse(text = paste(&quot;d&quot;, distribution, sep = &quot;&quot;))) # Puntos de corte q &lt;- q.distrib((1:(nclasses - 1))/nclasses, ...) tol &lt;- sqrt(.Machine$double.eps) xbreaks &lt;- c(min(x) - tol, q, max(x) + tol) # Gráficos y frecuencias if (output) { xhist &lt;- hist(x, breaks = xbreaks, freq = FALSE, lty = 2, border = &quot;grey50&quot;) curve(d.distrib(x, ...), add = TRUE) } else { xhist &lt;- hist(x, breaks = xbreaks, plot = FALSE) } # Cálculo estadístico y p-valor O &lt;- xhist$counts # Equivalente a table(cut(x, xbreaks)) pero más eficiente E &lt;- length(x)/nclasses DNAME &lt;- deparse(substitute(x)) METHOD &lt;- &quot;Pearson&#39;s Chi-squared test&quot; STATISTIC &lt;- sum((O - E)^2/E) names(STATISTIC) &lt;- &quot;X-squared&quot; PARAMETER &lt;- nclasses - nestpar - 1 names(PARAMETER) &lt;- &quot;df&quot; PVAL &lt;- pchisq(STATISTIC, PARAMETER, lower.tail = FALSE) # Preparar resultados classes &lt;- format(xbreaks) classes &lt;- paste(&quot;(&quot;, classes[-(nclasses + 1)], &quot;,&quot;, classes[-1], &quot;]&quot;, sep = &quot;&quot;) RESULTS &lt;- list(classes = classes, observed = O, expected = E, residuals = (O - E)/sqrt(E)) if (output) { cat(&quot;\\nPearson&#39;s Chi-squared test table\\n&quot;) print(as.data.frame(RESULTS)) } if (any(E &lt; 5)) warning(&quot;Chi-squared approximation may be incorrect&quot;) structure(c(list(statistic = STATISTIC, parameter = PARAMETER, p.value = PVAL, method = METHOD, data.name = DNAME), RESULTS), class = &quot;htest&quot;) } Por ejemplo, continuando con el generador congruencial anterior, obtendríamos: chisq.test.cont(u, distribution = &quot;unif&quot;, nclasses = 10, nestpar = 0, min = 0, max = 1) ## ## Pearson&#39;s Chi-squared test table ## classes observed expected residuals ## 1 (-1.490116e-08, 1.000000e-01] 51 50 0.1414214 ## 2 ( 1.000000e-01, 2.000000e-01] 49 50 -0.1414214 ## 3 ( 2.000000e-01, 3.000000e-01] 49 50 -0.1414214 ## 4 ( 3.000000e-01, 4.000000e-01] 50 50 0.0000000 ## 5 ( 4.000000e-01, 5.000000e-01] 51 50 0.1414214 ## 6 ( 5.000000e-01, 6.000000e-01] 51 50 0.1414214 ## 7 ( 6.000000e-01, 7.000000e-01] 49 50 -0.1414214 ## 8 ( 7.000000e-01, 8.000000e-01] 50 50 0.0000000 ## 9 ( 8.000000e-01, 9.000000e-01] 50 50 0.0000000 ## 10 ( 9.000000e-01, 9.980469e-01] 50 50 0.0000000 ## ## Pearson&#39;s Chi-squared test ## ## data: u ## X-squared = 0.12, df = 9, p-value = 1 Importante: Empleando los métodos genéricos del modo habitual, desconfiamos del generador si la muestra/secuencia no se ajusta a la distribución teórica (p-valor \\(\\leq \\alpha\\)). En este caso además, también se sospecha si se ajusta demasiado bien a la distribución teórica (p-valor \\(\\geq1-\\alpha\\)). Otro contraste de bondad de ajuste muy conocido es el test de Kolmogorov-Smirnov, implementado en ks.test. Ejercicio 2.4 Continuando con el generador congruencial anterior: initRANDC(321, 5, 1, 512) # Fijar semilla para reproductibilidad nsim &lt;- 500 system.time(u &lt;- RANDCN(nsim)) # Generar Realizar el contraste de Kolmogorov-Smirnov para estudiar el ajuste a una \\(\\mathcal{U}(0,1)\\). # Distribución empírica curve(ecdf(u)(x), type = &quot;s&quot;, lwd = 2) curve(punif(x, 0, 1), add = TRUE) # Test de Kolmogorov-Smirnov ks.test(u, &quot;punif&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: u ## D = 0.0033281, p-value = 1 ## alternative hypothesis: two-sided Obtener el gráfico secuencial y el de dispersión retardado, ¿se observa algún problema? Gráfico secuencial: plot(as.ts(u)) Gráfico de dispersión retardado: plot(u[-nsim],u[-1]) Estudiar las correlaciones del vector \\((u_{i},u_{i+k})\\), con \\(k=1,...,10\\). Contrastar si son nulas. Correlaciones: acf(u) Test de Ljung-Box: Box.test(u, lag = 10, type = &quot;Ljung&quot;) ## ## Box-Ljung test ## ## data: u ## X-squared = 22.533, df = 10, p-value = 0.01261 3.2.1 Repetición de contrastes Los contrastes se plantean habitualmente desde el punto de vista de la inferencia estadística en la práctica: se realiza una prueba sobre la única muestra disponible. Si se realiza una única prueba, en las condiciones de \\(H_{0}\\) hay una probabilidad \\(\\alpha\\) de rechazarla. En simulación tiene mucho más sentido realizar un gran número de pruebas: La proporción de rechazos debería aproximarse al valor de \\(\\alpha\\)(se puede comprobar para distintos valores de \\(\\alpha\\)). La distribución del estadístico debería ajustarse a la teórica bajo \\(H_{0}\\)(se podría realizar un nuevo contraste de bondad de ajuste). Los p-valores obtenidos deberían ajustarse a una \\(\\mathcal{U}\\left(0,1\\right)\\) (se podría realizar también un contraste de bondad de ajuste). Este procedimiento es también el habitual para validar un método de contraste de hipótesis por simulación. Ejemplo 3.1 Consideramos el generador congruencial RANDU: # Valores iniciales initRANDC(543210) # Fijar semilla para reproductibilidad # set.seed(543210) n &lt;- 500 nsim &lt;- 1000 estadistico &lt;- numeric(nsim) pvalor &lt;- numeric(nsim) # Realizar contrastes for(isim in 1:nsim) { u &lt;- RANDCN(n) # Generar # u &lt;- runif(n) tmp &lt;- chisq.test.cont(u, distribution=&quot;unif&quot;, nclasses=100, output=FALSE, nestpar=0, min=0, max=1) estadistico[isim] &lt;- tmp$statistic pvalor[isim] &lt;- tmp$p.value } Proporción de rechazos: # cat(&quot;\\nProporción de rechazos al 1% =&quot;, sum(pvalor &lt; 0.01)/nsim, &quot;\\n&quot;) cat(&quot;\\nProporción de rechazos al 1% =&quot;, mean(pvalor &lt; 0.01), &quot;\\n&quot;) ## ## Proporción de rechazos al 1% = 0.014 # cat(&quot;Proporción de rechazos al 5% =&quot;, sum(pvalor &lt; 0.05)/nsim, &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 5% =&quot;, mean(pvalor &lt; 0.05), &quot;\\n&quot;) ## Proporción de rechazos al 5% = 0.051 # cat(&quot;Proporción de rechazos al 10% =&quot;, sum(pvalor &lt; 0.1)/nsim, &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 10% =&quot;, mean(pvalor &lt; 0.1), &quot;\\n&quot;) ## Proporción de rechazos al 10% = 0.112 Análisis del estadístico contraste: # Histograma hist(estadistico, breaks = &quot;FD&quot;, freq=FALSE) curve(dchisq(x,99), add=TRUE) # Test ji-cuadrado # chisq.test.cont(estadistico, distribution=&quot;chisq&quot;, nclasses=20, nestpar=0, df=99) # Test de Kolmogorov-Smirnov ks.test(estadistico, &quot;pchisq&quot;, df=99) ## Warning in ks.test(estadistico, &quot;pchisq&quot;, df = 99): ties should not be ## present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: estadistico ## D = 0.023499, p-value = 0.6388 ## alternative hypothesis: two-sided Análisis de los p-valores: # Histograma hist(pvalor, freq=FALSE) abline(h=1) # curve(dunif(x,0,1), add=TRUE) # Test ji-cuadrado # chisq.test.cont(pvalor, distribution=&quot;unif&quot;, nclasses=20, nestpar=0, min=0, max=1) # Test de Kolmogorov-Smirnov ks.test(pvalor, &quot;punif&quot;, min=0, max=1) ## Warning in ks.test(pvalor, &quot;punif&quot;, min = 0, max = 1): ties should not be ## present for the Kolmogorov-Smirnov test ## ## One-sample Kolmogorov-Smirnov test ## ## data: pvalor ## D = 0.023499, p-value = 0.6388 ## alternative hypothesis: two-sided 3.2.2 Baterías de contrastes Contrastes específicos para generadores aleatorios: Diehard tests (The Marsaglia Random Number CDROM): http://www.stat.fsu.edu/pub/diehard. TestU01: http://www.iro.umontreal.ca/simardr/testu01/tu01.html. NIST test suite: http://csrc.nist.gov/groups/ST/toolkit/rng. Dieharder (paquete RDieHarder): http://www.phy.duke.edu/rgb/General/dieharder.php Entidad Certificadora (gratuita): CAcert. Documentación adicional: Randomness Tests: A Literature Survey http://www.ciphersbyritter.com/RES/RANDTEST.HTM- Marsaglia, Tsang (2002). Some Difficult-to-pass Tests of Randomness: http://www.jstatsoft.org/v07/i03 http://www.csis.hku.hk/cisc/download/idetect 3.3 Ejercicios de fin de práctica Ejercicio 3.1 Uno de los primeros generadores fue el denominado método de los cuadrados medios propuesto por Von Neumann (1946). Con este procedimiento se generan números pseudoaleatorios de 4 dígitos de la siguiente forma: Se escoge un número de cuatro dígitos \\(x_{0}\\) (semilla). Se eleva al cuadrado (\\(x_{0}^{2}\\)) y se toman los cuatro dígitos centrales (\\(x_{1}\\)). Se genera el número pseudo-aleatorio como\\[u_{1}=\\frac{x_{1}}{10^{4}}.\\] Volver al paso ii y repetir el proceso. Para obtener los \\(k\\) (número par) dígitos centrales de \\(x_{i}^{2}\\) se puede utilizar que: \\[x_{i+1}=\\left\\lfloor \\left( x_{i}^{2}-\\left\\lfloor \\dfrac{x_{i}^{2}}{10^{(2k-\\frac{k}{2})}}\\right\\rfloor 10^{(2k-\\frac{k}{2})}\\right) /10^{\\frac{k}{2}}\\right\\rfloor\\] El algoritmo está implementado en el fichero RANDVN.R: # Generador Von Neumann de números pseudoaleatorios # ================================================= # ------------------------------------------------- # initRANDVN(semilla,n) # Inicia el generador # n número de digitos centrales, 4 por defecto (debe ser un nº par) # Por defecto semilla del reloj # OJO: No se hace ninguna verificación de los parámetros initRANDVN &lt;- function(semilla = as.numeric(Sys.time()), n = 4) { .semilla &lt;&lt;- as.double(semilla) %% 10^n # Cálculos en doble precisión .n &lt;&lt;- n .aux &lt;&lt;- 10^(2*n-n/2) .aux2 &lt;&lt;- 10^(n/2) return(invisible(list(semilla=.semilla,n=.n))) } # ------------------------------------------------- # RANDVN() # Genera un valor pseudoaleatorio con el generador de Von Neumann # Actualiza la semilla (si no existe llama a initRANDVN) RANDVN &lt;- function() { if (!exists(&quot;.semilla&quot;, envir=globalenv())) initRANDVN() z &lt;- .semilla^2 .semilla &lt;&lt;- trunc((z-trunc(z/.aux)*.aux)/.aux2) return(.semilla/10^.n) } # ------------------------------------------------- # RANDVNN(n) # Genera un vector de valores pseudoaleatorios con el generador congruencial # (por defecto de dimensión 1000) # Actualiza la semilla (si no existe llama a initRANDVN) RANDVNN &lt;- function(n = 1000) { x &lt;- numeric(n) for(i in 1:n) x[i] &lt;- RANDVN() return(x) # return(replicate(n,RANDVN())) # Alternativa más rápida } Estudiar las características del generador de cuadrados medios a partir de una secuencia de 500 valores. Emplear únicamente métodos gráficos. Ejercicio 3.2 Considerando el generador congruencial multiplicativo de parámetros \\(a=7^{5}=16807\\), \\(c=0\\) y \\(m=2^{31}-1\\). ¿Se observan los mismos problemas que con el algoritmo RANDU al considerar las tripletas \\((x_{k},x_{k+1},x_{k+2})\\)? Ejercicio 3.3 (para entregar) Considera el generador congruencial definido por: \\[\\begin{aligned} x_{n+1} &amp; =(65x_{n}+1)\\ \\operatorname{mod}\\ 2048,\\nonumber\\\\ u_{n+1} &amp; =\\frac{x_{n+1}}{2048},\\ n=0,1,\\dots\\nonumber \\end{aligned}\\] Indicar razonadamente si es de ciclo máximo. Generar 1000 valores tomando como semilla inicial el nº de grupo multiplicado por 100 y obtener el tiempo de CPU. Representar gráficamente el ajuste a la densidad teórica y realizar el correspondiente contraste de Kolmogorov-Smirnov. Representar los pares de datos \\(\\left( u_{i}, u_{i+1} \\right)\\), ¿se observa algún problema?. Estudiar la aleatoriedad de este generador empleando repetidamente el test de Ljung-Box, considerando 500 pruebas con muestras de tamaño 50 y hasta el salto 10 (Box.test(u,lag=10, type=Ljung)). Comparar el ajuste de las distribuciones del estadístico y \\(p\\)-valor a las de referencia. "],
["analisis-de-los-resultados.html", "Capítulo 4 Análisis de los resultados 4.1 Convergencia 4.2 Estimación de la precisión 4.3 Teorema central del límite 4.4 Determinación del número de generaciones 4.5 El problema de la dependencia: 4.6 Observaciones", " Capítulo 4 Análisis de los resultados 4.1 Convergencia Supongamos que estamos interesados en aproximar la media teórica \\(E\\left( X\\right)\\) a partir de una secuencia i.i.d. \\(X_{1}\\), \\(X_{2}\\), \\(\\cdots\\), \\(X_{n}\\) mediante la media muestral \\(\\bar{X}_{n}\\) (probabilidad caso particular). Una justificación teórica de la validez de la aproximación obtenida mediante simulación es la ley (débil) de los grandes números: Si \\(X_{1}\\), \\(X_{2}\\), \\(\\cdots\\) es una secuencia de v.a.’s independientes con: \\[E\\left( X_{i}\\right) =\\mu \\text{ y }Var\\left( X_{i}\\right) =\\sigma^{2}&lt;\\infty,\\] entonces \\(\\overline{X}_{n}=\\left( X_{1}+\\cdots +X_{n}\\right) /n\\) converge en probabilidad a \\(\\mu\\). i.e. para cualquier \\(\\varepsilon &gt;0\\): \\[\\lim\\limits_{n\\rightarrow \\infty }P\\left( \\left\\vert \\overline{X}_{n}-\\mu \\right\\vert &lt;\\varepsilon \\right) = 1.\\] La ley fuerte establece la convergencia casi segura. Ejemplo: Aproximación de una probabilidad p &lt;- 0.5 set.seed(1) nsim &lt;- 10000 rx &lt;- runif(nsim) &lt;= p # Aproximación por simulación de p mean(rx) ## [1] 0.5047 # Evolución de la aproximación plot(cumsum(rx)/1:nsim, type=&quot;l&quot;, lwd=2, xlab=&quot;Número de generaciones&quot;, ylab=&quot;Proporción muestral&quot;, ylim=c(0,1)) abline(h = mean(rx), lty = 2) # valor teórico abline(h = p) Una suposición crucial es que las variables \\(X_{i}\\) deben tener varianza finita (realmente esta suposición puede relajarse: \\(E\\left( \\left\\vert X_{i} \\right\\vert \\right) &lt; \\infty\\)). En caso contrario la media muestral puede no converger a una constante. Un ejemplo conocido es la distribución de Cauchy: # Problemas convergencia set.seed(1) nsim &lt;- 10000 rx &lt;- rcauchy(nsim) plot(cumsum(rx)/1:nsim, type=&quot;l&quot;, lwd=2, xlab=&quot;Número de generaciones&quot;, ylab=&quot;Media muestral&quot;) Es recomendable representar la evolución de la aproximación de la característica de interés (sobre el número de generaciones) para detectar problemas de convergencia. Además del análisis descriptivo de las simulaciones: boxplot(rx) 4.2 Estimación de la precisión En el caso de la media muestral \\(\\overline{X}_{n}\\), un estimador insesgado de \\(Var\\left( \\overline{X}_{n}\\right) =\\sigma ^{2}/n\\) es: \\[\\widehat{Var}\\left( \\overline{X}_{n}\\right) =\\frac{\\widehat{S}^{2}}{n}\\] con: \\[\\widehat{S}_{n}^{2}=\\dfrac{1}{n-1}\\sum\\limits_{i=1}^{n}\\left( X_{i}- \\overline{X}\\right) ^{2}.\\] En el caso de una proporción \\(\\hat{p}_{n}\\): \\[\\widehat{Var}\\left( \\hat{p}_{n}\\right) =\\frac{\\hat{p}_{n}(1-\\hat{p}_{n})}{n-1}.\\] Los valores obtenidos servirían como medidas básicas de la precisión de la aproximación, aunque su principal aplicación es la construcción de intervalos de confianza. 4.3 Teorema central del límite Si \\(X_{1}\\), \\(X_{2}\\), \\(\\cdots\\) es una secuencia de v.a.’s independientes con \\(E\\left( X_{i}\\right) =\\mu\\) y \\(Var\\left( X_{i}\\right) = \\sigma ^{2}&lt;\\infty\\), entonces: \\[Z_{n}=\\frac{\\overline{X}_{n}-\\mu }{\\frac{\\sigma }{\\sqrt{n}}} \\overset{d}{ \\rightarrow } N(0,1)\\] i.e. \\(\\lim\\limits_{n\\rightarrow \\infty }F_{Z_{n}}(z)=\\Phi (z)\\). Por tanto, un intervalo de confianza asintótico para \\(\\mu\\) es: \\[IC_{1-\\alpha }(\\mu ) = \\left( \\overline{X}_{n} - z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}},\\ \\overline{X}+z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}} \\right).\\] Podemos considerar que \\(z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}}\\) es la precisión obtenida (con nivel de confianza \\(1-\\alpha\\)). # Teorema central del límite xsd &lt;- 1 xmed &lt;- 0 set.seed(1) nsim &lt;- 1000 rx &lt;- rnorm(nsim, xmed, xsd) # Aproximación por simulación de la media mean(rx) ## [1] -0.01164814 # &quot;Error&quot; de la aproximación 2*sd(rx)/sqrt(nsim) ## [1] 0.06545382 # Evolución de la aproximación n &lt;- 1:nsim est &lt;- cumsum(rx)/n esterr &lt;- sqrt(cumsum((rx-est)^2))/(n - 1) # Error estandar plot(est, type=&quot;l&quot;, lwd=2, xlab=&quot;Número de generaciones&quot;, ylab=&quot;Media y rango de error&quot;, ylim=c(-1,1)) abline(h = est[nsim], lty=2) lines(est + 2*esterr, lty=3) lines(est - 2*esterr, lty=3) abline(h = xmed) 4.4 Determinación del número de generaciones En muchas ocasiones puede interesar obtener una aproximación con un nivel de precisión fijado. Para una precisión absoluta \\(\\varepsilon\\), se trata de determinar \\(n\\) de forma que: \\[z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}}&lt;\\varepsilon\\] Un algoritmo podría ser el siguiente: Hacer \\(j=0\\) y fijar un tamaño inicial \\(n_{0}\\) (e.g. 30 ó 60). Generar \\(\\left\\{ X_{i}\\right\\} _{i=1}^{n_{0}}\\) y calcular \\(\\widehat{S}_{n_{0}}\\). Mientras \\(\\left. z_{1-\\alpha /2}\\widehat{S}_{n_{j-1}}\\right/ \\sqrt{n_{j}}&gt;\\varepsilon\\) hacer: \\(\\qquad j=j+1\\). \\(\\qquad n_{j}=\\left\\lceil \\left( \\left. z_{1-\\alpha /2}\\widehat{S} _{n_{j-1}}\\right/ \\varepsilon \\right)^{2}\\right\\rceil\\). \\(\\qquad\\) Generar \\(\\left\\{ X_{i}\\right\\} _{i=n_{j-1}+1}^{n_{j}}\\) y calcular \\(\\widehat{S}_{n_{j}}\\). Para una precisión relativa \\(\\varepsilon \\left\\vert \\mu \\right\\vert\\) se procede análogamente de forma que:\\[z_{1-\\alpha /2}\\dfrac{\\widehat{S}_{n}}{\\sqrt{n}}&lt;\\varepsilon \\left\\vert \\overline{X}_{n}\\right\\vert .\\] 4.5 El problema de la dependencia: En el caso de dependencia, la estimación de la precisión se complica: \\[Var\\left( \\overline{X}\\right) =\\frac{1}{n^{2}}\\left( \\sum_{i=1}^{n}Var\\left( X_{i} \\right) + 2\\sum_{i&lt;j}Cov\\left( X_{i},X_{j}\\right) \\right).\\] # Variable dicotómica 0/1 (FALSE/TRUE) set.seed(1) nsim &lt;- 10000 # Tomamos como idea que en A Coruña llueve de media 1/3 días al año, # y suponemos que 0.656 es la prob de que llueva si el día # anterio llovió y 0.056 si no. # x == &quot;llueve&quot; alpha &lt;- 0.03 # prob de lluvia si FALSE/0 (prob. de cambio) beta &lt;- 0.94 # prob de lluvia si TRUE/1 (prob. de cambio 0.06) rx &lt;- logical(nsim) rx[1] &lt;- FALSE # El primer día no llueve for (i in 2:nsim) rx[i] &lt;- runif(1) &lt; (if (rx[i-1]) beta else alpha) n &lt;- 1:nsim est &lt;- cumsum(rx)/n esterr &lt;- sqrt(est*(1-est)/(n-1)) IMPORTANTE: Al ser datos dependientes esta estimación del error estandar no es adecuada. acf(as.numeric(rx)) En este caso al haber dependencia positiva se produce una subestimación del verdadero error estandar. plot(est, type=&quot;l&quot;, lwd=2, ylab=&quot;Probabilidad&quot;, xlab=&quot;Número de simulaciones&quot;, ylim=c(0,0.6)) abline(h = est[nsim], lty=2) lines(est + 2*esterr, lty=2) # OJO! Supone independencia lines(est - 2*esterr, lty=2) abline(h = 1/3, col=&quot;darkgray&quot;) # Prob. teor. cadenas Markov El gráfico de autocorrelaciones sugiere que si tomamos 1 de cada 25 podemos suponer independencia. lag &lt;- 24 xlag &lt;- c(rep(FALSE, lag), TRUE) rxi &lt;- rx[xlag] acf(as.numeric(rxi)) n &lt;- 1:length(rxi) est &lt;- cumsum(rxi)/n esterr &lt;- sqrt(est*(1-est)/(n-1)) plot(est, type=&quot;l&quot;, lwd=2, ylab=&quot;Probabilidad&quot;, xlab=paste(&quot;Número de simulaciones /&quot;, lag), ylim=c(0,0.6)) abline(h = est[length(rxi)], lty=2) lines(est + 2*esterr, lty=2) # Supone independencia lines(est - 2*esterr, lty=2) abline(h = 1/3, col=&quot;darkgray&quot;) # Prob. teor. cadenas Markov No será la proximación más eficiente… rxi &lt;- matrix(rx, ncol = lag - 1, byrow = TRUE) ## Warning in matrix(rx, ncol = lag - 1, byrow = TRUE): la longitud de los ## datos [10000] no es un submúltiplo o múltiplo del número de filas [435] en ## la matriz n &lt;- 1:nrow(rxi) for(i in 1:ncol(rxi)) { est &lt;- cumsum(rxi[,i])/n esterr &lt;- sqrt(est*(1-est)/(n-1)) if (i == 1) plot(est, type=&quot;l&quot;, ylab=&quot;Probabilidad&quot;, xlab = paste(&quot;Número de simulaciones /&quot;, lag), ylim = c(0,0.6)) else lines(est) lines(est + 2*esterr, col = &quot;lightgray&quot;, lty=3) # Supone independencia... lines(est - 2*esterr, col = &quot;lightgray&quot;, lty=3) } abline(h = mean(rx), lwd = 2, lty=2) abline(h = 1/3, col=&quot;darkgray&quot;) # Prob. teor. cadenas Markov Trataremos este tipo de problemas en la diagnosis de algoritmos de simulación Monte Carlo de Cadenas de Markov (MCMC). Aparecen también en la simulación dinámica (por eventos o cuantos). 4.5.1 Periodo de calentamiento Ejercicio 2.3 Repetir el ejemplo anterior considerando como punto de partida un día lluvioso. 4.6 Observaciones En el caso de que la característica de interés de la distribución de \\(X\\) no sea la media, los resultados anteriores no serían en principio aplicables. Incluso en el caso de la media, las bandas de confianza obtenidas con el TCL son puntuales (si generamos nuevas secuencias de simulación es muy probable que no estén contenidas). Se podría recurrir a la generación de múltiples secuencias (equivalentemente emplear un método de macro-micro replicaciones para la estimación de la varianza - bach means: medias por lotes). En muchos casos la generación de múltiples secuencias de simulación puede suponer un coste computacional importante, por lo que puede ser preferible emplear un método de remuestreo. "],
["aplicaciones-de-la-simulacion-en-inferencia-estadistica.html", "Capítulo 5 Aplicaciones de la simulación en Inferencia Estadística 5.1 Aplicaciones de la simulación 5.2 Distribución en el muestreo 5.3 Intervalos de confianza 5.4 Contrastes de hipótesis 5.5 Comparación de estimadores 5.6 Remuestreo Bootstrap", " Capítulo 5 Aplicaciones de la simulación en Inferencia Estadística 5.1 Aplicaciones de la simulación Aplicaciones: Estadística: Muestreo, aproximación de distribuciones, remuestreo,… Optimización: Algoritmos genéticos, … Análisis numérico: Evaluación de expresiones, … … En este capítulo nos centraremos en algunas de las aplicaciones en inferencia estadística, aunque habría muchas: Distribución de estimadores puntuales/estadísticos: Aproximación de la distribución. Aproximación de características de la distribución. Valided de la distribución asintótica. Comparación de estimadores. Estimación por intervalo de confianza: Obtención de intervalos/bandas de confianza (probabilidad). Análisis de un estimador por intervalo de confianza. Contrastes de hipótesis: Aproximación del \\(p\\)-valor. Análisis de un contraste de hipótesis. Validación teoría. Métodos de remuestro bootstrap. Inferencia Bayesiana … En el siguiente capítulo trararemos la Integración y Optimización Monte Carlo… 5.2 Distribución en el muestreo Ejercicio 5.1 Si \\(X_{1},\\ldots,X_{n}\\) es una muestra aleatoria simple de una variable aleatoria \\(X\\) \\(\\in\\) \\(N\\left( \\mu, \\sigma \\right),\\) la distribución en el muestreo de: \\[\\hat{\\mu}=\\overline{X}=\\dfrac{1}{n}\\sum_{i=1}^{n}X_{i}\\] es: \\[\\overline{X}\\in N\\left( \\mu,\\dfrac{\\sigma}{\\sqrt{n}}\\right)\\] Confirmar este resultado mediante simulación, para ello: Crear un conjunto de datos muestras con 500 muestras de tamaño \\(n=10\\) de una \\(N(1,2)\\). Añadir al conjunto de datos las estimaciones de la media y desviación típica obtenidas con cada una de las muestras. Valores iniciales: set.seed(54321) # Fijar semilla para reproductibilidad nsim &lt;- 500 nx &lt;- 10 Valores teóricos: mux &lt;- 1 sdx &lt;- 2 Simulación de las muestras (al estilo Rcmdr): muestras &lt;- as.data.frame(matrix(rnorm(nsim*nx, mean=mux, sd=sdx), ncol=nx)) rownames(muestras) &lt;- paste(&quot;muestra&quot;, 1:nsim, sep=&quot;&quot;) colnames(muestras) &lt;- paste(&quot;obs&quot;, 1:nx, sep=&quot;&quot;) str(muestras) ## &#39;data.frame&#39;: 500 obs. of 10 variables: ## $ obs1 : num 0.642 -0.856 -0.568 -2.301 0.184 ... ## $ obs2 : num 3.483 2.216 1.1 4.305 0.677 ... ## $ obs3 : num 1.24 -1.51 -3.98 2.29 2.46 ... ## $ obs4 : num 3.286 0.947 0.953 -1.663 2.623 ... ## $ obs5 : num 3.77 -1.34 1.61 -2.46 1.11 ... ## $ obs6 : num -2.044 0.32 3.046 0.136 3.555 ... ## $ obs7 : num 0.6186 -1.8614 4.3386 0.0996 0.8334 ... ## $ obs8 : num -0.829 2.202 -1.688 1.534 -0.114 ... ## $ obs9 : num 0.4904 -0.6713 0.5451 -0.6517 0.0168 ... ## $ obs10: num 2.79 2.84 1.27 3.93 2.17 ... Estimaciones muestras$mean &lt;- rowMeans(muestras[,1:nx]) muestras$sd &lt;- apply(muestras[,1:nx], 1, sd) Nota: La fila muestras[i,] contiene las observaciones de la i-ésima muestra y la correspondiente media y desv. tip. muestras[1,] ## obs1 obs2 obs3 obs4 obs5 obs6 obs7 ## muestra1 0.6421985 3.482661 1.242483 3.28559 3.766896 -2.04443 0.6186323 ## obs8 obs9 obs10 mean sd ## muestra1 -0.8293636 0.4903819 2.790091 1.344514 1.951292 Generar el histograma (en escala de densidades) de las medias muestrales y compararlo con la densidad teórica. Distribución de la media muestral: hist(muestras$mean, freq=FALSE, breaks=&quot;FD&quot;, main=&quot;Distribución de la media muestral&quot;, xlab=&quot;Medias&quot;, ylab=&quot;Densidad&quot;) # Densidad observada (estimación) lines(density(muestras$mean)) # Densidad teórica (bajo normalidad) curve(dnorm(x,mux,sdx/sqrt(nx)), lwd=2, col=&quot;blue&quot;, add=TRUE) # Aproximación del valor esperado de la media muestral mediante simulación abline(v=mean(muestras$mean),lty=2) # Valor esperado de la media muestral (teórico) abline(v=mux, col=&quot;blue&quot;) Ejercicio 5.2 Si \\(X_{1},\\ldots,X_{n}\\) es una m.a.s. de una variable aleatoria \\(X\\) (cualquiera) con \\(E\\left( X \\right) = \\mu\\) y \\(Var\\left( X \\right) = \\sigma^{2}\\), por el Teorema Central del Límite, la distribución en el muestreo de \\(\\hat{\\mu}=\\overline{X}\\) se aproxima a la normalidad: \\[\\overline{X}\\underset{n\\rightarrow\\infty}{\\longrightarrow} N\\left( \\mu, \\dfrac{\\sigma}{\\sqrt{n}}\\right)\\] Típicamente se suele considerar que esta aproximación es buena para tamaños muestrales \\(n&gt;30\\), aunque dependerá de las características de la distribución de \\(X\\). Repetir el ejercicio anterior considerando muestras de una \\(Exp(1)\\) (tener en cuenta que \\(X\\in Exp(\\lambda)\\Rightarrow\\mu_{X}=\\sigma_{X}=1/\\lambda\\)). ¿Qué ocurre con la distribución de la media muestral? set.seed(54321) # Fijar semilla para reproductibilidad nsim &lt;- 500 nx &lt;- 10 # nx &lt;- 50 Valores teóricos: lambda &lt;- 1 muexp &lt;- 1/lambda sdexp &lt;- muexp Simulación de las muestras: muestras2 &lt;- as.data.frame(matrix(rexp(nsim*nx, rate=lambda), ncol=nx)) rownames(muestras2) &lt;- paste(&quot;muestra&quot;, 1:nsim, sep=&quot;&quot;) colnames(muestras2) &lt;- paste(&quot;obs&quot;, 1:nx, sep=&quot;&quot;) Estimaciones: muestras2$mean &lt;- rowMeans(muestras2[,1:nx]) muestras2$sd &lt;- apply(muestras2[,1:nx], 1, sd) Distribución de la media muestral: hist(muestras2$mean, xlim = c(-0.1, 2.5), freq=FALSE, breaks=&quot;FD&quot;, main=&quot;Distribución de la media muestral&quot;, xlab=&quot;Medias&quot;, ylab=&quot;Densidad&quot;) # Densidad observada (estimación) lines(density(muestras2$mean)) # Distribución asintótica (TCL) curve(dnorm(x,muexp,sdexp/sqrt(nx)), lwd=2, col=&quot;blue&quot;, add=TRUE) # Aproximación del valor esperado de la media muestral mediante simulación abline(v=mean(muestras2$mean),lty=2) # Valor esperado de la media muestral (teórico) abline(v=muexp, col=&quot;blue&quot;) Aumentar el tamaño muestral a 50. ¿Se aproxima más la distribución de las medias muestrales a la teórica bajo normalidad? Ejecutar el código del apartado anterior fijando nx &lt;- 50. 5.3 Intervalos de confianza Ejercicio 2.4 Siguiendo el enunciado del ejercicio 1, se deduce que el intervalo de confianza (de nivel \\(1-\\alpha\\)) para la media \\(\\mu\\) de una población normal con varianza conocida es: \\[IC_{1-\\alpha}\\left( \\mu\\right) = \\left( \\overline{X}-z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}},\\ \\overline{X} + z_{1-\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}} \\right).\\] La idea es que el \\(100(1-\\alpha)\\%\\) de los intervalos así construidos contentrán el verdadero valor del parámetro. Utilizando el conjunto de datos muestras del ejercicio 1 (500 muestras de tamaño \\(n=10\\) de una \\(N(1,2)\\)), añadir en dos nuevas variables los extremos del intervalo de confianza para la media con varianza conocida al conjunto de datos. Analizar la cobertura de estas estimaciones por IC. IC para la media con varianza conocida (bajo normalidad) alfa &lt;- 0.05 z &lt;- qnorm(1 - alfa/2) muestras$ici &lt;- muestras$mean - z*sdx/sqrt(nx) muestras$ics &lt;- muestras$mean + z*sdx/sqrt(nx) Cobertura de las estimaciones por IC: muestras$cob &lt;- (muestras$ici &lt; mux) &amp; (mux &lt; muestras$ics) ncob &lt;- sum(muestras$cob) # Nº de intervalos que contienen la verdadera media ncob ## [1] 480 100*ncob/nsim # Proporción de intervalos ## [1] 96 100*(1 - alfa) # Proporción teórica bajo normalidad ## [1] 95 Solo como ejemplo ilustrativo: Gráfico de los primeros 50 intervalos: m &lt;- 50 tmp &lt;- muestras[1:m,] attach(tmp) color &lt;- ifelse(cob,&quot;blue&quot;,&quot;red&quot;) plot(1:m, mean, col = color, ylim = c(min(ici),max(ics)), main = &quot;Cobertura de las estimaciones por IC&quot;, xlab = &quot;Muestra&quot;, ylab = &quot;IC&quot;) arrows(1:m, ici, 1:m, ics, angle = 90, length = 0.05, code = 3, col = color) abline(h = mux, lty = 3) detach(tmp) Repetir el apartado anterior considerando muestras de una \\(Exp(1)\\). ¿Qué ocurre con la cobertura del intervalo de confianza obtenido bajo normalidad? Ejecutar el código del apartado a) del ejercicio 2. IC para la media con varianza conocida (bajo normalidad) alfa &lt;- 0.05 z &lt;- qnorm(1 - alfa/2) muestras2$ici &lt;- muestras2$mean - z*sdexp/sqrt(nx) muestras2$ics &lt;- muestras2$mean + z*sdexp/sqrt(nx) Cobertura de las estimaciones por IC: muestras2$cob &lt;- (muestras2$ici &lt; muexp) &amp; (muexp &lt; muestras2$ics) ncob &lt;- sum(muestras2$cob) # Nº de intervalos que contienen la verdadera media ncob ## [1] 469 100*ncob/nsim # Proporción de intervalos ## [1] 93.8 100*(1 - alfa) # Proporción teórica bajo normalidad ## [1] 95 Solo como ejemplo ilustrativo: Gráfico de los primeros 100 intervalos: m &lt;- 100 tmp &lt;- muestras2[1:m,] attach(tmp) color &lt;- ifelse(cob,&quot;blue&quot;,&quot;red&quot;) plot(1:m, mean, col = color, ylim = c(min(ici),max(ics)), main = &quot;Cobertura de las estimaciones por IC&quot;, xlab = &quot;Muestra&quot;, ylab = &quot;IC&quot;) arrows(1:m, ici, 1:m, ics, angle = 90, length = 0.05, code = 3, col = color) abline(h = muexp, lty = 3) detach(tmp) ¿Qué ocurre si aumentamos el tamaño muestral a 50? Ejecutar el código del ejercicio anterior fijando nx &lt;- 50 y el del apartado anterior. En los apartadps b) y c) podíamos considerar bootstrap descrito al final de este capítulo. Podemos aproximar por simulación los intervalos de probabilidad de la media muestral (tendríamos una idea del valor esperado de lo que obtendríamos con el bootstrap percentil; en este caso el estimador es insesgado…): # Distribución de la media muestral hist(muestras2$mean, freq=FALSE, breaks=&quot;FD&quot;, main=&quot;Distribución de la media muestral&quot;, xlab=&quot;Medias&quot;, ylab=&quot;Densidad&quot;) # Densidad observada (estimación) lines(density(muestras2$mean), lwd=2, col=&#39;red&#39;) # Densidad teórica (bajo normalidad) curve(dnorm(x,muexp,sdexp/sqrt(nx)), col=&quot;blue&quot;, add=TRUE) # Aproximación por simulación del valor esperado de la media muestral abline(v=mean(muestras2$mean), lty=2) # Valor esperado de la media muestral (teórico) abline(v=muexp, col=&quot;blue&quot;) # IP bajo normalidad ic.aprox &lt;- apply(muestras2[ ,c(&#39;ici&#39;,&#39;ics&#39;)], 2, mean) ## ic.aprox ## ici ics ## 0.3865199 1.6261099 # Intervalo de probabilidad para la media muestral aproximado bajo normalidad abline(v = ic.aprox, col=&#39;blue&#39;) # Intervalo de probabilidad para la media muestral (aproximado por simulación) ic.sim &lt;- quantile(muestras2$mean, c(alfa/2, 1 - alfa/2)) ## ic.sim ## 2.5% 97.5% ## 0.4714233 1.8059094 # IP (aprox.) abline(v=ic.sim, lty=2, col=&#39;red&#39;) Nota: Estimaciones puntuales, por intervalo de confianza y contrastes de hipótesis para la media con varianza desconocida bajo normalidad se pueden obtener con la función t.test. Ejercicio 5.3 El Intervalo de confianza para una proporción construido usando la aproximación normal tiene un mal comportamiento cuando el tamaño de la muestra es pequeño. Una simple y efectiva mejora consiste en añadir a la muestra \\(2a\\) elementos, \\(a\\) exitos y \\(a\\) fracasos. Así el intervalo de confianza al \\(\\left( 1-\\alpha\\right) 100\\%\\) para una proporción mejorado es: \\[\\begin{aligned} IC_{1-\\alpha}^{a}\\left( p\\right) &amp; =\\left( \\tilde{p}-z_{1-\\alpha/2}\\sqrt{\\frac{\\tilde{p}(1-\\tilde{p})}{\\tilde{n}}} \\text{ , } \\tilde{p}+z_{1-\\alpha/2}\\sqrt{\\frac{\\tilde{p}(1-\\tilde{p})}{\\tilde{n}}}\\right) ,\\\\ \\text{siendo }\\tilde{n} &amp; = n+2a \\text{, } \\tilde{p} = \\frac{np+a}{\\tilde{n}}. \\end{aligned}\\] En el caso de \\(a=2\\) se denomina IC Agresti-Coull. Teniendo en cuenta que la v.a. \\(X=n\\hat{p}\\sim\\mathcal{B}(n,p)\\), obtener y representar gráficamente la cobertura teórica del intervalo de confianza estándar (\\(a=0\\)) de una proporción para una muestra de tamaño \\(n=30\\), \\(\\alpha=0.05\\) y distintos valores de \\(p\\) (p.teor &lt;- seq(1/n, 1 - 1/n, length = 1000)). Parámetros: n &lt;- 30 alpha &lt;- 0.05 adj &lt;- 0 # (adj &lt;- 2 para Agresti-Coull) Probabilidades teóricas: m &lt;- 1000 p.teor &lt;- seq(1/n, 1 - 1/n, length = m) Posibles resultados: x &lt;- 0:n p.est &lt;- (x + adj)/(n + 2 * adj) ic.err &lt;- qnorm(1 - alpha/2) * sqrt(p.est * (1 - p.est)/(n + 2 * adj)) lcl &lt;- p.est - ic.err ucl &lt;- p.est + ic.err Recorrer prob. teóricas: p.cov &lt;- numeric(m) for (i in 1:m) { # cobertura de los posibles intervalos cover &lt;- (p.teor[i] &gt;= lcl) &amp; (p.teor[i] &lt;= ucl) # prob. de los posibles intervalos p.rel &lt;- dbinom(x[cover], n, p.teor[i]) # prob. total de cobertura p.cov[i] &lt;- sum(p.rel) } Gráfico coberturas: plot(p.teor, p.cov, type = &quot;l&quot;, ylim = c(1 - 4 * alpha, 1)) abline(h = 1 - alpha, lty = 2) Fuente Suess y Trumbo (2010). Repetir el apartado anterior considerando intervalos de confianza Agresti-Coull (\\(a=2\\)). Parámetros: n &lt;- 30 alpha &lt;- 0.05 adj &lt;- 2 # Agresti-Coull # Probabilidades teóricas: m &lt;- 1000 p.teor &lt;- seq(1/n, 1 - 1/n, length = m) # Posibles resultados: x &lt;- 0:n p.est &lt;- (x + adj)/(n + 2 * adj) ic.err &lt;- qnorm(1 - alpha/2) * sqrt(p.est * (1 - p.est)/(n + 2 * adj)) lcl &lt;- p.est - ic.err ucl &lt;- p.est + ic.err # Recorrer prob. teóricas: p.cov &lt;- numeric(m) for (i in 1:m) { # cobertura de los posibles intervalos cover &lt;- (p.teor[i] &gt;= lcl) &amp; (p.teor[i] &lt;= ucl) # prob. de los posibles intervalos p.rel &lt;- dbinom(x[cover], n, p.teor[i]) # prob. total de cobertura p.cov[i] &lt;- sum(p.rel) } # Gráfico coberturas: plot(p.teor, p.cov, type = &quot;l&quot;, ylim = c(1 - 4 * alpha, 1)) abline(h = 1 - alpha, lty = 2) Repetir el apartado anterior empleando simulación para aproximar la cobertura. Parámetros: n &lt;- 30 alpha &lt;- 0.05 adj &lt;- 2 #&#39; (2 para Agresti-Coull) set.seed(54321) nsim &lt;- 500 # Probabilidades teóricas: m &lt;- 1000 p.teor &lt;- seq(1/n, 1 - 1/n, length = m) Recorrer prob. teóricas: # m &lt;- length(p.teor) p.cov &lt;- numeric(m) for (i in 1:m) { # Equivalente a simular nsim muestras de tamaño n # ry &lt;- matrix(rbinom(n*nsim, 1, p.teor[i]), ncol=n) # rx &lt;- apply(ry, 1, sum) rx &lt;- rbinom(nsim, n, p.teor[i]) p.est &lt;- (rx + adj)/(n + 2 * adj) ic.err &lt;- qnorm(1 - alpha/2) * sqrt(p.est * (1 - p.est)/(n + 2 * adj)) p.cov[i] &lt;- mean( abs(p.est - p.teor[i]) &lt; ic.err ) } Representar: plot(p.teor, p.cov, type = &quot;l&quot;, ylim = c(1 - 4 * alpha, 1)) abline(h = 1 - alpha, lty = 2) 5.4 Contrastes de hipótesis Ejercicio 5.4 En el tema 2 se propuso el análisis de la bondad de ajuste de un generador de números pseudo-aleatorios mediante el test de Kolmogorov-Smirnov. Sin embargo, si \\(H_{0}\\) es compuesta (los parámetros desconocidos se estiman por máxima verosimilitud y se trabaja con \\(\\hat{F}_{0}\\)) los cuantiles de la distribución (asintótica) de \\(D_{n}\\) pueden ser demasiado conservativos y sería preferible utilizar la distribución exacta. Analizar el comportamiento del contraste de Kolmogorov-Smirnov para contrastar normalidad empleando repetidamente este test, considerando 1000 pruebas con muestras de tamaño 30 de una \\(\\mathcal{N}(0,1)\\). Comparar gráficamente el ajuste de la distribución del \\(p\\)-valor a la de referencia (estudiar el tamaño del contraste). Valores iniciales: set.seed(54321) nx &lt;- 30 mx &lt;- 0 sx &lt;- 1 nsim &lt;- 1000 estadistico &lt;- numeric(nsim) pvalor &lt;- numeric(nsim) Realizar contrastes for(isim in 1:nsim) { rx &lt;- rnorm(nx, mx, sx) tmp &lt;- ks.test(rx, &quot;pnorm&quot;, mean(rx), sd(rx)) estadistico[isim] &lt;- tmp$statistic pvalor[isim] &lt;- tmp$p.value } Proporción de rechazos: { cat(&quot;\\nProporción de rechazos al 1% =&quot;, mean(pvalor &lt; 0.01), &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 5% =&quot;, mean(pvalor &lt; 0.05), &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 10% =&quot;, mean(pvalor &lt; 0.1), &quot;\\n&quot;) } ## ## Proporción de rechazos al 1% = 0 ## Proporción de rechazos al 5% = 0 ## Proporción de rechazos al 10% = 0.001 Análisis de los p-valores: hist(pvalor, freq=FALSE) abline(h=1, lty=2) # curve(dunif(x,0,1), add=TRUE) # Distribución empírica curve(ecdf(pvalor)(x), type = &quot;s&quot;, lwd = 2, main = &#39;Tamaño del contraste&#39;, ylab = &#39;Proporción de rechazos&#39;, xlab = &#39;Nivel de significación&#39;) abline(a=0, b=1, lty=2) # curve(punif(x, 0, 1), add = TRUE) Repetir el apartado anterior considerando el test de Lilliefors (rutina lillie.test del paquete nortest). library(nortest, quietly = TRUE) Valores iniciales: set.seed(54321) nx &lt;- 30 mx &lt;- 0 sx &lt;- 1 nsim &lt;- 1000 estadistico &lt;- numeric(nsim) pvalor &lt;- numeric(nsim) Realizar contrastes for(isim in 1:nsim) { rx &lt;- rnorm(nx, mx, sx) # tmp &lt;- ks.test(rx, &quot;pnorm&quot;, mean(rx), sd(rx)) tmp &lt;- lillie.test(rx) estadistico[isim] &lt;- tmp$statistic pvalor[isim] &lt;- tmp$p.value } Proporción de rechazos: { cat(&quot;\\nProporción de rechazos al 1% =&quot;, mean(pvalor &lt; 0.01), &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 5% =&quot;, mean(pvalor &lt; 0.05), &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 10% =&quot;, mean(pvalor &lt; 0.1), &quot;\\n&quot;) } ## ## Proporción de rechazos al 1% = 0.01 ## Proporción de rechazos al 5% = 0.044 ## Proporción de rechazos al 10% = 0.089 Análisis de los p-valores: hist(pvalor, freq=FALSE) abline(h=1, lty=2) # curve(dunif(x,0,1), add=TRUE) # Distribución empírica curve(ecdf(pvalor)(x), type = &quot;s&quot;, lwd = 2, main = &#39;Tamaño del contraste&#39;, ylab = &#39;Proporción de rechazos&#39;, xlab = &#39;Nivel de significación&#39;) abline(a=0, b=1, lty=2) # curve(punif(x, 0, 1), add = TRUE) Repetir el apartado a) contrastando una distribución exponencial y considerando 500 pruebas con muestras de tamaño 30 de una \\(Exp(1)\\). Valores iniciales: set.seed(54321) nx &lt;- 30 ratex &lt;- 1 nsim &lt;- 500 estadistico &lt;- numeric(nsim) pvalor &lt;- numeric(nsim) Realizar contrastes for(isim in 1:nsim) { rx &lt;- rexp(nx, ratex) tmp &lt;- ks.test(rx, &quot;pexp&quot;, 1/mean(rx)) estadistico[isim] &lt;- tmp$statistic pvalor[isim] &lt;- tmp$p.value } Proporción de rechazos: { cat(&quot;\\nProporción de rechazos al 1% =&quot;, mean(pvalor &lt; 0.01), &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 5% =&quot;, mean(pvalor &lt; 0.05), &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 10% =&quot;, mean(pvalor &lt; 0.1), &quot;\\n&quot;) } ## ## Proporción de rechazos al 1% = 0 ## Proporción de rechazos al 5% = 0.004 ## Proporción de rechazos al 10% = 0.008 Análisis de los p-valores: hist(pvalor, freq=FALSE) abline(h=1, lty=2) # curve(dunif(x,0,1), add=TRUE) # Distribución empírica curve(ecdf(pvalor)(x), type = &quot;s&quot;, lwd = 2, main = &#39;Tamaño del contraste&#39;, ylab = &#39;Proporción de rechazos&#39;, xlab = &#39;Nivel de significación&#39;) abline(a=0, b=1, lty=2) # curve(punif(x, 0, 1), add = TRUE) Diseñar una rutina que permita realizar el contraste KS de bondad de ajuste de una variable exponencial aproximando el \\(p\\)-valor por simulación y repetir el apartado anterior empleando esta rutina. ks.exp.sim &lt;- function(x, nsim = 10^3) { DNAME &lt;- deparse(substitute(x)) METHOD &lt;- &quot;Kolmogorov-Smirnov Test of pexp by simulation&quot; n &lt;- length(x) RATE &lt;- 1/mean(x) ks.exp.stat &lt;- function(x, rate=1/mean(x)) { DMinus &lt;- pexp(sort(x), rate=rate) - (0:(n - 1))/n DPlus &lt;- 1/n - DMinus Dn = max(c(DMinus, DPlus)) } STATISTIC &lt;- ks.exp.stat(x, rate = RATE) names(STATISTIC) &lt;- &quot;Dn&quot; # PVAL &lt;- 0 # for(i in 1:nsim) { # rx &lt;- rexp(n, rate = RATE) # if (STATISTIC &lt;= ks.exp.stat(rx)) PVAL &lt;- PVAL+1 # } # PVAL &lt;- PVAL/nsim # PVAL &lt;- PVAL/(nsim + 1) # PVAL &lt;- (PVAL + 1)/(nsim + 2) rx &lt;- matrix(rexp(n*nsim, rate = RATE), ncol=n) PVAL &lt;- mean(STATISTIC &lt;= apply(rx, 1, ks.exp.stat)) return(structure(list(statistic = STATISTIC, alternative = &quot;two.sided&quot;, p.value = PVAL, method = METHOD, data.name = DNAME), class = &quot;htest&quot;)) } Simulación: set.seed(54321) nx &lt;- 30 ratex &lt;- 1 nsim &lt;- 500 estadistico &lt;- numeric(nsim) pvalor &lt;- numeric(nsim) Realizar contrastes for(isim in 1:nsim) { rx &lt;- rexp(nx, ratex) # tmp &lt;- ks.test(rx, &quot;pexp&quot;, 1/mean(rx)) tmp &lt;- ks.exp.sim(rx, nsim = 200) estadistico[isim] &lt;- tmp$statistic pvalor[isim] &lt;- tmp$p.value } Proporción de rechazos: { cat(&quot;\\nProporción de rechazos al 1% =&quot;, mean(pvalor &lt; 0.01), &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 5% =&quot;, mean(pvalor &lt; 0.05), &quot;\\n&quot;) cat(&quot;Proporción de rechazos al 10% =&quot;, mean(pvalor &lt; 0.1), &quot;\\n&quot;) } ## ## Proporción de rechazos al 1% = 0.008 ## Proporción de rechazos al 5% = 0.058 ## Proporción de rechazos al 10% = 0.106 Análisis de los p-valores: hist(pvalor, freq=FALSE) abline(h=1, lty=2) # curve(dunif(x,0,1), add=TRUE) # Distribución empírica curve(ecdf(pvalor)(x), type = &quot;s&quot;, lwd = 2, main = &#39;Tamaño del contraste&#39;, ylab = &#39;Proporción de rechazos&#39;, xlab = &#39;Nivel de significación&#39;) abline(a=0, b=1, lty=2) # curve(punif(x, 0, 1), add = TRUE) Estudiar la potencia de los contrastes de los apartados c) y d), considerando como alternativa una distribución Weibull. La distribución exponencial es un caso particular de la Weibull: dexp(x, ratex) == dweibull(x, 1, 1/ratex). Estudiamos lo que ocurre al desplazar dweibull(x, shape, 1/ratex) con 0 &lt; shape &lt; 2. CUIDADO: las simulaciones pueden requerir de mucho tiempo de computación (consideramos valores pequeños de nx y nsim en datos y en ks.exp.sim). set.seed(54321) nx &lt;- 20 ratex &lt;- 1 # Puede ser interesante representarlo variando rate nsim &lt;- 200 alfa &lt;- 0.1 # Puede ser interesante representarlo variando alfa shapex &lt;- seq(0.25, 1.75, len=21) preject &lt;- numeric(length(shapex)) # Porporciones de rechazos con ks.test ks.test.p &lt;- function(x) ks.test(x, &quot;pexp&quot;, 1/mean(x))$p.value preject2 &lt;- preject # Porporciones de rechazos con ks.exp.sim ks.exp.sim.p &lt;- function(x) ks.exp.sim(x, 200)$p.value for (i in seq_along(shapex)) { rx &lt;- matrix(rweibull(nx*nsim, shape = shapex[i], scale = 1/ratex), ncol=nx) preject[i] &lt;- mean( apply(rx, 1, ks.test.p) &lt;= alfa ) preject2[i] &lt;- mean( apply(rx, 1, ks.exp.sim.p) &lt;= alfa ) } plot(shapex, preject, type=&quot;l&quot;, main = paste(&quot;Potencia del contraste ( alfa =&quot;, alfa, &quot;)&quot;), xlab = &quot;shape&quot;, ylab = &quot;Proporción de rechazos&quot;) lines(shapex, preject2, lty = 2) abline(h = alfa, v = 1, lty = 3) 5.5 Comparación de estimadores Supongamos que estamos interesados en estudiar el efecto de datos atípicos en la estimación de la media y de la mediana. Consideramos una v.a. con distribución normal contaminada, en la que una observación procede de una \\(N(0,1)\\) con probabilidad 0.95 y de una \\(N(3,3^2)\\) con probabilidad 0.05 (mixtura). Se puede generar una muestra de esta variable mediante los comandos: p.sim &lt;- rbinom(n, 1, 0.05) dat.sim &lt;- rnorm(n, 3*p.sim, 1+2*p.sim) Función de densidad: curve(0.95*dnorm(x, 0, 1) + 0.05*dnorm(x, 3, 3), -3, 13, ylab = &#39;f(x)&#39;) curve(dnorm(x, 0, 1), add = TRUE, lty = 3) Nota: También es habitual simular este tipo de datos generando un porcentaje alto de valores (95%) de la distribución base (\\(N(0,1)\\)) y el resto (5%) de la distibución “contaminadora” (\\(N(3,3^2)\\)), aunque se suele considerar un porcentaje de contaminación del 1% o inferior. Aproximar mediante simulación (500 generaciones) el sesgo y error estandar de la media y la mediana en el caso de una muestra de tamaño \\(n=100\\) (suponiendo que se pretende estimar la media no contaminada 0). # media y mediana xsd &lt;- 1 xmed &lt;- 0 ndat &lt;- 100 nsim &lt;- 500 # for (isim in 1:nsim) # evitar matrix y apply set.seed(1) ntsim &lt;- ndat*nsim p.sim &lt;- rbinom(ntsim, 1, 0.05) dat.sim &lt;- rnorm(ntsim, 3*p.sim, 1+2*p.sim) dat.sim &lt;- matrix(dat.sim, ncol=nsim) Cada columna es una muestra dat.sim[,1] ## [1] 0.196562061 -0.419942692 1.163269540 -0.405755971 0.744098701 ## [6] 0.476610571 0.541320049 0.610621794 0.058199506 0.778196530 ## [11] 1.845466311 0.762380157 0.235252872 -0.153094268 -2.303784221 ## [16] 0.305694818 -0.161943516 4.308629104 0.149177256 1.029315503 ## [21] 1.170858694 0.950419709 0.069506571 0.240735091 0.859410656 ## [26] 0.712231511 -1.422835468 -1.384827145 0.234677147 -0.080716383 ## [31] -0.547488673 0.146213387 -0.068111798 0.848256028 0.750380551 ## [36] -1.081404512 0.369325420 0.354818278 -0.009652750 -0.876200879 ## [41] -0.028408218 -0.762112440 1.164225248 0.475244888 0.603328627 ## [46] 1.675016050 -1.658518333 0.743926076 0.485889672 -1.188672379 ## [51] 0.352357443 -0.007039027 -0.350304817 -0.881873963 1.198211993 ## [56] -0.124834042 1.834799620 -0.798907092 0.559121337 1.474703088 ## [61] 1.292524060 1.770138751 -1.112064572 0.692627114 0.363946337 ## [66] -0.804760157 1.067903455 1.182083664 0.106025622 -1.007755068 ## [71] 0.011728132 1.226832332 -0.001422142 -1.272307652 0.430236753 ## [76] -0.244613030 -0.093350580 -0.819546535 -2.016538552 5.323444676 ## [81] -0.200906391 -0.634235786 0.304042106 0.095836718 0.937605624 ## [86] 0.458534075 1.141302604 1.274959366 -0.856441475 -2.042713860 ## [91] 0.432273740 0.395935345 0.590817675 1.310925310 0.243082252 ## [96] -0.743848789 -2.443538927 1.107184477 0.863190914 1.080970461 hist(dat.sim[,1]) Calculo los estimadores mean.sim &lt;- apply(dat.sim, 2, mean) median.sim &lt;- apply(dat.sim, 2, median) Estimo sus características: mean(mean.sim) # Coincide con el sesgo (media teórica es 0) ## [1] 0.1459986 sd(mean.sim) ## [1] 0.1349537 mean(median.sim) # Coincide con el sesgo (media teórica es 0) ## [1] 0.04453509 sd(median.sim) ## [1] 0.1300611 Sesgo boxplot(mean.sim-xmed, median.sim-xmed, names=c(&quot;Media&quot;,&quot;Mediana&quot;), ylab=&quot;Sesgo&quot;) abline(h = 0, lty = 2) Error cuadrático boxplot((mean.sim-xmed)^2, (median.sim-xmed)^2, names=c(&quot;Media&quot;,&quot;Mediana&quot;), ylab=&quot;Error cuadrático&quot;) Estadísticos error cuadrático: # SE media summary((mean.sim-xmed)^2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000005 0.0045072 0.0206272 0.0394917 0.0591531 0.3619587 # SE mediana summary((median.sim-xmed)^2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000001 0.0016481 0.0070625 0.0188654 0.0243903 0.2618368 5.6 Remuestreo Bootstrap 5.6.1 Idea: Consideramos un conjunto de datos simulado: set.seed(1) data &lt;- runif(50) La idea es aproximar características poblacionales por las correspondientes de la distribución empírica de los datos observados: # Distribución bootstrap curve(ecdf(data)(x), ylab = &quot;FD&quot;, type = &quot;s&quot;, lwd = 2) # Distribución teórica abline(a = 0, b = 1, lty = 2) Las características de la distribución empírica se pueden aproximar mediante simulación. En el caso i.i.d. esto puede ser implementado mediante remuestreo, realizando repetidamente muestreo aleatorio con reemplazamiento del conjunto de datos original (manteniendo el tamaño muestral): # Muestra bootstrap xboot &lt;- sample(data, replace=TRUE) 5.6.2 Métodos de remuestreo Bootstrap Método de remuestreo (Efron, 1979) utilizado para aproximar características de la distribución en el muestreo de un estadístico: Aproximación del sesgo y de la varianza. Construcción de intervalos de confianza Realizar contrastes de hipótesis. De utilidad cuando no se dispone la distribución exacta, no es adecuado emplear la distribución asintótica, etc. La idea es aproximar características poblacionales por las correspondientes de la distribución empírica de los datos observados. En el caso i.i.d. esto puede ser implementado mediante remuestreo, realizando repetidamente muestreo aleatorio con reemplazamiento del conjunto de datos original (manteniendo el tamaño muestral). Si \\(\\mathbf{x} = \\left( x_{1},x_{2},\\cdots ,x_{n}\\right) ^{t}\\) es una muestra i.i.d. \\(F_{\\theta }\\) y \\(\\hat{\\theta} = T\\left( \\mathbf{x} \\right)\\) es un estimador de \\(\\theta\\). Para \\(b = 1,\\ldots ,B:\\) \\(\\mathbf{x}_{b}^{\\ast } = \\left( x_{b1}^{\\ast },x_{b2}^{\\ast },\\cdots ,x_{bn}^{\\ast }\\right) ^{t}\\) muestra bootstrap obtenida mediante muestreo con reemplazamiento de \\(\\left\\{ x_{1},x_{2},\\cdots ,x_{n}\\right\\}\\). \\(\\hat{\\theta}_{b}^{\\ast } = T\\left( \\mathbf{x}_{b}^{\\ast }\\right)\\) valor del estadístico en la muestra bootstrap. La idea original (bootstrap natural, Efron) es que la variabilidad de \\(\\hat{\\theta}_{b}^{\\ast }\\) en torno a \\(\\hat{\\theta}\\) aproxima la variabilidad de \\(\\hat{\\theta}\\) en torno a \\(\\theta\\): La distribución de \\(\\hat{\\theta}_{b}^{\\ast }-\\hat{\\theta}\\) aproxima la distribución de \\(\\hat{\\theta}-\\theta\\). En general podríamos decir que: la muestra es a la población lo que la muestra bootstrap es a la muestra. Para información adicional sobre bootstrap ver p.e.: Davison, A.C. and Hinkley, D.V. (1997). Bootstrap Methods and Their Application. Cambridge University Press 5.6.3 Ejemplo Como ejemplo consideraremos una muestra de tamaño \\(n=100\\) de una normal estandar para tratar de aproximar el sesgo y error estandar de la media y la mediana mediante bootstrap. # dat &lt;- dat.sim[, 1] set.seed(54321) ndat &lt;- 100 datmed &lt;- 0 datsd &lt;- 1 dat &lt;- rnorm(ndat, mean=datmed, sd=datsd) Consideramos 1000 réplicas bootstrap nboot &lt;- 1000 Es habitual tomar nboot+1 entero múltiplo de 100 e.g. nboot=999 ó 1999 (facilita el cálculo de percentiles, orden (nboot+1)*p) Valor del estadístico en la muestra: stat.dat &lt;- mean(dat) Generación de las réplicas bootstrap: set.seed(1) stat.boot &lt;- numeric(nboot) for (i in 1:nboot) { dat.boot &lt;- sample(dat, replace=TRUE) stat.boot[i] &lt;- mean(dat.boot) } # Valor esperado bootstrap del estadístico mean.boot &lt;- mean(stat.boot) mean.boot ## [1] -0.09202607 Bootstrap percentil: hist(stat.boot, freq=FALSE, ylim = c(0,4)) abline(v=mean.boot, lwd=2) # abline(v=stat.dat) # Distribución poblacional curve(dnorm(x, datmed, datsd/sqrt(ndat)), lty=2, add=TRUE) abline(v=datmed, lwd=2, lty=2) Bootstrap natural/básico: hist(stat.boot-stat.dat, freq=FALSE, ylim = c(0,4)) abline(v=mean.boot-stat.dat, lwd=2) # Distribución poblacional # Distribución teórica de stat.dat - stat.teor curve(dnorm(x, 0, datsd/sqrt(ndat)), lty=2, add=TRUE) abline(v=0, lwd=2, lty=2) Sesgo y error estandar bootstrap: # sesgo (teor=0) mean.boot - stat.dat ## [1] 0.00135433 # error estandar sd(stat.boot) ## [1] 0.1027024 # error estandar teórico datsd/sqrt(ndat) ## [1] 0.1 Versión “optimizada” para R: boot.strap &lt;- function(dat, nboot=1000, statistic=mean) { ndat &lt;- length(dat) dat.boot &lt;- sample(dat, ndat*nboot, replace=T) dat.boot &lt;- matrix(dat.boot, ncol=nboot, nrow=ndat) stat.boot &lt;- apply(dat.boot, 2, statistic) } fstatistic &lt;- function(dat){ # mean(dat) mean(dat, trim=0.2) # median(dat) # max(dat) } set.seed(1) stat.dat &lt;- fstatistic(dat) stat.boot &lt;- boot.strap(dat, nboot, fstatistic) res.boot &lt;- c(stat.dat, mean(stat.boot)-stat.dat, sd(stat.boot)) names(res.boot) &lt;- c(&quot;Estadístico&quot;, &quot;Sesgo&quot;, &quot;Err.Std&quot;) res.boot ## Estadístico Sesgo Err.Std ## -0.144801929 0.006044904 0.117978183 5.6.4 Paquetes R: bootstrap, boot library(boot) # ?boot Función estadístico: boot.f &lt;- function(data, indices){ # data[indices] va a ser las muestras bootstrap mean(data[indices]) } Generación de las muestras set.seed(1) stat.boot &lt;- boot(dat, boot.f, nboot) stat.boot ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = dat, statistic = boot.f, R = nboot) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* -0.0933804 0.00135433 0.105385 names(stat.boot) ## [1] &quot;t0&quot; &quot;t&quot; &quot;R&quot; &quot;data&quot; &quot;seed&quot; ## [6] &quot;statistic&quot; &quot;sim&quot; &quot;call&quot; &quot;stype&quot; &quot;strata&quot; ## [11] &quot;weights&quot; 5.6.5 Gráficos hist(stat.boot$t, freq=FALSE) plot(stat.boot) jack.after.boot(stat.boot) 5.6.6 Intervalos de confianza bootstrap boot.ci(stat.boot, type=c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;, &quot;bca&quot;)) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = stat.boot, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;, ## &quot;bca&quot;)) ## ## Intervals : ## Level Normal Basic ## 95% (-0.3013, 0.1118 ) (-0.2900, 0.1212 ) ## ## Level Percentile BCa ## 95% (-0.3079, 0.1032 ) (-0.3258, 0.0973 ) ## Calculations and Intervals on Original Scale Ejercicio 5.5 Repetir el ejemplo anterior considerando la media recortada al 10% (ejemplo con parámetros adicionales). boot.f &lt;- function(data, indices, trim=0.1){ mean(data[indices], trim) } set.seed(1) boot(dat, boot.f, nboot, trim=0.2) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = dat, statistic = boot.f, R = nboot, trim = 0.2) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* -0.1448019 0.005146914 0.1226472 Ejercicio 5.6 fin de práctica En el tema 2 se propuso el análisis de la aleatoriedad de un generador de números pseudo-aleatorios mediante el test de rachas, que se podría implementar repetidamente. Sin embargo, la aproximación asintótica empleada por la rutina runs.test de la librería tseries no es adecuada para tamaños muéstrales pequeños (\\(n&lt;40\\)) y sería preferible utilizar la distribución exacta (o por lo menos utilizar una corrección por continuidad). Analizar el comportamiento del contraste empleando repetidamente el test de rachas, considerando 500 pruebas con muestras de tamaño 10 de una \\(Bernoulli(0.5)\\). ¿Se observa algo extraño? Realiza un programa que permita aproximar por simulación la función de masa de probabilidad del estadístico número de rachas (a partir de valores de una \\(Bernoulli(0.5)\\)). Representarla gráficamente y compararla con la densidad normal. Obtener los puntos críticos para contrastar la hipótesis nula de aleatoriedad para \\(\\alpha=0.01,0.05\\) y \\(0.1\\). ¿Es esta dístribución adecuada para el contraste de aleatoriedad de variables continuas?¿Cual debería ser la probabilidad de obtener una única racha al aplicar el test a una variable continua? Diseñar una rutina que permita realizar el contraste de aleatoriedad de una variable continua aproximando el \\(p\\)-valor por simulación. Asumir que la distribución del estadístico puede ser asimétrica, en cuyo caso el \\(p\\)-valor \\(p=2\\min\\left\\{ P\\left( R\\leq\\hat{R}\\mid H_{0}\\right) , P\\left( R\\geq \\hat{R}\\mid H_{0}\\right) \\right\\}\\). Diseñar una rutina que permita realizar el contraste de aleatoriedad de una variable continua aproximando el \\(p\\)-valor mediante bootstrap. "],
["integracion-y-optimizacion-montecarlo.html", "Capítulo 6 Integración y Optimización Montecarlo 6.1 Integración Monte Carlo (clásica) 6.2 Muestreo por importancia 6.3 Optimización Monte Carlo 6.4 Temple simulado 6.5 Algoritmos genéticos", " Capítulo 6 Integración y Optimización Montecarlo Pendiente: ejemplos y ejercicios 6.1 Integración Monte Carlo (clásica) La integración Monte Carlo se emplea principalmente para aproximar integrales multidimensionales de la forma: \\[I = \\int \\cdots \\int h\\left( x_{1},\\ldots ,x_{d}\\right) dx_{1}\\cdots dx_{d}\\] donde puede presentar ventajas respecto a los métodos tradicionales de integración numérica. Supongamos que nos interesa: \\[I = \\int_{0}^{1}h\\left( x\\right) dx\\] Si \\(x_{1},x_{2},\\ldots ,x_{n}\\) i.i.d. \\(\\mathcal{U}\\left( 0,1\\right)\\) entonces: \\[I = E\\left( h\\left( \\mathcal{U}\\left( 0,1\\right) \\right) \\right) \\approx \\frac{1}{n}\\sum\\limits_{i=1}^{n}h\\left( x_{i}\\right)\\] Si el intervalo de integración es genérico: \\[I = \\int_{a}^{b}h\\left( x\\right) dx = (b-a)\\int_{a}^{b}h\\left( x\\right) \\frac{1}{(b-a)}dx.\\] Si \\(x_{1},x_{2},\\ldots ,x_{n}\\) i.i.d. \\(\\mathcal{U}\\left( a, b\\right)\\): \\[I\\approx \\frac{1}{n}\\sum\\limits_{i=1}^{n}h\\left( x_{i}\\right) (b-a)\\] De forma más general, si nos interesa aproximar: \\[\\theta = E\\left( h\\left( X\\right) \\right) = \\int h\\left( x\\right) f(x)dx\\]siendo \\(X\\sim f\\), entonces, si \\(x_{1},x_{2},\\ldots ,x_{n}\\) i.i.d. \\(X\\): \\[\\theta \\approx \\frac{1}{n}\\sum\\limits_{i=1}^{n}h\\left( x_{i}\\right)\\] 6.2 Muestreo por importancia Para aproximar la integral: \\[\\theta = E\\left( h\\left( X\\right) \\right) = \\int h\\left( x\\right) f(x)dx,\\] puede ser preferible generar observaciones de una densidad \\(g\\) que tenga una forma similar al producto \\(hf\\). Si \\(Y\\sim g\\): \\[\\theta = \\int h\\left( x\\right) f(x)dx = \\int \\frac{h\\left( x\\right) f(x)}{g(x)}g(x)dx = E\\left( q\\left( Y\\right) \\right).\\] siendo \\(q\\left( x\\right) = \\frac{h\\left( x\\right) f(x)}{g(x)}\\). Si \\(y_{1},y_{2},\\ldots ,y_{n}\\) i.i.d. \\(Y\\sim g\\): \\[\\theta \\approx \\frac{1}{n}\\sum\\limits_{i=1}^{n}q\\left( y_{i}\\right) = \\frac{1}{n}\\sum\\limits_{i=1}^{n}w(y_{i})h\\left( y_{i}\\right) = \\hat{\\theta}_{g}\\] con \\(w\\left( x\\right) = \\frac{f(x)}{g(x)}\\). En este caso \\(Var(\\hat{\\theta}_{g}) = Var\\left( q\\left( Y\\right) \\right) /n\\), pudiendo reducirse significativamente respecto al método clásico si: \\[g(x)\\underset{aprox.}{\\propto }h\\left( x\\right) f(x).\\] Para aplicar el TCL, la varianza del estimador \\(\\hat{\\theta}_{g}\\) es finita si: \\[E\\left( q^{2}\\left( Y\\right) \\right) = \\int \\frac{h^{2}\\left( x\\right)^{2}(x)}{g(x)}dx = \\int h^{2}\\left( x\\right) \\frac{f^{2}(x)}{g(x)}dx &lt; \\infty.\\] La idea básica es que si la densidad \\(g\\) tiene colas más pesadas que la densidad \\(f\\) con mayor facilidad puede dar lugar a “simulaciones” con varianza finita (podría emplearse en casos en los que no existe \\(E \\left( h^{2} \\left( X \\right) \\right)\\); ver Sección XX en el Capítulo XX). La distribución de los pesos \\(w(y_{i})\\) debería ser homogénea para evitar datos influyentes (inestabilidad). 6.2.1 Remuestreo (del muestreo) por importancia Cuando \\(f\\) y/o \\(g\\) son cuasi-densidades, para evitar calcular ctes normalizadoras, se emplea como aproximación: \\[\\theta \\approx \\frac{\\sum\\limits_{i=1}^{n}w(y_{i})h\\left( y_{i}\\right) }{ \\sum\\limits_{i=1}^{n}w(y_{i})}.\\] Adicionalmente, puede verse que con un muestreo de \\(\\left\\{y_{1},y_{2},\\ldots ,y_{n}\\right\\}\\) ponderado por \\(w(y_{i})\\) (prob. \\(=w(y_{i})\\left/ \\sum\\nolimits_{i=1}^{n}w(y_{i}) \\right.\\) ) se obtiene una simulación aproximada de \\(f\\) (Sample importance resampling, Rubin, 1987). 6.3 Optimización Monte Carlo Supongamos que estamos interesados en la minimización de una función: \\[\\underset{\\mathbf{x}\\in D}{\\min }f(\\mathbf{x}).\\] Hay una gran cantidad de algoritmos numéricos para resolver problemas de optimización no lineal multidimensional, por ejemplo los basados en el método de Newton-Raphson. La idea original consiste en buscar los ceros de su primera derivada (o del gradiente) empleando una aproximación iterativa: \\[\\mathbf{x}_{i+1} = \\mathbf{x}_{i}-[Hf(\\mathbf{x}_{i})]^{-1}\\nabla f(\\mathbf{x} = _{i}),\\]donde \\(Hf(\\mathbf{x}_{i})\\) es el hessiano de la función (matriz de segundas derivadas) y \\(\\nabla f(\\mathbf{x}_{i})\\) el gradiente (vector de primeras derivadas). Estos métodos normalmente funcionan muy bien cuando la función objetivo no tiene mínimos locales (ideal \\(f\\) cuadrática). Los resultados obtenidos pueden ser muy malos en caso contrario (especialmente en el caso multidimensional) y dependen en gran medida del punto inicial. Un ejemplo donde es habitual que aparezcan este tipo de problemas es en la estimación por máxima verosimilitud (la función objetivo puede ser multimodal). Una alternativa sería tratar de generar valores aleatoriamente de forma que las regiones donde la función objetivo es menor tuviesen mayor probabilidad y menor probabilidad las regiones donde la función objetivo es mayor. Por ejemplo, se podría pensar en generar valores de acuerdo a una densidad (tranformación Boltzman-Gibbs): \\[g(x)\\propto \\exp \\left( -f(x)/T\\right) ,\\]donde \\(T&gt;0\\) es un parámetro (denominado temperatura) seleccionado de forma que se garantize la integrabilidad. Entre los métodos de optimización Monte Carlo podríamos destacar: Métodos con gradiente aleatorio. Temple simulado. Algoritmos genéticos. Montecarlo EM. … 6.4 Temple simulado Método inspirado en el templado de un metal (se calienta el metal a alta temperatura y se va enfriando lentamente). En cada paso se reemplaza la aproximación actual por un valor aleatorio “cercano”, elegido con una probabilidad que depende de la mejora en la función objetivo y de un parámetro \\(T\\) (denominado temperatura) que disminuye gradualmente durante el proceso. Cuando la temperatura es grande los cambios son bastante probables en cualquier dirección. Al ir disminuyendo la temperatura los cambios tienden a ser siempre “cuesta abajo”. Al tener una probabilidad no nula de aceptar una modificación “cuesta arriba” se trata de evitar quedar atrapado en un óptimo local. Algoritmo: # Algoritmo temp &lt;- TEMP.INIT place &lt;- INIT.PLACEMENT() cost.place &lt;- COST(place) while(temp &lt; TEMP.FINAL) { while(LOOP.CRITERION()) { place.new &lt;- PERTURB(place, temp) cost.new &lt;- COST(place.new) cost.inc &lt;- cost.new - cost.place temp &lt;- SCHEDULE(temp) if ((cost.inc &lt; 0) || (runif(1) &gt; exp(-(cost.inc/temp)))) break } place &lt;- place.new cost.place &lt;- cost.new # temp &lt;- SCHEDULE(temp) } COST &lt;- function(place, ...) {...} SCHEDULE &lt;- function(temp, ...) {...} INIT.PLACEMENT &lt;- function(...) {...} LOOP.CRITERION &lt;- function(...) {...} Adaptado de Premchand Akella (ppt). Este algoritmo se puede ver como una adaptación del método de Metropolis-Hastings que veremos más adelante (Sección XX). 6.5 Algoritmos genéticos Los algoritmos genéticos tratan de encontrar la mejor solución (entre un conjunto de soluciones posibles) imitando los procesos de evolución biológica: Población: formada por \\(n\\) individuos \\(\\mathbf{x}_{i}\\) codificados en cromosomas. \\(f(\\mathbf{x}_{i})\\) ajuste/capacidad/adaptación del individuo \\(\\mathbf{x}_{i}\\). Selección: los individuos mejor adaptados tienen mayor probabilidad de ser padres. Cruzamiento: los cromosomas de dos padres se combinan para generar hijos. Mutación: modificación al azar del cromosoma de los hijos (variabilidad). Elitismo: el mejor individuo pasa a la siguiente generación. Los paquetes de R DEOptim y gafit implementan algunos de estos tipos de algoritmos. "],
["simulacion-de-variables-continuas.html", "Capítulo 7 Simulación de variables continuas 7.1 Método de inversión 7.2 Método de aceptación-rechazo 7.3 Ejercicios 7.4 Métodos específicos para generación de distribuciones notables", " Capítulo 7 Simulación de variables continuas Pendiente: ejemplos y teoría 7.1 Método de inversión Ejercicio 5.1 La distribución doble exponencial (o distribución de Laplace) de parámetro \\(\\lambda\\) tiene función de densidad: \\[f\\left( x\\right) =\\frac{\\lambda}{2}e^{-\\lambda\\left\\vert x\\right\\vert }\\text{, }x\\in\\mathbb{R}\\] y función de distribución: \\[F\\left( x\\right) =\\int_{-\\infty}^{x}f\\left( t\\right) dt=\\left\\{ \\begin{array}{ll} \\frac{1}{2}e^{\\lambda x} &amp; x&lt;0\\\\ 1-\\frac{1}{2}e^{-\\lambda x} &amp; x\\geq0 \\end{array} \\ \\right.\\] Escribir una función que permita generar, por el método de inversión, una muestra de \\(n\\) observaciones de esta distribución (NOTA: esta distribución puede generarse fácilmente simulando una distribución exponencial y otorgarle un signo positivo o negativo con equiprobabilidad; ver método de composición). ddexp &lt;- function(x, lambda = 1){ # Densidad doble exponencial lambda*exp(-lambda*abs(x))/2 } rdexp &lt;- function(lambda = 1){ # Simulación por inversión # Doble exponencial U &lt;- runif(1) if (U&lt;0.5) { return(log(2*U)/lambda) } else { return(-log(2*(1-U))/lambda) } } rdexpn &lt;- function(n = 1000, lambda = 1) { # Simulación n valores de doble exponencial x &lt;- numeric(n) for(i in 1:n) x[i]&lt;-rdexp(lambda) return(x) } Generar \\(10^{4}\\) valores de la distribución doble exponencial de parámetro \\(\\lambda=2\\) y obtener el tiempo de CPU que tarda en generar la secuencia. set.seed(54321) system.time(x &lt;- rdexpn(10^4, 2)) ## user system elapsed ## 0.15 0.06 0.16 Representar el histograma y compararlo con la densidad teórica. hist(x, breaks = &quot;FD&quot;, freq = FALSE) # lines(density(x), col = &#39;blue&#39;) curve(ddexp(x, 2), add = TRUE) 7.2 Método de aceptación-rechazo Ejercicio 7.1 Desarrollar el código necesario para generar, por el método de aceptación-rechazo, una muestra de \\(n\\) observaciones de una distribución normal estándar: \\[f\\left( x\\right) =\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^{2}}{2}}\\text{, }x\\in\\mathbb{R}\\text{, }\\] empleando como distribución auxiliar una doble exponencial. Puede verse que la elección de la densidad auxiliar óptima se corresponde con \\(\\lambda=1\\) y que la cota optima es:\\[c_{\\text{opt}}=\\sqrt{\\frac{2e}{\\pi}}\\simeq1.\\,3155.\\] Para esteblecer la condición de aceptación o rechazo se puede tener en cuenta que: \\[c\\cdot U\\cdot\\frac{g\\left( T\\right) }{f\\left( T\\right) }=\\sqrt{\\frac {2e}{\\pi}}U\\sqrt{\\frac{\\pi}{2}}\\exp\\left( \\frac{T^{2}}{2}-\\left\\vert T\\right\\vert \\right) =U\\cdot\\exp\\left( \\frac{T^{2}}{2}-\\left\\vert T\\right\\vert +\\frac{1}{2}\\right) ,\\] aunque en general puede ser recomendable emplear \\(c\\cdot U\\cdot g\\left( T\\right) \\leq f\\left( T\\right)\\). # densidad objetivo: dnorm # densidad auxiliar: ddexp # EJECUTAR CÓDIGO DEL APARTADO A DEL EJERCICIO 1 c.opt &lt;- sqrt(2*exp(1)/pi) lambda.opt &lt;- 1 ngen &lt;- 0 rnormAR &lt;- function() { # Simulación por aceptación-rechazo # Normal estandar a partir de doble exponencial while (TRUE) { U &lt;- runif(1) X &lt;- rdexp(1) ngen &lt;&lt;- ngen+1 # Comentar esta línea para uso normal # if (U*exp((X^2+1)*0.5-abs(X)) &lt;= 1) return(X) if (c.opt * U * ddexp(X, lambda.opt) &lt;= dnorm(X)) return(X) } } rnormARn &lt;- function(n=1000) { # Simulación n valores N(0,1) x &lt;- numeric(n) for(i in 1:n) x[i]&lt;-rnormAR() return(x) } # Grafico curve(c.opt * ddexp(x), xlim = c(-4, 4), lty = 2) curve(dnorm(x), add = TRUE) Generar una muestra de \\(10^{4}\\) observaciones empleando este algoritmo. Obtener el tiempo de CPU y calcular el número medio de generaciones de la distribución auxiliar. set.seed(54321) nsim &lt;- 10^4 ngen &lt;- 0 system.time(x &lt;- rnormARn(nsim)) ## user system elapsed ## 0.08 0.00 0.08 # Nº generaciones { cat(&quot;\\nNº de generaciones = &quot;, ngen) cat(&quot;\\nNº medio de generaciones = &quot;, ngen/nsim) cat(&quot;\\nProporción de rechazos = &quot;, 1-nsim/ngen, &quot;\\n&quot;) } ## ## Nº de generaciones = 13163 ## Nº medio de generaciones = 1.3163 ## Proporción de rechazos = 0.2402948 Representar el histograma y compararlo con la densidad teórica. hist(x, breaks=&quot;FD&quot;, freq=FALSE) curve(dnorm(x), add=TRUE) Aproximar la cota óptima numéricamente. # Obtención de un valor c óptimo aproximado optimize(f=function(x){dnorm(x)/ddexp(x)}, maximum=TRUE, interval=c(-1,1)) ## $maximum ## [1] -0.999959 ## ## $objective ## [1] 1.315489 # NOTA: Cuidado con los límites # optimize(f=function(x){dnorm(x)/ddexp(x)}, maximum=TRUE, interval=c(0,2)) # Valor óptimo real # sqrt(2*exp(1)/pi) c.opt ## [1] 1.315489 Aproximar el parámetro óptimo de la densidad auxiliar numéricamente (normalmente comenzaríamos por este paso). # Obtención de valores c y lambda óptimos aproximados fopt &lt;- function(lambda) { # Obtiene c fijado lambda optimize(f = function(x){dnorm(x)/ddexp(x,lambda)}, maximum=TRUE, interval=c(0,2))$objective } # Encontar lambda que minimiza res &lt;- optimize(f=function(x){fopt(x)}, interval=c(0.5,2)) lambda.opt2 &lt;- res$minimum c.opt2 &lt;- res$objective Ejercicio 7.2 Para la estimación Bayes de la media de una normal se suele utilizar como distribución a priori una Cauchy. Generar una muestra i.i.d. \\(X_{i}\\sim N(\\theta_{0},1)\\) de tamaño \\(n=10\\) con \\(\\theta_{0}=1\\). Utilizar una \\(Cauchy(0,1)\\) (rcauchy) como distribución a priori y como densidad auxiliar para simular por aceptación-rechazo una muestra de la densidad a posteriori (emplear dnorm para construir la verosimilitud). Obtener el intervalo de probabilidad al 95%. mu0 &lt;- 1 n &lt;- 10 nsim &lt;- 10^3 set.seed(54321) x &lt;- rnorm(n, mean = mu0) # Función de verosimilitud lik &lt;- function(mu){prod(dnorm(x, mean = mu))} # Cota óptima # Estimación por máxima verosimilitud emv &lt;- optimize(f = lik, int = range(x), maximum = TRUE) emv ## $maximum ## [1] 0.7353805 ## ## $objective ## [1] 3.303574e-08 c &lt;- emv$objective mean(x) ## [1] 0.7353958 c &lt;- lik(mean(x)) c ## [1] 3.303574e-08 # Simulación distribución a posteriori # Simulación por aceptación-rechazo a partir de Cauchy(0,1) ngen &lt;- nsim Y &lt;- rcauchy(nsim) ind &lt;- (c*runif(nsim) &gt; sapply(Y, lik)) # TRUE si no verifica condición # Volver a generar si no verifica condición while (sum(ind)&gt;0){ le &lt;- sum(ind) ngen &lt;- ngen + le Y[ind] &lt;- rcauchy(le) ind[ind] &lt;- (c*runif(le) &gt; sapply(Y[ind], lik)) # TRUE si no verifica condición } { # Nº generaciones cat(&quot;\\nNº de generaciones = &quot;, ngen) cat(&quot;\\nNº medio de generaciones = &quot;, ngen/nsim) cat(&quot;\\nProporción de rechazos = &quot;, 1-nsim/ngen,&quot;\\n&quot;) } ## ## Nº de generaciones = 5898 ## Nº medio de generaciones = 5.898 ## Proporción de rechazos = 0.830451 # Intervalo de probabilidad al 95% (IC Bayes) q &lt;- quantile(Y, c(0.025, 0.975)) # Representar estimador e IC Bayes hist(Y, freq=FALSE, main=&quot;Distribución a posteriori&quot;) # abline(v = mean(x), lty = 3) # Estimación frecuentista abline(v = mean(Y), lty = 2, lwd = 2) # Estimación Bayesiana abline(v = q, lty = 2) Repetir el apartado anterior con \\(n=100\\). 7.3 Ejercicios Ejercicio 7.3 para entregar Escribir el código necesario para generar, por el método de inversión, una muestra de \\(n\\) observaciones de una distribución de Cauchy. Generar una muestra de \\(10^{4}\\) observaciones y obtener el tiempo de CPU. Representar el histograma (limitar el rango, e.g. xlim = c(-10, 10)) y compararlo con la densidad teórica (dcauchy). Obtener conclusiones sobre la existencia de una media teórica a partir de la media muestral aproximada por simulación (estudiar la convergencia de la media muestral). Suponiendo que el vector x contiene las simulaciones, estudiar la convergencia de la media muestral mediante el gráfico:plot(1:nsim, cumsum(x)/(1:nsim), type=&quot;l&quot;, ylab=&quot;Media muestral&quot;, xlab=&quot;Nº de simulaciones&quot;) Ejercicio 2.4 para entregar El tiempo de respuesta (en centésimas de segundo) de un servidor de bases de datos es una variable con función de densidad:\\[f(x)=xe^{-x}\\text{ si }x\\geq0.\\] Escribir el código necesario para generar, por el método de aceptación-rechazo, una muestra de \\(n\\) observaciones de esta distribución empleando como densidad auxiliar una exponencial:\\[g(x)=\\lambda e^{-\\lambda x}\\text{ si }x\\geq0.\\] Aproximar numéricamente el parámetro óptimo (\\(\\lambda_{opt}&lt;1\\)) y la cota óptima (\\(c_{opt}\\)) de la densidad auxiliar y compararlos con los valores teóricos: \\(\\lambda_{opt}=1/2\\) y \\(c_{opt}=4/e\\). Generar una muestra de 1000 observaciones de la distribución de interés (tomando como semilla inicial el nº de grupo multiplicado por 100). Obtener el tiempo de CPU que tarda en generar la secuencia y calcular el número medio de generaciones de la distribución auxiliar. Representar el histograma y compararlo con la densidad teórica. 7.4 Métodos específicos para generación de distribuciones notables Pendiente: algunos ejemplos del libro de Ricardo y de las funciones implementadas en R "],
["simulacion-de-variables-discretas.html", "Capítulo 8 Simulación de variables discretas 8.1 Ejercicios 8.2 Métodos específicos para generación de distribuciones notables", " Capítulo 8 Simulación de variables discretas Pendiente: ejemplos y teoría 8.1 Ejercicios Ejercicio 5.1 Se pretende simular valores de una distribución \\(\\mathcal{B}(10,0.5)\\). Para ello: Generar, por el método de la transformación cuantil usando búsqueda secuencial, una muestra de \\(nsim=10^{5}\\) observaciones de esta variable. Obtener el tiempo de CPU empleado. Aproximar por simulación la función de masa de probabilidad, representarla gráficamente y compararla con la teórica. Calcular también la media muestral (compararla con la teórica \\(np\\)) y el número medio de comparaciones para generar cada observación. rfmp &lt;- function(x, prob = 1/length(x), nsim = 1000) { # Simulación nsim v.a. discreta a partir de fmp # por inversión generalizada (transformación cuantil) # Inicializar FD Fx &lt;- cumsum(prob) # Simular X &lt;- numeric(nsim) U &lt;- runif(nsim) for(j in 1:nsim) { i &lt;- 1 while (Fx[i] &lt; U[j]) i &lt;- i + 1 X[j] &lt;- x[i] ncomp &lt;&lt;- ncomp + i } return(X) } # Simular binomial set.seed(54321) n &lt;- 10 p &lt;- 0.5 nsim &lt;- 10^5 x &lt;- 0:n fmp &lt;- dbinom(x, n, p) ncomp &lt;- 0 system.time( rx &lt;- rfmp(x, fmp, nsim) ) ## user system elapsed ## 0.07 0.00 0.06 # Aproximación media mean(rx) ## [1] 5.00322 n*p ## [1] 5 # Número de comparaciones ncomp/nsim ## [1] 6.00322 sum((1:length(x))*fmp) # Valor teórico ## [1] 6 # Análisis resultados res &lt;- as.data.frame(table(rx)/nsim) names(res) &lt;- c(&quot;x&quot;, &quot;psim&quot;) # Comparación teórica plot(as.matrix(res), type=&quot;h&quot;) points(x, fmp, pch=4) res$pteor &lt;- fmp res ## x psim pteor ## 1 0 0.00107 0.0009765625 ## 2 1 0.00990 0.0097656250 ## 3 2 0.04432 0.0439453125 ## 4 3 0.11778 0.1171875000 ## 5 4 0.20425 0.2050781250 ## 6 5 0.24375 0.2460937500 ## 7 6 0.20454 0.2050781250 ## 8 7 0.11898 0.1171875000 ## 9 8 0.04419 0.0439453125 ## 10 9 0.01023 0.0097656250 ## 11 10 0.00099 0.0009765625 # Errores max(abs(res$psim - res$pteor)) ## [1] 0.00234375 max(abs(res$psim - res$pteor) / res$pteor) ## [1] 0.09568 # NOTA: Puede ocurrir que no todos los valores sean generados en la simulación # Si length(x) &gt; length(psim) el código anterior (res$pteor &lt;- fmp) producirá un error # Alternativamente: res &lt;- data.frame(x = x, pteor = fmp, psim = 0) res.sim &lt;- table(rx)/nsim index &lt;- match(names(res.sim), x) res$psim[index] &lt;- res.sim # entre muchas otras posibilidades... Repetir el apartado anterior ordenando previamente las probabilidades en orden decreciente y también empleando la función sample de R. tini &lt;- proc.time() ncomp &lt;- 0 ind &lt;- order(fmp, decreasing=TRUE) rx &lt;- rfmp(x[ind], fmp[ind], nsim) tiempo &lt;- proc.time() - tini tiempo ## user system elapsed ## 0.05 0.02 0.04 # Número de comparaciones ncomp/nsim ## [1] 3.08969 sum((1:length(x))*fmp[ind]) # Valor teórico ## [1] 3.083984 Como se comentó en el Capítulo de Números aleatorios en R en R se recomienda emplear la función sample (implementa eficientemente el método de Alias, descrito en el apartado d)): sample(x, size, replace = FALSE, prob = NULL). system.time( rx &lt;- sample(x, nsim, replace = TRUE, prob = fmp) ) ## user system elapsed ## 0 0 0 # Si n &gt;&gt; 30 # Aproximación normal con corrección continuidad mean &lt;- n*p sd &lt;- sqrt(n*p*(1-p)) # curve(dnorm(x, mean, sd), add = TRUE, col=&#39;blue&#39;) res$pnorm &lt;- pnorm(x+0.5,mean,sd)-pnorm(x-0.5,mean,sd) res ## x pteor psim pnorm ## 1 0 0.0009765625 0.00107 0.001961154 ## 2 1 0.0097656250 0.00990 0.011215085 ## 3 2 0.0439453125 0.04432 0.043494801 ## 4 3 0.1171875000 0.11778 0.114467707 ## 5 4 0.2050781250 0.20425 0.204523961 ## 6 5 0.2460937500 0.24375 0.248170366 ## 7 6 0.2050781250 0.20454 0.204523961 ## 8 7 0.1171875000 0.11898 0.114467707 ## 9 8 0.0439453125 0.04419 0.043494801 ## 10 9 0.0097656250 0.01023 0.011215085 ## 11 10 0.0009765625 0.00099 0.001961154 max(abs(res$pnorm - res$pteor)) ## [1] 0.002719793 max(abs(res$pnorm - res$pteor) / res$pteor) ## [1] 1.008222 system.time( rx &lt;- pmax(0, pmin(n, round( rnorm(nsim, mean, sd) ))) ) ## user system elapsed ## 0.02 0.00 0.01 table(rx)/nsim ## rx ## 0 1 2 3 4 5 6 7 8 ## 0.00216 0.01107 0.04317 0.11406 0.20454 0.24924 0.20373 0.11500 0.04352 ## 9 10 ## 0.01121 0.00230 # Realmente las prob de los extremos serían mayores... res$pnorm[1] &lt;- pnorm(0.5, mean, sd) res$pnorm[n+1] &lt;- 1 - pnorm(n-0.5, mean, sd) Diseñar una rutina que permita generar \\(nsim\\) valores de una distribución discreta usando una tabla guía. Repetir los pasos de los apartados anteriores empleando esta rutina (con \\(m=n-1\\)). rfmp.tabla &lt;- function(x, prob = 1/length(x), m, nsim = 1000) { # Simulación v.a. discreta a partir de función de masa de probabilidad # por tabla guia de tamaño m # Inicializar tabla y FD Fx &lt;- cumsum(prob) g &lt;- rep(1,m) i &lt;- 1 for(j in 2:m) { while (Fx[i] &lt; (j-1)/m) i &lt;- i+1 g[j] &lt;- i } # Generar valores X &lt;- numeric(nsim) U &lt;- runif(nsim) for(j in 1:nsim) { i &lt;- g[floor(U[j]*m)+1] while (Fx[i] &lt; U[j]) i &lt;- i + 1 X[j] &lt;- x[i] } return(X) } system.time( rx &lt;- rfmp.tabla(x, fmp, n-1, nsim) ) ## user system elapsed ## 0.08 0.02 0.06 # Análisis resultados res &lt;- as.data.frame(table(rx)/nsim) names(res) &lt;- c(&quot;x&quot;, &quot;psim&quot;) # Comparación teórica plot(as.matrix(res), type=&quot;h&quot;) points(x, fmp, pch=4) Diseñar una rutina que permita generar \\(nsim\\) valores de una distribución discreta usando el método de Alias. Repetir los pasos de los apartados anteriores empleando esta rutina. Comparar los resultados obtenidos. rfmp.alias &lt;- function(x, prob = 1/length(x), nsim = 1000) { # Inicializar tablas a &lt;- numeric(length(x)) q &lt;- prob*length(x) low &lt;- q &lt; 1 high &lt;- which(!low) low &lt;- which(low) while (length(high) &amp;&amp; length(low)) { l &lt;- low[1] h &lt;- high[1] a[l] &lt;- h q[h] &lt;- q[h] - (1 - q[l]) if (q[h] &lt; 1) { high &lt;- high[-1] low[1] &lt;- h } else low &lt;- low[-1] } # while # Generar valores V &lt;- runif(nsim) i &lt;- floor(runif(nsim)*length(x)) + 1 return( x[ ifelse( V &lt; q[i], i, a[i]) ] ) } system.time( rx &lt;- rfmp.alias(x,fmp,nsim) ) ## user system elapsed ## 0.03 0.01 0.03 # Análisis resultados res &lt;- as.data.frame(table(rx)/nsim) names(res) &lt;- c(&quot;x&quot;, &quot;psim&quot;) # Comparación teórica plot(as.matrix(res), type=&quot;h&quot;) points(x, fmp, pch=4) Ejercicio 2.2 Considera la variable aleatoria con función de distribución dada por: \\[F(x)=\\left\\{ \\begin{array} [c]{cl}0 &amp; \\mbox{si $x&lt;0$}\\\\ \\frac{x}{2}+\\frac{1}{10} &amp; \\mbox{si $x\\in[0,\\frac{1}{5})$}\\\\ x+\\frac{1}{10} &amp; \\mbox{si $x\\in[\\frac{1}{5},\\frac{9}{10}]$}\\\\ 1 &amp; \\mbox{en otro caso} \\end{array} \\right.\\] Función de distribución: fdistr &lt;- function(x) { ifelse(x &lt; 0, 0, ifelse(x &lt; 1/5, x/2 + 1/10, ifelse(x &lt;= 9/10, x + 1/10, 1) ) ) } # Empleando ifelse se complica un poco más pero el resultado es una función vectorial. curve(fdistr(x), from = -0.1, to = 1.1, type = &#39;s&#39;, main = &#39;Función de distribución&#39;) # Discontinuidades en 0 y 1/5 abline(h = c(1/10, 2/10, 3/10), lty = 2) Nota: Esta variable toma los valores 0 y 1/5 con probabilidad 2/10 = 1/5. Diseña un algoritmo basándote en el método de inversión generalizado para generar observaciones de esta variable. El algoritmo general es siempre el mismo. Empleando la función cuantil: \\[Q\\left( u\\right) = \\inf \\left\\{ x\\in \\mathbb{R}:F\\left( x\\right) \\geq u\\right\\},\\] el algoritmo sería: Generar \\(U\\sim \\mathcal{U}\\left( 0,1\\right)\\) Devolver \\(X=Q\\left( U\\right)\\) En este caso concreto: Generar \\(U\\sim \\mathcal{U}\\left( 0,1\\right)\\) Si \\(U &lt; \\frac{1}{10}\\) devolver \\(X = 0\\) Si \\(U &lt; \\frac{2}{10}\\) devolver \\(X = 2(U - \\frac{1}{10})\\) Si \\(U &lt; \\frac{3}{10}\\) devolver \\(X = \\frac{2}{10}\\) En caso contrario devolver \\(X = U - \\frac{1}{10}\\) Implementa el algoritmo en una función que permita generar \\(nsim\\) valores de la variable. # Función cuantil: fquant &lt;- function(u) { ifelse(u &lt; 1/10, 0, ifelse(u &lt; 2/10, 2*(u - 1/10), ifelse(u &lt; 3/10, 2/10, u - 1/10) ) ) } # Función para generar nsim valores: rx &lt;- function(nsim) fquant(runif(nsim)) Ejemplo: set.seed(1) nsim &lt;- 10^4 system.time(simx &lt;- rx(nsim)) ## user system elapsed ## 0 0 0 hist(simx, breaks = &quot;FD&quot;, freq = FALSE) En este caso como no es una variable absolutamente continua mejor emplear la función de distribución para compararla con la teórica: curve(ecdf(simx)(x), from= -0.1, to = 1.1, type = &quot;s&quot;) curve(fdistr(x), type = &quot;s&quot;, lty = 2, add = TRUE) Ejercicio 7.3 fin de práctica Se pretende simular \\(nsim=10^{4}\\) observaciones de una variable hipergeométrica (dhyper(x, m, n, k)) de parámetros \\(m=\\) el nº de grupo multiplicado por 10, \\(n=100-m\\) y \\(k=20\\) Comprobar que el rango de posibles valores de esta variable es max(0, k-n):min(m, k). Generar los valores empleando el método de la transformación cuantil usando búsqueda secuencial. Obtener el tiempo de CPU empleado. Aproximar por simulación la función de masa de probabilidad, representarla gráficamente y compararla con la teórica. Calcular también la media muestral (compararla con la teórica \\(km/(m+n)\\)) y el número medio de comparaciones para generar cada observación. Repetir el apartado anterior ordenando previamente las probabilidades en orden decreciente y también: empleando la función sample de R, mediante una tabla guía (con \\(k-1\\) subintervalos) y usando el método de Alias. 8.2 Métodos específicos para generación de distribuciones notables Pendiente: algunos ejemplos del libro de Ricardo y de las funciones implementadas en R "],
["simulacion-de-distribuciones-multidimensionales.html", "Capítulo 9 Simulación de Distribuciones Multidimensionales 9.1 Introducción 9.2 Factorización de la matriz de covarianzas 9.3 Simulación condicional e incondicional 9.4 Simulación basada en cópulas 9.5 Simulación de distribuciones multidimensionales discretas", " Capítulo 9 Simulación de Distribuciones Multidimensionales Pendiente: ejemplos y teoría 9.1 Introducción Las funciones implementadas en el paquete base de R permiten simular fácilmente en el caso independiente: f1 &lt;- function(x) dnorm(x) # 2/pi*sqrt(1-x^2) # ifelse(abs(x) &lt; 1, 2/pi*sqrt(1-x^2), 0) f2 &lt;- function(x) dnorm(x, -0.5, 0.5) curve(f1, -3, 3, ylim = c(0, f2(-0.5)), ylab = &quot;f_i(x)&quot;) curve(f2, add = TRUE, lty = 2) rnorm(2, c(0, -0.5), c(1, 0.5)) ## [1] 0.3877025 -0.1671189 9.2 Factorización de la matriz de covarianzas Ejercicio 2.1 Considerar la variable funcional: \\[Y(x)=\\sin\\left( 2\\pi x\\right) +\\varepsilon\\left( x\\right)\\] con \\(0\\leq x\\leq1\\) y \\(Cov(\\varepsilon\\left( x\\right) ,\\varepsilon\\left( y\\right) )=e^{-\\left\\Vert x-y\\right\\Vert }\\). Obtener una muestra de tamaño 100 de esta variable considerando 50 puntos de discretización. # Datos funcionales (proceso temporal) n &lt;- 100 p &lt;- 50 x &lt;- seq(0, 1, length = p) # Media mu &lt;- sin(2*pi*x) # Covarianzas x.dist &lt;- as.matrix(dist(x)) x.cov &lt;- exp(-x.dist) # Factorización de la matriz de covarianzas U &lt;- chol(x.cov) L &lt;- t(U) # Simulación: # mu + t(U) %*% rnorm(p) mu + L %*% rnorm(p) ## [,1] ## 1 -0.70013677 ## 2 -0.82707461 ## 3 -0.51768109 ## 4 -0.45267791 ## 5 -0.05590327 ## 6 0.24120142 ## 7 0.44279784 ## 8 0.64979563 ## 9 0.95028658 ## 10 0.85826801 ## 11 0.78349572 ## 12 0.80538373 ## 13 0.99452860 ## 14 1.04000498 ## 15 0.86405263 ## 16 0.78418018 ## 17 0.51289803 ## 18 0.54843428 ## 19 0.52063491 ## 20 0.33888695 ## 21 0.17065623 ## 22 0.41228392 ## 23 0.53684951 ## 24 0.60726887 ## 25 0.35999987 ## 26 0.35726215 ## 27 0.21982231 ## 28 -0.24149036 ## 29 -0.07400991 ## 30 -0.25546586 ## 31 -0.69647164 ## 32 -1.17587919 ## 33 -1.01362840 ## 34 -0.67732362 ## 35 -0.92254123 ## 36 -0.94091824 ## 37 -0.88191254 ## 38 -0.61215685 ## 39 -0.49156551 ## 40 -0.54066508 ## 41 -0.67035743 ## 42 -0.35345849 ## 43 -0.19572609 ## 44 -0.08843785 ## 45 -0.11021688 ## 46 0.23167928 ## 47 0.15872403 ## 48 -0.09772555 ## 49 0.18087885 ## 50 0.63848857 # Simulación set.seed(54321) z &lt;- matrix(rnorm(n * p), nrow = p) # y &lt;- mu + t(U) %*% z y &lt;- mu + L %*% z matplot(x, y, type = &quot;l&quot;) lines(x, mu, lwd=2) Alternativamente se podría emplear, por ejemplo, la funcion mvrnorm del paquete MASS que emplea la factorización espectral (eigen) library(MASS) y &lt;- mvrnorm(100, mu, x.cov) matplot(x, t(y), type = &quot;l&quot;) lines(x, mu, lwd=2) 9.3 Simulación condicional e incondicional Ejercicio 7.1 Considerando un proceso espacial bidimensional normal \\(Z(\\mathbf{s})\\equiv Z(x,y)\\) de media 0 y covariograma exponencial: \\[Cov(Z(\\mathbf{s}_{1}),Z(\\mathbf{s}_{2})) = C(\\left\\Vert \\mathbf{s}_{1}-\\mathbf{s}_{2}\\right\\Vert ) = e^{-\\left\\Vert \\mathbf{s}_{1}-\\mathbf{s}_{2}\\right\\Vert }.\\] Nota: Puede ser de utilidad emplear herramientas del paquete geoR. library(geoR) Obtener una simulación del proceso en las posiciones \\(\\left\\{(0,0),(0,1),(1,0),(1,1)\\right\\}.\\) # SIMULACIÓN INCONDICIONAL # Posiciones datos nx &lt;- c(2,2) n &lt;- prod(nx) data.s &lt;- expand.grid(x = seq(0, 1, l=nx[1]), y = seq(0, 1, l=nx[2])) plot(data.s, type = &quot;p&quot;, pch = 20) # Representar posiciones # Modelo de dependencia curve(cov.spatial(x, cov.pars=c(1,1)), from = 0, to = 3, xlab = &quot;distancia&quot;, ylab = &quot;covarianza&quot;, ylim = c(0,1), main = &quot;modelo de dependencia&quot;) abline(h = 0, lty = 3) abline(v = 1, lty = 3) # Matriz de varianzas covarianzas cov.matrix &lt;- varcov.spatial(coords=data.s, cov.pars=c(1,1))$varcov cov.matrix ## [,1] [,2] [,3] [,4] ## [1,] 1.0000000 0.3678794 0.3678794 0.2431167 ## [2,] 0.3678794 1.0000000 0.2431167 0.3678794 ## [3,] 0.3678794 0.2431167 1.0000000 0.3678794 ## [4,] 0.2431167 0.3678794 0.3678794 1.0000000 # Simular valores set.seed(54321) L &lt;- t(chol(cov.matrix)) # Bucle simulación nsim &lt;- 1 # 1000 for (i in 1:nsim) { z &lt;- L %*% rnorm(n) # calcular estadísticos, errores,... } z ## [,1] ## [1,] -0.1789007 ## [2,] -0.9287775 ## [3,] -0.8967493 ## [4,] -1.9876270 # Simular utilizando geoR z &lt;- grf(n, grid=&quot;reg&quot;, cov.pars=c(1,1)) ## grf: generating grid 2 * 2 with 4 points ## grf: process with 1 covariance structure(s) ## grf: nugget effect is: tausq= 0 ## grf: covariance model 1 is: exponential(sigmasq=1, phi=1) ## grf: decomposition algorithm used is: cholesky ## grf: End of simulation procedure. Number of realizations: 1 names(z) ## $coords ## [1] &quot;x&quot; &quot;y&quot; ## ## $data ## [1] &quot;data&quot; ## ## $borders ## [1] &quot;borders&quot; ## ## $other ## [1] &quot;cov.model&quot; &quot;nugget&quot; &quot;cov.pars&quot; &quot;kappa&quot; ## [5] &quot;lambda&quot; &quot;aniso.pars&quot; &quot;method&quot; &quot;.Random.seed&quot; ## [9] &quot;messages&quot; &quot;call&quot; z$coords ## x y ## [1,] 0 0 ## [2,] 1 0 ## [3,] 0 1 ## [4,] 1 1 z$data ## [1] -0.4080665 -1.1688230 -1.8384001 1.3498610 Generar simulaciones en una rejilla regular \\(10\\times10\\) en el cuadrado unidad \\([0,1] \\times [0,1]\\) condicionadas a los valores generados en el apartado anterior. # SIMULACIÓN CONDICIONAL nnx &lt;- c(10,10) nn &lt;- prod(nnx) ndata.s &lt;- expand.grid(x=seq(0, 1, l = nnx[1]), y = seq(0, 1, l = nnx[2])) plot(data.s, type = &quot;p&quot;, pch = 20) # Representar posiciones points(ndata.s) set.seed(54321) s.out &lt;- output.control(n.predictive = 100) kc &lt;- krige.conv(z, loc = ndata.s, krige = krige.control(cov.pars = c(1,1)),output = s.out) ## krige.conv: results will be returned only for prediction locations inside the borders ## krige.conv: model with constant mean ## krige.conv: sampling from the predictive distribution (conditional simulations) ## krige.conv: Kriging performed using global neighbourhood # Generar gráficos par.old &lt;- par(mfrow=c(2,2), mar=c(3.5,3.5,1,0), mgp=c(1.5,.5,0)) zlim &lt;- range(kc$simul[,1:4]) # Escala común image(kc, val=kc$simul[,1], main=&quot;simul. cond. 1&quot;, zlim=zlim) image(kc, val=kc$simul[,2], main=&quot;simul. cond. 2&quot;, zlim=zlim) image(kc, val=kc$simul[,3], main=&quot;simul. cond. 3&quot;, zlim=zlim) image(kc, val=kc$simul[,4], main=&quot;simul. cond. 3&quot;, zlim=zlim) par(par.old) Los valores en las posiciones \\(\\left\\{(0,0),(0,1),(1,0),(1,1)\\right\\}\\) coinciden con los generados en el apartado anterior. 9.4 Simulación basada en cópulas Ejercicio 2.3 Consideramos una v.a. bidimensional con distribuciónes marginales uniformes y distribución bidimensional determinada por la cópula de Clayton. Teniendo en cuenta que en este caso: \\[C_{u}^{-1}(w)\\equiv\\left( u^{-\\alpha}\\left( w^{-\\frac{\\alpha}{\\alpha+1}}-1\\right) + 1 \\right)^{-\\frac{1}{\\alpha}},\\] diseñar una rutina que permita generar una muestra de tamaño \\(n\\) de esta distribución. rcclayton &lt;- function(alpha, n) { val &lt;- cbind(runif(n), runif(n)) val[, 2] &lt;- (val[, 1]^(-alpha) * (val[, 2]^(-alpha/(alpha + 1)) - 1) + 1)^(-1/alpha) return(val) } Utilizando la rutina anterior generar una muestra de tamaño 10000 y representar gráficamente los valores obtenidos y sus distribuciones marginales. set.seed(54321) rcunif &lt;- rcclayton(2,10000) plot(rcunif, xlab = &quot;u&quot;, ylab = &quot;v&quot;) Representar la densidad conjunta (con sm::sm.density()) y las marginales: # Densidad conjunta # if(!require(sm)) stop(&#39;Required pakage `sm` not installed.&#39;) sm::sm.density(rcunif) ## Warning: weights overwritten by binning # Distribuciones marginales hist(rcunif[,1], freq = FALSE) abline(h = 1) hist(rcunif[,2], freq = FALSE) abline(h = 1) Empleando el paquete copula: if(!require(copula)) stop(&#39;Required pakage `copula` not installed.&#39;) clayton.cop &lt;- claytonCopula(2, dim = 2) # caso bidimensional y &lt;- rCopula(10000, clayton.cop) plot(y) clayton.cop &lt;- claytonCopula(2, dim = 3) # caso tridimensional y &lt;- rCopula(10000, clayton.cop) scatterplot3d::scatterplot3d(y) A partir de la muestra anterior generar una muestra de una v.a. bidimensional con distribuciones marginales exponenciales de parámetros 1 y 2 respectivamente (y distribución bidimensional determinada por la cópula de Clayton). rcexp &lt;- cbind(qexp(rcunif[,1], 1), qexp(rcunif[,2], 2)) plot(rcexp, xlab = &quot;exp1&quot;, ylab = &quot;exp2&quot;) # Distribuciones marginales hist(rcexp[,1], freq = FALSE) curve(dexp(x,1), add = TRUE) hist(rcexp[,2], freq = FALSE) curve(dexp(x,2), add = TRUE) # ... z &lt;- 1:10 xy &lt;- matrix(z, ncol = 2) xy ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 as.vector(xy) ## [1] 1 2 3 4 5 6 7 8 9 10 # ... 9.5 Simulación de distribuciones multidimensionales discretas 9.5.1 Simulación de una variable discreta bidimensional Ejemplo 9.1 Consideramos datos recogidos en un estudio de mejora de calidad en una fábrica de semiconductores. Se obtuvo una muestra de obleas que se clasificaron dependiendo de si se encontraron partículas en la matriz que producía la oblea y de si la calidad de oblea era buena (Para más detalles Hall, 1994. Analysis of defectivity of semiconductor wafers by contigency table. Proceedings of the Institute of Environmental Sciences 1, 177-183). n &lt;- c(320, 14, 80, 36) particulas &lt;- gl(2, 1, 4, labels = c(&quot;no&quot;,&quot;si&quot;)) calidad &lt;- gl(2, 2, labels = c(&quot;buena&quot;, &quot;mala&quot;)) df &lt;- data.frame(n, particulas, calidad) df ## n particulas calidad ## 1 320 no buena ## 2 14 si buena ## 3 80 no mala ## 4 36 si mala En lugar de estar en el formato de un conjunto de datos (data.frame) puede que los datos estén en formato de tabla (table, matrix): tabla &lt;- xtabs(n ~ calidad + particulas) tabla ## particulas ## calidad no si ## buena 320 14 ## mala 80 36 Lo podemos convertir directamente a data.frame: as.data.frame(tabla) ## calidad particulas Freq ## 1 buena no 320 ## 2 mala no 80 ## 3 buena si 14 ## 4 mala si 36 En este caso definimos las probabilidades a partir de las frecuencias: df$p &lt;- df$n/sum(df$n) df ## n particulas calidad p ## 1 320 no buena 0.71111111 ## 2 14 si buena 0.03111111 ## 3 80 no mala 0.17777778 ## 4 36 si mala 0.08000000 En formato tabla: pij &lt;- tabla/sum(tabla) pij ## particulas ## calidad no si ## buena 0.71111111 0.03111111 ## mala 0.17777778 0.08000000 Para simular la variable bidimensional consideramos una variable unidimensional de índices: z &lt;- 1:nrow(df) z ## [1] 1 2 3 4 Con probabilidades: pz &lt;- df$p pz ## [1] 0.71111111 0.03111111 0.17777778 0.08000000 Si las probabilidades estuviesen en una matriz, las convertiríamos a un vector con: as.vector(pij) ## [1] 0.71111111 0.17777778 0.03111111 0.08000000 Si simulamos la variable unidimenional: set.seed(1) nsim &lt;- 20 rz &lt;- sample(z, nsim, replace = TRUE, prob = pz) Podríamos obtener simulaciones bidimensionales, por ejemplo: etiquetas &lt;- as.matrix(df[c(&#39;particulas&#39;, &#39;calidad&#39;)]) rxy &lt;- data.frame(etiquetas[rz, ]) head(rxy) ## particulas calidad ## 1 no buena ## 2 no buena ## 3 no buena ## 4 si mala ## 5 no buena ## 6 si mala # Alternativa: etiquetas &lt;- df[c(&#39;particulas&#39;, &#39;calidad&#39;)] rxy &lt;- df[rz, ] head(rxy) ## n particulas calidad p ## 1 320 no buena 0.7111111 ## 1.1 320 no buena 0.7111111 ## 1.2 320 no buena 0.7111111 ## 4 36 si mala 0.0800000 ## 1.3 320 no buena 0.7111111 ## 4.1 36 si mala 0.0800000 9.5.2 Simulación de tablas de contingencia El código anterior puede ser empleado para simular tablas de contingencia. Aunque en estos casos se suele fijar el total de la tabla (o incluso las frecuencias marginales). En este caso, sólo habría que fijar el nº de simulaciones al total de la tabla: nsim &lt;- sum(n) set.seed(1) rz &lt;- sample(z, nsim, replace = TRUE, prob = pz) rtable &lt;- table(rz) # Tabla de frecuencias unidimensional matrix(rtable, ncol = 2) # Tabla de frecuencias bidimensional ## [,1] [,2] ## [1,] 321 78 ## [2,] 15 36 Aunque puede ser preferible emplear directamente rmultinom si se van a generar muchas: ntsim &lt;- 1000 rtablas &lt;- rmultinom(ntsim, sum(n), pz) rtablas[ , 1:5] # Las cinco primeras simulaciones ## [,1] [,2] [,3] [,4] [,5] ## [1,] 298 329 323 323 307 ## [2,] 15 21 5 15 15 ## [3,] 92 68 91 77 92 ## [4,] 45 32 31 35 36 Por ejemplo, si se quiere simular bajo independencia, estimando las probabilidades a partir de la tabla: tabla ## particulas ## calidad no si ## buena 320 14 ## mala 80 36 Consideraríamos como probabilidades: pind &lt;- (rowSums(tabla) %o% colSums(tabla))/(sum(tabla)^2) matrix(pind, nrow = nrow(tabla)) ## [,1] [,2] ## [1,] 0.6597531 0.08246914 ## [2,] 0.2291358 0.02864198 rtablas &lt;- rmultinom(ntsim, sum(n), pind) rtablas[ , 1:5] # Las cinco primeras simulaciones ## [,1] [,2] [,3] [,4] [,5] ## [1,] 292 285 309 303 290 ## [2,] 96 105 97 84 113 ## [3,] 48 48 36 49 39 ## [4,] 14 12 8 14 8 Para realizar el contraste de independencia: res &lt;- chisq.test(tabla) res ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tabla ## X-squared = 60.124, df = 1, p-value = 8.907e-15 Ejercicio 5.4 Aproximar por simulación la distribución (exacta) del estadístico ji-cuadrado bajo independencia. simstat &lt;- apply(rtablas, 2, function(x){chisq.test(matrix(x,nrow=nrow(tabla)))$statistic}) hist(simstat, freq = FALSE, breaks = &#39;FD&#39;) # Distribución asintótica (aproximación ji-cuadrado) curve(dchisq(x, res$parameter), add = TRUE) "],
["tecnicas-de-reduccion-de-la-varianza.html", "Capítulo 10 Técnicas de reducción de la varianza 10.1 Reducción de la varianza 10.2 Variables antitéticas 10.3 Estratificación 10.4 Variables de control 10.5 Números aleatorios comunes 10.6 Ejercicios fin de práctica", " Capítulo 10 Técnicas de reducción de la varianza 10.1 Reducción de la varianza Éstas técnicas son aplicadas normalmente cuando se pretende ofrecer respuestas lo más precisas posibles (con menor costo computacional) y principalmente sobre cantidades medias. Supongamos que estamos interesados en aproximar la media de un estadístico mediante simulación y no nos interesa aproximar su varianza. Existe un sinfín de técnicas encaminadas a reducir la varianza en un estudio de simulación (respecto a una aproximación estandar). Algunas de ellas son: Muestreo por importancia. Variables antitéticas. Muestreo estratificado. Variables de control. Números aleatorios comunes. Métodos de remuestreo. Condicionamiento. … 10.2 Variables antitéticas Supongamos que pretendemos aproximar \\[\\theta=E\\left( Z\\right)\\] con \\(Var\\left( Z \\right) = \\sigma^{2}\\). Si generamos \\(n\\) pares \\(\\left( X_{1},Y_{1}\\right), ... ,\\left( X_{n},Y_{n}\\right)\\) de \\(X\\sim Y\\sim Z\\) con \\(Cov\\left( X,Y\\right) &lt; 0\\), el estimador combinado tiene menor varianza: \\[\\begin{aligned} Var\\left( \\frac{\\overline{X}+\\overline{Y}}{2}\\right) &amp; =\\frac{1}{4}\\left( Var\\left( \\overline{X}\\right) +Var\\left( \\overline{Y}\\right) +2Cov\\left( \\overline{X},\\overline{Y}\\right) \\right) \\\\ &amp; =\\frac{\\sigma^{2}}{2n}+\\frac{1}{2n}Cov\\left( X,Y\\right) \\\\ &amp; =\\frac{\\sigma^{2}}{2n}\\left( 1+\\rho \\left( X,Y\\right) \\right), \\end{aligned}\\] que el equivalente a una muestra unidimensional independiente con el mismo número de observaciones \\(2n\\) (con una reducción del \\(-100\\rho \\left( X,Y\\right) \\%\\)). 10.2.1 Ejemplo: Integración Monte Carlo Para aproximar: \\[I=\\int_{0}^{1}h\\left( x\\right) dx,\\] a partir de \\(x_{1},x_{2},\\ldots,x_{n}\\) \\(i.i.d.\\) \\(\\mathcal{U}\\left(0,1\\right)\\). Podemos emplear: \\[\\begin{aligned} I &amp; =E\\left( \\frac{h\\left( U\\right) +h\\left( 1-U\\right) }{2}\\right) \\\\ &amp; \\approx \\frac{1}{2n}\\sum \\limits_{i=1}^{n}\\left( h\\left( x_{i}\\right) +h\\left( 1-x_{i}\\right) \\right). \\end{aligned}\\] 10.2.2 Generación de variables antitéticas Cuando se utiliza el método de inversión resulta sencillo obtener pares de variables con correlación negativa: \\(U\\sim \\mathcal{U}\\left( 0,1\\right)\\) para simular \\(X\\). \\(1-U\\) para simular la variable antitética \\(Y\\). En el caso general, si \\(X=h\\left( U_{1},\\ldots,U_{d}\\right)\\) y \\(h\\) es monótona puede verse (e.g. Ross, 1997) que \\(Y=h\\left( 1-U_{1},\\ldots,1-U_{d}\\right)\\) está negativamente correlada con \\(X\\). Si \\(X\\sim \\mathcal{N}(\\mu,\\sigma)\\) puede tomarse como variable antitética \\[Y=2\\mu-X\\] En general esto es válido para cualquier variable simétrica repecto a un parámetro \\(\\mu\\). (e.g. \\(X\\sim \\mathcal{U}(a,b)\\) e \\(Y=a+b-X\\)). Ejercicio 5.1 Variables antitéticas en integración Monte Carlo Crear una función que implemente la técnica de variables antitéticas para aproximar integrales del tipo:\\[I=\\int_{a}^{b}h\\left( x\\right) dx.\\] Emplearla para aproximar: \\[E\\left( e^{\\mathcal{U}(0,2)}\\right) =\\int_{0}^{2}\\frac{1}{2}e^{x}dx\\approx3.194,\\label{integralmc}\\] y representar gráficamente la aproximación en función de \\(n\\). Función objetivo: a &lt;- 0; b &lt;- 2 ftn &lt;- function(x) return(exp(x)/(b-a)) curve(ftn, a, b, ylim=c(0,4)) abline(h=0,lty=2) abline(v=c(a,b),lty=2) Se trata de calcular la media de \\(e^{\\mathcal{U}(0,2)}\\): teor &lt;- (exp(b)-exp(a))/(b-a) teor ## [1] 3.194528 Para la aproximación por integración Monte Carlo podemos emplear la función del capítulo anterior: mc.integral &lt;- function(ftn, a, b, n, plot=TRUE) { fx &lt;- sapply(runif(n, a, b), ftn)*(b-a) if (plot) { estint &lt;- cumsum(fx)/(1:n) esterr &lt;- sqrt(cumsum((fx-estint)^2))/(1:n) plot(estint, ylab=&quot;Media y rango de error&quot;, type=&quot;l&quot;, lwd= 2, ylim=mean(fx)+2*c(-esterr[1],esterr[1]), xlab=&quot;Iteraciones&quot;) abline(h = estint[n], lty=2) lines(estint+2*esterr, lty = 3) lines(estint-2*esterr, lty = 3) return(list(valor=estint[n], error=2*esterr[n])) } else return(list(valor=mean(fx), error=2*sd(fx)/sqrt(n))) } set.seed(54321) res &lt;- mc.integral(ftn, a, b, 500) abline(h = teor) res ## $valor ## [1] 3.184612 ## ## $error ## [1] 0.1619886 Para la integración Monte Carlo con variables antitéticas podríamos considerar: mc.integrala &lt;- function(ftn, a, b, n, plot=TRUE,...) { # n es el nº de evaluaciones de la función objetivo (para facilitar comparaciones, solo se genera la mitad) x &lt;- runif(n%/%2, a, b) # La siguiente línea solo para representar alternando x &lt;- as.numeric(matrix(c(x,a+b-x),nrow=2,byrow=TRUE)) # bastaría con emplear p.e. c(x,a+b-x) fx &lt;- sapply(x, ftn)*(b-a) if (plot) { estint &lt;- cumsum(fx)/(1:n) esterr &lt;- sqrt(cumsum((fx-estint)^2))/(1:n) plot(estint, ylab=&quot;Media y rango de error&quot;,type=&quot;l&quot;, lwd = 2, ylim=mean(fx)+2*c(-esterr[1],esterr[1]),xlab=&quot;Iteraciones&quot;,...) abline(h = estint[n], lty=2) lines(estint+2*esterr, lty = 3) lines(estint-2*esterr, lty = 3) return(list(valor=estint[n],error=2*esterr[n])) } else return(list(valor=mean(fx),error=2*sd(fx)/sqrt(n))) } set.seed(54321) res &lt;- mc.integrala(ftn, a, b, 500) res ## $valor ## [1] 3.222366 ## ## $error ## [1] 0.1641059 Pero aunque aparentemente converge antes, parece no haber una mejora en la precisión de la aproximación. Si calculamos el porcentaje (estimado) de reducción del error: 100*(0.1619886-0.1641059)/0.1619886 ## [1] -1.307067 El problema es que en este caso se está estimando mal la varianza (asumiendo independencia). Hay que tener cuidado con las técnicas de reducción de la varianza si uno de los objetivos de la simulación es precisamente estimar la variabilidad. En este caso, una versión de la función anterior para integración Monte Carlo con variables antitéticas, con aproximación del error bajo dependencia podría ser: mc.integrala2 &lt;- function(ftn, a, b, n, plot = TRUE,...) { # n es el nº de evaluaciones de la función objetivo (para facilitar comparaciones, solo se genera la mitad) x &lt;- runif(n%/%2, a, b) # La siguiente línea solo para representar alternando x &lt;- matrix(c(x,a+b-x),nrow=2,byrow=TRUE) # bastaría con emplear p.e. c(x,a+b-x) fx &lt;- apply(x, 1, ftn)*(b-a) corr &lt;- cor(fx[,1], fx[,2]) fx &lt;- as.numeric(fx) return(list(valor=mean(fx), error=2*sd(fx)/sqrt(n)*sqrt(1+corr))) } set.seed(54321) res &lt;- mc.integrala2(ftn, a, b, 500) res ## $valor ## [1] 3.222366 ## ## $error ## [1] 0.05700069 Porcentaje estimado de reducción del error: 100*(0.1619886-0.05700069)/0.1619886 ## [1] 64.81191 En este caso puede verse que la reducción teórica de la varianza es del 96.7% 10.3 Estratificación Si se divide la población en estratos y se genera en cada uno un número de observaciones proporcional a su tamaño (a la probabilidad de cada uno) nos aseguramos de que se cubre el dominio de interés y se puede acelerar la convergencia. Por ejemplo, para generar una muestra de tamaño \\(n\\) de una \\(\\mathcal{U}\\left( 0,1\\right)\\), se pueden generar \\(l=\\frac{n}{k}\\) observaciones (\\(1\\leq k\\leq n\\)) de la forma: \\[U_{j_{1}},\\ldots,U_{j_{l}}\\sim \\mathcal{U}\\left( \\frac{(j-1)}{k},\\frac{j}{k}\\right) \\text{ para }j=1,...,k.\\] Si en el número de obsevaciones se tiene en cuenta la variabilidad en el estrato se puede obtener una reducción significativa de la varianza. Ejemplo 10.1 Muestreo estratificado de una exponencial (libro Ricardo) Supóngase el siguiente problema (absolutamente artificial pero ilustrativo para comprender esta técnica). Dada una muestra de tamaño 10 de una población con distribución: \\[X \\sim \\exp\\left( 1 \\right),\\] se desea aproximar la media poblacional (es sobradamente conocido que es 1) a partir de 10 simulaciones. Supongamos que para evitar que, por puro azar, exista alguna zona en la que la exponencial toma valores, no representada en la muestra simulada de 10 datos, se consideran tres estratos. Por ejemplo, el del 40% de valores menores, el siguiente 50% de valores (intermedios) y el 10% de valores más grandes para esta distribución. El algoritmo de inversión (optimizado) para simular una \\(\\exp\\left(1\\right)\\) es: Generar \\(U\\sim U\\left( 0,1\\right)\\). Hacer \\(X=-\\ln U\\). Dado que, en principio, simulando diez valores \\(U_{1},U_{2},\\ldots,U_{10}\\sim U\\left( 0,1\\right)\\), no hay nada que nos garantice que las proporciones de los estratos son las deseadas (aunque sí lo serán en media). Una forma de garantizar el que obtengamos 4, 5 y 1 valores, repectivamente, en cada uno de los tres estratos, consiste en simular: 4 valores de \\(U[0.6,1)\\) para el primer estrato, 5 valores de \\(U[0.1,0.6)\\) para el segundo y uno de \\(U[0,0.1)\\) para el tercero. Otra forma de proceder consistiría en rechazar valores de \\(U\\) que caigan en uno de esos tres intervalos cuando el cupo de ese estrato esté ya lleno (lo cual no sería computacionalmente eficiente). El algoritmo con la estratificación propuesta sería como sigue: Para \\(i=1,2,\\ldots, 10\\): Generar \\(U_{i}\\): 2a. Generar \\(U\\sim U\\left( 0,1\\right)\\). 2b. Si \\(i\\leq4\\) hacer \\(U_{i} = 0.4 \\cdot U + 0.6\\). 2c. Si \\(4&lt;i\\leq9\\) hacer \\(U_{i} = 0.5 \\cdot U + 0.1\\). 2d. Si \\(i=10\\) hacer \\(U_{i} = 0.1 \\cdot U\\). Devolver \\(X_{i}=-\\ln U_{i}\\). No es difícil probar que: \\(Var\\left( X_{i}\\right) = 0.0214644\\) si \\(i=1,2,3,4\\), \\(Var\\left( X_{i}\\right) = 0.229504\\) si \\(i=5,6,7,8,9\\) y \\(Var\\left( X_{10}\\right) = 1\\). Como consecuencia: \\[Var\\left( \\overline{X}\\right) =\\frac{1}{10^{2}}\\sum_{i=1}^{10} Var\\left( X_{i} \\right) = 0.022338\\] que es bastante menor que 1 (la varianza en el caso de muestreo aleatorio simple no estratificado). Ejercicio 2.3 Integración Monte Carlo con estratificación Aproximar la integral anterior empleando la técnica de estratificación, considerando \\(k\\) subintervalos regularmente espaciados en el intervalo \\(\\left[ 0, 2 \\right]\\). ¿Como varía la reducción en la varianza dependiendo del valor de \\(k\\)? mc.integrale &lt;- function(ftn, a, b, n, k) { # Integración Monte Carlo con estratificación l &lt;- n%/%k int &lt;- seq(a, b, len=k+1) x &lt;- runif(l*k, rep(int[-(k+1)], each=l), rep(int[-1], each=l)) # l uniformes en cada uno de los intervalos [(j-1)/k , j/k] fx &lt;- sapply(x, ftn)*(b-a) return(list(valor=mean(fx), error=2*sd(fx)/sqrt(n))) # error mal calculado } set.seed(54321) res &lt;- mc.integral(ftn, a, b, 500) abline(h = teor) res ## $valor ## [1] 3.184612 ## ## $error ## [1] 0.1619886 set.seed(54321) mc.integrale(ftn, a, b, 500, 50) ## $valor ## [1] 3.193338 ## ## $error ## [1] 0.1597952 set.seed(54321) mc.integrale(ftn, a, b, 500, 100) ## $valor ## [1] 3.193927 ## ## $error ## [1] 0.1599089 De esta forma no se tiene en cuenta la variabilidad en el estrato. El tamaño de las submuestras debería incrementarse hacia el extremo superior. Ejercicio 10.1 Repetir el ejemplo anterior considerando intervalos regularmente espaciados en escala exponencial. 10.4 Variables de control En este caso se trata de sacar partido tanto a una covarianza positiva como negativa. La idea básica es emplear una variable \\(Y\\), con media conocida \\(\\mu_{Y}\\), para controlar la variable \\(X\\) (con media desconocida), de forma que ambas variables estén “suficientemente” correlacionadas. La versión “controlada” de \\(X\\) será: \\[X^{\\ast}=X+\\alpha \\left( Y-\\mu_{Y}\\right)\\] con \\(E(X^{\\ast})=E(X)=\\theta\\). Puede verse que \\(Var(X^{\\ast})=Var(X)+\\alpha^{2}Var(Y)+2\\alpha Cov(X,Y)\\) es mínima para: \\[\\alpha^{\\ast}=-\\frac{Cov(X,Y)}{Var(Y)},\\] con \\(Var(X^{\\ast}) = Var(X) \\left( 1-\\rho^{2} \\left( X, Y \\right) \\right)\\) (lo que supone una reducción del \\(100\\rho^{2}\\left( X, Y \\right) \\%\\)). En la práctica normalmente \\(\\alpha^{\\ast}\\) no es conocida. Para estimarlo se puede realizar ajuste lineal de \\(X\\) sobre \\(Y\\) (a partir de los datos simulados \\(X_{i}\\) e \\(Y_{i}\\), \\(1\\leq i\\leq n\\)): Si \\(\\hat{x}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}y\\) es la recta ajustada, con \\(\\hat{\\beta}_{1} = \\dfrac{S_{XY}}{S_{Y}^{2}}\\) y \\(\\hat{\\beta}_{0} = \\overline{X}-\\hat{\\beta}_{1}\\overline{Y}\\), la estimación sería: \\[\\hat{\\alpha}^{\\ast}=-\\hat{\\beta}_{1}\\] Adicionalmente, para aproximar \\(\\theta\\): \\[\\begin{aligned} \\hat{\\theta} &amp; =\\overline{X}^{\\ast}=\\overline{X}-\\hat{\\beta}_{1}\\left( \\overline{Y}-\\mu_{Y}\\right) \\\\ &amp; =\\hat{\\beta}_{0}+\\hat{\\beta}_{1}\\mu_{Y} \\end{aligned}\\] Si \\(\\mu_{Y}=0\\Rightarrow \\hat{\\theta}=\\overline{X}^{\\ast}=\\hat{\\beta}_{0}\\). Ejercicio 7.3 Integración Monte Carlo con variables de control Aproximar la integral anterior empleando la variable \\(U\\sim\\mathcal{U}(0,2)\\) para controlar la variable \\(e^{U}\\). Se trata de calcular la media de \\(exp(\\mathcal{U}(a,b))\\): a &lt;- 0; b &lt;- 2 teor &lt;- (exp(b)-exp(a))/(b-a) teor ## [1] 3.194528 Aproximación clásica por simulación: set.seed(54321) nsim &lt;- 1000 u &lt;- runif(nsim, a, b) expu &lt;- exp(u) mean(expu) ## [1] 3.182118 Con variable control: plot(u, expu) reg &lt;- lm(expu ~ u)$coef abline(reg, col=&#39;blue&#39;) # summary(lm(expu ~ u)) # R-squared: 0.9392 reg[1]+reg[2] # Coincidirá con la solución mean(expuc) ## (Intercept) ## 3.204933 Lo siguiente ya no sería necesario: expuc &lt;- expu - reg[2]*(u-1) mean(expuc) ## [1] 3.204933 Estimación del porcentaje de reducción en la varianza: 100*(var(expu)-var(expuc))/var(expu) ## [1] 93.91555 10.5 Números aleatorios comunes Se trataría de una técnica básica del diseño de experimentos: realizar comparaciones homogéneas (bloquear). Por ejemplo cuando se diseña un experimento para la comparación de la media de dos variables, se pueden emplear las denominadas muestras apareadas, en lugar de muestras independientes. Supóngamos que estamos interesados en las diferencias entre dos estrategias (e.g. dos estimadores): \\[E\\left( X\\right) -E\\left( Y\\right) =E\\left( X-Y\\right).\\] Para ello se generan dos secuencias \\(X_{1}\\), \\(X_{2}\\), \\(\\ldots\\), \\(X_{n}\\), e \\(Y_{1}\\), \\(Y_{2}\\), \\(\\ldots\\), \\(Y_{n}\\) y se calcula: \\[\\overline{X}-\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}\\left( X_{i}-Y_{i}\\right)\\] Si las secuencias se generan de modo independiente: \\[Var\\left( \\overline{X} - \\overline{Y} \\right) = \\frac{1}{n} \\left( Var\\left( X \\right) + Var\\left( Y \\right) \\right)\\] Si se generar las secuencias empleando la misma semilla, los datos son dependientes: \\[Cov\\left( X_{i}, Y_{i} \\right) &gt; 0\\] y tendríamos que: \\[\\begin{aligned} Var\\left( \\overline{X}-\\overline{Y}\\right) &amp; = \\frac{1}{n^{2}}\\sum_{i=1}^{N}Var\\left( X_{i}-Y_{i}\\right) = \\frac{1}{n}Var\\left( X_{i}-Y_{i}\\right) \\\\ &amp; = \\frac{1}{n}\\left( Var\\left( X_{i} \\right) + Var\\left( Y_{i} \\right) - 2Cov\\left( X_{i},Y_{i} \\right) \\right) \\\\ &amp; \\leq \\frac{1}{n}\\left( Var\\left( X_{i} \\right) + Var\\left( Y_{i}\\right) \\right) \\end{aligned}\\] En el capítulo de aplicaciones de la simulación se empleó esta técnica para comparar distribuciones de estimadores… 10.6 Ejercicios fin de práctica Ejercicio 10.2 Aproximar mediante integración Monte Carlo (clásica) la media de una distribución exponencial de parámetro \\(1/2\\):\\[I=\\int_{0}^{\\infty}\\frac{x}{2}e^{-\\frac{x}{2}}dx\\] y representar gráficamente la aproximación en función de \\(n\\). Comparar los resultados con los obtenidos empleando variables antitéticas, ¿se produce una reducción en la varianza? Nota: Puede ser recomendable emplear el método de inversión para generar las muestras (antitéticas) de la exponencial. MC clásico: nsim &lt;- 1000 lambda &lt;- 0.5 set.seed(1) x &lt;- - log(runif(nsim)) / lambda # Aprox por MC da media mean(x) # valor teor 1/lambda = 2 ## [1] 1.97439 # Aprox da precisión var(x) ## [1] 3.669456 MC con variables antitéticas: # xa &lt;- # mean(xa) # Aprox por MC da media (valor teor 1/lambda = 2) # var(xa) # Aprox da precisión supoñendo independencia # corr &lt;- cor(x1,x2) # var(xa)*(1 + corr) # Estimación varianza supoñendo dependencia Estimación del porcentaje de reducción en la varianza # 100*(var(x) - var(xa))/var(x) "],
["pendiente-ejercicios-bookdown.html", "Capítulo 11 Pendiente (ejercicios bookdown)", " Capítulo 11 Pendiente (ejercicios bookdown) Homogeneizar títulos capítulos Capítulo 6 Integración y Optimización Montecarlo: ejercicios (“Practica7.Rmd”) Capítulo 7 Simulación de variables continuas: ejemplos (“Simulación por inversión.r”, “Simulación por aceptación-rechazo.r”, “modsis_2.tex”) y teoría (“3_Metodos_generales_continuas.tex”) 7.4 Métodos específicos para generación de distribuciones notables: algunos ejemplos del libro de Ricardo y de las funciones implementadas en R (“modsis_2.tex”) Capítulo 8 Simulación de variables discretas: ejemplos (“Inversión discreta.r”, “Discreta dominio infinito.r”, “modsis_2.tex”) y teoría (“4_Metodos_generales_discretas.tex”) 8.2 Métodos específicos para generación de distribuciones notables: algunos ejemplos del libro de Ricardo y de las funciones implementadas en R (“modsis_2.tex”) Capítulo 9 Simulación de Distribuciones Multidimensionales: ejemplos(“modsis_2.tex”) y teoría (“5_Simulacion_multidimensional.tex”) Capítulo 11 Simulación dinámica: ejemplos (paquete arqas, “Colas clasicas.r”), ejercicios (“Practica_8_Cola_discreta.r”) y teoría (“modsis_2.tex”, “Simulación por eventos discretos”) Simulación por eventos y por cuantos. Problemas de estabilización y dependencia. Capítulo 12 Introducción a los métodos de cadenas de Markov Monte Carlo: ejemplos (“10_MCMC.r”), ejercicios (“Practica_10_MCMC.r”) y teoría (“10_MCMC.tex”) Muestreo de Gibbs. Algoritmo Metropolis Hastings. Diagnosis de un algoritmo MCMC. "],
["referencias.html", "Referencias Bibliografía básica Bibliografía complementaria", " Referencias Bibliografía básica Cao, R. (2002). Introducción a la simulación y a la teoría de colas. NetBiblo. Gentle, J.E. (2003). Random number generation and Monte Carlo methods. Springer‐Verlag. Jones, O. et al. (2009). Introduction to Scientific Programming and Simulation Using R. CRC. Ripley, B.D. (1987). Stochastic Simulation. John Wiley &amp; Sons. Robert, C.P. y G. Casella (2010). Introducing Monte Carlo Methods with R. Springer. Ross, S.M. (1999).Simulación. Prentice Hall. Suess, E.A. y Trumbo, B.E. (2010). Introduction to probability simulation and Gibbs sampling with R. Springer. Bibliografía complementaria Azarang, M. R. y García Dunna, E. (1996). Simulación y análisis de modelos estocásticos. McGraw-Hill. Bratley, P., Fox, B. L. y Schrage L. E. (1990). A guide to simulation. Springer-Verlag. Devroye, L. (1986). Non-uniform random variate generation. Springer-Verlag. Evans, M. y Swartz, T. (2000). Approximating integrals via Monte Carlo and determinstic methods. Oxford University Press. Gentle, J.E. (1998). Random number generation and Monte Carlo methods. Springer-Verlag. Hörmann, W. et al. (2004). Automatic Nonuniform Random Variate Generation. Springer. Law, A.M. y Kelton, W.D. (1991). Simulation, modeling and analysis. McGraw-Hill. Moeschlin, O., Grycko, E., Pohl, C. y Steinert, F. (1998). Experimental stochastics. Springer-Verlag. Nelson, R. (1995). Probability, stochastic processes, and queueing theory: the mathematics of computer performance modelling. Springer-Verlag. Pardo, L. y Valdés, T. (1987). Simulación. Aplicaciones prácticas a la empresa. Díaz de Santos. Robert, C.P. y G. Casella (2004). Monte Carlo statistical methods. Springer. "],
["recursos-para-el-aprendizaje-de-r.html", "A Recursos para el aprendizaje de R A.1 Cursos A.2 Libros A.3 Material A.4 Ayuda", " A Recursos para el aprendizaje de R A.1 Cursos Para información sobre cursos en castellano se puede recurrir a la web de R-Hispano en el apartado formación. Algunos de los cursos que aparecen en entradas antiguas son gratuitos. Ver: Cursos MOOC relacionados con R. Algunos cursos gratuitos: Coursera Introducción a Data Science: Programación Estadística con R Mastering Software Development in R DataCamp Introducción a R Stanford online Statistical Learning Curso UCA: Introducción a R, R-commander y shiny A.2 Libros A.2.1 Iniciación 2011 - The Art of R Programming. A Tour of Statistical Software Design - Matloff (No Starch Press) R for Data Science (online, O’Reilly) Hands-On Programming with R: Write Your Own Functions and Simulations, by Garrett Grolemund (O’Reilly) A.2.2 Avanzados 2008 - Software for Data Analysis: Programming with R - Chambers (Springer) Advanced R by Hadley Wickham (online, Chapman &amp; Hall) R packages by Hadley Wickham (online, O’Reilly) A.3 Material En la web se puede encontrar mucho material, por ejemplo: CRAN Contributed Documentation http://oscarperpinan.github.io/R/ Bookdown A.4 Ayuda Ayuda online sobre funciones o paquetes: RDocumentation Buscador RSeek StackOverflow "],
["integracion-numerica.html", "B Integración numérica B.1 Introducción B.2 Integración numérica unidimensional B.3 Integración numérica bidimensional", " B Integración numérica B.1 Introducción En muchos casos nos puede interesar la aproximación de una integral definida. En estadística, además del caso de Inferencia Bayesiana (que trataremos más adelante empleando Integración Montecarlo), nos puede interesar por ejemplo aproximar mediante simulación el error cuadrático integrado medio (MISE) de un estimador. En el caso de una densidad univariante sería de la forma: \\[MISE \\left\\{ \\hat{f} \\right\\} = \\operatorname{E}\\int (\\hat{f}(x)-f(x))^2 \\, dx\\] Cuendo el numero de dimensiones es pequeño, nos puede ineteresar emplear un método numérico para aproximar este tipo de integrales. B.2 Integración numérica unidimensional Supongamos que nos interesa aproximar una integral de la forma: \\[I=\\int_{a}^{b}h\\left( x\\right)dx\\]. Consideraremos como ejemplo: \\[\\int_{0}^{1}4x^{3}dx=1\\]. fun &lt;- function(x) return(4 * x^3) curve(fun, 0, 1) abline(h=0,lty=2) abline(v=c(0,1),lty=2) B.2.1 Método del trapezoide La regla de los trapecios es una forma de aproximar la integral utilizando \\(n\\) trapecios. Si se consideran \\(n\\) subintervalos en \\([a,b]\\) de longitud \\(h= \\frac{b-a}{n}\\) (i.e. \\(n + 1\\) puntos regularmente espaciados cubriendo el dominio), y se aproxima linealmente la función en cada subintervalo, se obtiene que: \\[\\int_a^b f(x)\\, dx \\approx \\frac{h}{2} [f(a)+2f(a+h)+2f(a+2h)+...+f(b)]\\] trapezoid.vec &lt;- function(f.vec, h = 0.01) { # Integración numérica unidimensional entre a y b # utilizando el método del trapezoide # (se aproxima f linealmente en cada intervalo) n &lt;- length(f.vec) return(h*(f.vec[1]/2 + sum(f.vec[2:(n-1)]) + f.vec[n]/2)) } trapezoid &lt;- function(fun, a=0, b=1, n=100) { # Integración numérica de fun (función unidimensional) entre a y b # utilizando el método del trapezoide con n subdivisiones # (se aproxima f linealmente en cada intervalo) # Se asume a &lt; b y n entero positivo h &lt;- (b-a)/n x.vec &lt;- seq(a, b, by = h) f.vec &lt;- sapply(x.vec, fun) return(trapezoid.vec(f.vec, h)) } trapezoid(fun, 0, 1, 20) ## [1] 1.0025 El error en esta aproximación se corresponde con: \\[ \\frac{(b-a)^3}{12n^2}\\,f&#39;&#39;(\\xi), \\] para algún \\(a\\leq \\xi \\leq b\\) (dependiendo del signo de la segunda derivada, i.e. de si la función es cóncava o convexa, el error será negativo ó positivo). El error máximo absoluto es \\(\\frac{(b-a)^3}{12n^2}\\max_{a\\leq \\xi \\leq b}\\left|f&#39;&#39;(\\xi)\\right|\\). En el caso general multidimensional sería \\(O(n^{-\\frac{2}{d}})\\). B.2.2 Regla de Simpson Se divide el intervalo \\(n\\) subintervalos de longitud \\(h= \\frac{b-a}{n}\\) (con \\(n\\) par), considerando \\(n + 1\\) puntos regularmente espaciados \\(x_i = a + ih\\), para \\(i = 0, 1, ..., n\\). Aproximando de forma cuadrática la función en cada subintervalo \\([x_{j-1},x_{j+1}]\\) (considerando 3 puntos), se obtiene que: \\[ \\int_a^b f(x) \\, dx \\approx \\frac{h}{3} \\bigg[ f(x_0)+2\\sum_{j=1}^{(n/2)-1}f(x_{2j})+ 4\\sum_{j=1}^{n/2}f(x_{2j-1})+f(x_n) \\bigg],\\] simpson &lt;- function(fun, a, b, n = 100) { # Integración numérica de fnt entre a y b # utilizando la regla de Simpson con n subdivisiones # (se aproxima fun de forma cuadrática en cada par de intervalos) # fnt es una función de una sola variable # Se asume a &lt; b y n entero positivo par n &lt;- max(c(2*(n %/% 2), 4)) h &lt;- (b-a)/n x.vec1 &lt;- seq(a+h, b-h, by = 2*h) x.vec2 &lt;- seq(a+2*h, b-2*h, by = 2*h) f.vec1 &lt;- sapply(x.vec1, fun) f.vec2 &lt;- sapply(x.vec2, fun) return(h/3*(fun(a) + fun(b) + 4*sum(f.vec1) + 2*sum(f.vec2))) # Una cota del error en valor absoluto es: # h^4*(b-a)*max(c(f.vec1, fvec.2))^4/180. } simpson(fun, 0, 1, 20) ## [1] 1 El máximo error (en el caso unidimensional) viene dado por la expresión: \\[\\frac{(b-a)^5}{180n^4}\\,\\max_{a\\leq \\xi \\leq b}\\left| f^{(4)}(\\xi) \\right|.\\] En el caso general multidimensional sería \\(O(n^{-\\frac{4}{d}})\\). B.2.3 Cuadratura adaptativa En lugar de evaluar la función en una rejilla regular (muestrear por igual el dominio), puede interesar ir añadiendo puntos sólo en los lugares “necesarios”. quadrature &lt;- function(fun, a, b, tol=1e-8) { # numerical integration using adaptive quadrature simpson2 &lt;- function(fun, a, b) { # numerical integral using Simpson&#39;s rule # assume a &lt; b and n = 2 return((b-a)/6 * (fun(a) + 4*fun((a+b)/2) + fun(b))) } quadrature_internal &lt;- function(S.old, fun, a, m, b, tol, level) { level.max &lt;- 100 if (level &gt; level.max) { cat (&quot;recursion limit reached: singularity likely\\n&quot;) return (NULL) } S.left &lt;- simpson2(fun, a, m) S.right &lt;- simpson2(fun, m, b) S.new &lt;- S.left + S.right if (abs(S.new-S.old) &gt; tol) { S.left &lt;- quadrature_internal(S.left, fun, a, (a+m)/2, m, tol/2, level+1) S.right &lt;- quadrature_internal(S.right, fun, m, (m+b)/2, b, tol/2, level+1) S.new &lt;- S.left + S.right } return(S.new) } level = 1 S.old &lt;- (b-a) * (fun(a) + fun(b))/2 S.new &lt;- quadrature_internal(S.old, fun, a, (a+b)/2, b, tol, level+1) return(S.new) } quadrature(fun, 0, 1) ## [1] 1 Fuente r-blogger Guangchuang Yu? B.2.4 Comandos de R integrate(fun, 0, 1) # Permite límites infinitos ## 1 with absolute error &lt; 1.1e-14 ## Cuidado: fun debe ser vectorial... require(MASS) area(fun, 0, 1) ## [1] 1 B.3 Integración numérica bidimensional Supongamos que nos interesa aproximar una integral de la forma: \\[I=\\int_{a_x}^{b_x}\\int_{a_y}^{b_y}f(x, y)dy dx\\]. Consideraremos como ejemplo: \\[\\int_{-1}^{1} \\int_{-1}^{1} \\left( x^2 - y^2 \\right) dx dy = 0\\]. f2d &lt;- function(x,y) x^2 - y^2 Es habitual (especialmente en simulación) que la función se evalúe en una rejilla: ax = -1 ay = -1 bx = 1 by = 1 nx = 21 ny = 21 x &lt;- seq(ax, bx, length = nx) y &lt;- seq(ay, by, length = ny) z &lt;- outer(x, y, f2d) hx &lt;- x[2]-x[1] hy &lt;- y[2]-y[1] B.3.1 Representación gráfica Puede ser de utilidad las herramientas de los paquetes plot3D y plot3Drgl (también se pueden utilizar las funciones spersp, simage, spoints y splot del paquete npsp). if(!require(plot3D)) stop(&#39;Required pakage `plot3D` not installed.&#39;) # persp3D(z = z, x = x, y = y) persp3D.f2d &lt;- function(f2d, ax=-1, bx=1, ay=-1, by=1, nx=21, ny=21, ...) { x &lt;- seq(ax, bx, length = nx) y &lt;- seq(ay, by, length = ny) hx &lt;- x[2]-x[1] hy &lt;- y[2]-y[1] z &lt;- outer(x, y, f2d) persp3D(x, y, z, ...) } persp3D.f2d(f2d, -1, 1, -1, 1, 50, 50, 1, ticktype = &quot;detailed&quot;) B.3.2 Método del trapezoide Error \\(O(n^{-\\frac{2}{d}})\\). trapezoid.mat &lt;- function(z, hx, hy) { # Integración numérica bidimensional # utilizando el método del trapezoide (se aproxima f linealmente) f.vec &lt;- apply(z, 1, function(x) trapezoid.vec(x, hx)) return(trapezoid.vec(f.vec, hy)) } # trapezoid.mat(z, hx, hy) trapezoid.f2d &lt;- function(f2d, ax=-1, bx=1, ay=-1, by=1, nx=21, ny=21) { x &lt;- seq(ax, bx, length = nx) y &lt;- seq(ay, by, length = ny) hx &lt;- x[2]-x[1] hy &lt;- y[2]-y[1] z &lt;- outer(x, y, f2d) trapezoid.mat(z, hx, hy) } trapezoid.f2d(f2d, -1, 1, -1, 1, 101, 101) ## [1] -8.881784e-18 B.3.3 Comandos R Suponiendo que la función es vectorial, podemos emplear: integrate( function(y) { sapply(y, function(y) { integrate(function(x) f2d(x,y), ax, bx)$value }) }, ay, by) ## -2.775558e-17 with absolute error &lt; 1.1e-14 Si la función no es vectorial y solo admite parámetros escalares: integrate(function(y) { sapply(y, function(y) { integrate(function(x) { sapply(x, function(x) f2d(x,y)) }, ax, bx)$value }) }, ay, by) Fuente tolstoy.newcastle.edu.au. Alternativamente se podría emplear la función adaptIntegrate del paquete cubature. "]
]
